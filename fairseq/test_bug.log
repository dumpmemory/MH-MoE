2023-06-30 14:17:04 | INFO | fairseq.distributed.utils | distributed init (rank 1): env://
2023-06-30 14:17:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-06-30 14:17:04 | INFO | fairseq.distributed.utils | distributed init (rank 0): env://
2023-06-30 14:17:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-06-30 14:17:04 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
2023-06-30 14:17:04 | INFO | fairseq.distributed.utils | initialized host GCRHYP3C356 as rank 0
2023-06-30 14:17:04 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
2023-06-30 14:17:04 | INFO | fairseq.distributed.utils | initialized host GCRHYP3C356 as rank 1
[1688134627.869002] [GCRHYP3C356:90683:f]        vfs_fuse.c:281  UCX  ERROR inotify_add_watch(/tmp) failed: No space left on device
[1688134628.205762] [GCRHYP3C356:90684:f]        vfs_fuse.c:281  UCX  ERROR inotify_add_watch(/tmp) failed: No space left on device
2023-06-30 14:17:10 | INFO | fairseq_cli.train | nvidia-smi stats: {'gpu_0_mem_used_gb': 18.9736328125, 'gpu_1_mem_used_gb': 19.3271484375, 'gpu_2_mem_used_gb': 13.3076171875, 'gpu_3_mem_used_gb': 13.2998046875, 'gpu_4_mem_used_gb': 13.3076171875, 'gpu_5_mem_used_gb': 13.2919921875, 'gpu_6_mem_used_gb': 13.3017578125, 'gpu_7_mem_used_gb': 13.2724609375, 'gpu_8_mem_used_gb': 13.1748046875, 'gpu_9_mem_used_gb': 13.3017578125, 'gpu_10_mem_used_gb': 13.3017578125, 'gpu_11_mem_used_gb': 13.2880859375, 'gpu_12_mem_used_gb': 13.2998046875, 'gpu_13_mem_used_gb': 13.3017578125, 'gpu_14_mem_used_gb': 13.3017578125, 'gpu_15_mem_used_gb': 13.2216796875}
2023-06-30 14:17:10 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 50, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': '/mnt1/msranlpintern/wuxun/MoE/MoE_results/test_moe/small-baseline-redstone_v2-flash_attn-8experts/tb-logs', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'log_nvidia_smi': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None, 'is_moe': False}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 2, 'num_workers_valid': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 2, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': None, 'batch_size_valid': 2, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 300000, 'stop_time_hours': 0.0, 'clip_norm': 2.0, 'sentence_avg': False, 'update_freq': [8], 'lr': [0.0006], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/mnt1/msranlpintern/wuxun/MoE/MoE_results/test_moe/small-baseline-redstone_v2-flash_attn-8experts', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_best_checkpoints': False, 'no_save_optimizer_state': False, 'no_save_optimizer_state_on_training_finished': False, 'symlink_best_and_last_checkpoints': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '-rank-0', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 's3_upload_path': None, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807, 'stats_path': None, 'max_valid_steps': None}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'gpt_small', 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 768, 'decoder_output_dim': 768, 'decoder_input_dim': 768, 'decoder_ffn_embed_dim': 3072, 'decoder_layers': 12, 'decoder_attention_heads': 12, 'decoder_normalize_before': True, 'no_token_positional_embeddings': True, 'share_decoder_input_output_embed': True, 'decoder_learned_pos': True, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'min_params_to_wrap': 100000000, 'moe_freq': 2, 'moe_expert_count': 8, 'moe_gating_use_fp32': True, 'moe_second_expert_policy': 'random', 'moe_normalize_gate_prob_before_dropping': True, 'moe_expert_ffn_dim': None, 'moe_top1_expert': False, 'moe_eval_capacity_token_fraction': -1.0, 'moe_normalize_expert_grad': 'world_size', 'record_a2a_perf_stats': False, 'dummy_a2a': False, 'moe_batch_prioritized_routing': False, 'use_xmoe': True, 'use_mhmoe': True, 'mhmoe_heads_number': 2, 'flash_attention': False, 'xpos_rel_pos': True, 'scale_length': 2048, 'add_bos_token': False, 'tokens_per_sample': 2048, 'max_target_positions': None, 'tpu': False, 'memory_efficient_fp16': True, 'fp16': True, 'fp16_no_flatten_grads': False, 'ddp_backend': 'c10d', 'world_size': 2, 'distributed_rank': 0, 'ddp_rank': 0, 'deepnorm': False, 'subln': True, 'rel_pos_buckets': 0, 'max_rel_pos': 0, 'group_norm_size': 1, 'model_parallel_size': 1}, 'task': {'_name': 'gpt', 'data': '/mnt/msranlp/shaohanh/data/redstone_v2_1_config/', 'sample_break_mode': 'none', 'tokens_per_sample': 2048, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_source_positions': None, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': 2, 'batch_size_valid': 2, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'spm_model': '', 'tiktoken_model': 'cl100k_base', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe', 'dict_path': '/mnt/msranlp/shaohanh/exp/unigpt_exp/data/tiktoken/cl100k_w_code_dict.txt', 'batch_read_ahead': 10000, 'pad_to_max_len': True, 'absolute_path': False, 'required_batch_size_multiple': 1}, 'criterion': {'_name': 'moe_cross_entropy', 'moe_gate_loss_wt': 0.01, 'moe_gate_loss_combine_method': 'sum', 'moe_gate_loss_transform': 'none', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0006], 'block_wise': False}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 375, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 300000.0, 'lr': [0.0006]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2023-06-30 14:17:14 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-06-30 14:17:14 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.
2023-06-30 14:17:14 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:3 to store for rank: 0
2023-06-30 14:17:14 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 2 nodes.
2023-06-30 14:17:14 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:4 to store for rank: 0
2023-06-30 14:17:14 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 2 nodes.
2023-06-30 14:17:14 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:5 to store for rank: 0
2023-06-30 14:17:14 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 2 nodes.
2023-06-30 14:17:16 | INFO | fairseq_cli.train | LanguageModel(
  (decoder): LMDecoder(
    (dropout_module): Dropout(p=0.1, inplace=False)
    (embed_tokens): Embedding(100287, 768, padding_idx=1)
    (output_projection): Linear(in_features=768, out_features=100287, bias=False)
    (layers): ModuleList(
      (0): DecoderLayer(
        (dropout_module): Dropout(p=0.1, inplace=False)
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
          (inner_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (xpos): XPOS()
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn): FeedForwardNetwork(
          (activation_dropout_module): Dropout(p=0.0, inplace=False)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (dropout_module): Dropout(p=0.1, inplace=False)
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
          (inner_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (xpos): XPOS()
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (moe_layer): MH_MOELayer(
          (gate): Top2Gate(
            (wg_reduction): Linear(in_features=384, out_features=16, bias=False)
          )
          (experts): ModuleList(
            (0): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
            (1): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
            (2): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
            (3): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
          )
          (multi_heads): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (dropout_module): Dropout(p=0.1, inplace=False)
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
          (inner_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (xpos): XPOS()
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn): FeedForwardNetwork(
          (activation_dropout_module): Dropout(p=0.0, inplace=False)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (dropout_module): Dropout(p=0.1, inplace=False)
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
          (inner_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (xpos): XPOS()
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (moe_layer): MH_MOELayer(
          (gate): Top2Gate(
            (wg_reduction): Linear(in_features=384, out_features=16, bias=False)
          )
          (experts): ModuleList(
            (0): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
            (1): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
            (2): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
            (3): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
          )
          (multi_heads): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (dropout_module): Dropout(p=0.1, inplace=False)
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
          (inner_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (xpos): XPOS()
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn): FeedForwardNetwork(
          (activation_dropout_module): Dropout(p=0.0, inplace=False)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (dropout_module): Dropout(p=0.1, inplace=False)
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
          (inner_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (xpos): XPOS()
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (moe_layer): MH_MOELayer(
          (gate): Top2Gate(
            (wg_reduction): Linear(in_features=384, out_features=16, bias=False)
          )
          (experts): ModuleList(
            (0): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
            (1): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
            (2): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
            (3): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
          )
          (multi_heads): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (dropout_module): Dropout(p=0.1, inplace=False)
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
          (inner_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (xpos): XPOS()
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn): FeedForwardNetwork(
          (activation_dropout_module): Dropout(p=0.0, inplace=False)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (dropout_module): Dropout(p=0.1, inplace=False)
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
          (inner_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (xpos): XPOS()
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (moe_layer): MH_MOELayer(
          (gate): Top2Gate(
            (wg_reduction): Linear(in_features=384, out_features=16, bias=False)
          )
          (experts): ModuleList(
            (0): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
            (1): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
            (2): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
            (3): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
          )
          (multi_heads): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (dropout_module): Dropout(p=0.1, inplace=False)
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
          (inner_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (xpos): XPOS()
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn): FeedForwardNetwork(
          (activation_dropout_module): Dropout(p=0.0, inplace=False)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (dropout_module): Dropout(p=0.1, inplace=False)
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
          (inner_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (xpos): XPOS()
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (moe_layer): MH_MOELayer(
          (gate): Top2Gate(
            (wg_reduction): Linear(in_features=384, out_features=16, bias=False)
          )
          (experts): ModuleList(
            (0): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
            (1): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
            (2): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
            (3): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
          )
          (multi_heads): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (dropout_module): Dropout(p=0.1, inplace=False)
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
          (inner_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (xpos): XPOS()
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn): FeedForwardNetwork(
          (activation_dropout_module): Dropout(p=0.0, inplace=False)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (dropout_module): Dropout(p=0.1, inplace=False)
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
          (inner_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (xpos): XPOS()
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (moe_layer): MH_MOELayer(
          (gate): Top2Gate(
            (wg_reduction): Linear(in_features=384, out_features=16, bias=False)
          )
          (experts): ModuleList(
            (0): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
            (1): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
            (2): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
            (3): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
          )
          (multi_heads): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
  )
)
2023-06-30 14:17:16 | INFO | fairseq_cli.train | task: GPTPretrainingTask
2023-06-30 14:17:16 | INFO | fairseq_cli.train | model: LanguageModel
2023-06-30 14:17:16 | INFO | fairseq_cli.train | criterion: MoECrossEntropyCriterion
2023-06-30 14:17:16 | INFO | fairseq_cli.train | num. non-expert model params: 140,921,856 (num. trained: 140,921,856)
2023-06-30 14:17:16 | INFO | fairseq_cli.train | num. expert model params: 113,485,824 (num. trained: 113,485,824)
2023-06-30 14:17:17 | INFO | fairseq_cli.train | nvidia-smi stats: {'gpu_0_mem_used_gb': 18.9755859375, 'gpu_1_mem_used_gb': 19.3291015625, 'gpu_2_mem_used_gb': 13.3076171875, 'gpu_3_mem_used_gb': 13.2998046875, 'gpu_4_mem_used_gb': 13.3076171875, 'gpu_5_mem_used_gb': 13.2919921875, 'gpu_6_mem_used_gb': 13.3017578125, 'gpu_7_mem_used_gb': 13.2724609375, 'gpu_8_mem_used_gb': 13.1748046875, 'gpu_9_mem_used_gb': 13.3017578125, 'gpu_10_mem_used_gb': 13.3017578125, 'gpu_11_mem_used_gb': 13.2880859375, 'gpu_12_mem_used_gb': 13.2998046875, 'gpu_13_mem_used_gb': 13.3017578125, 'gpu_14_mem_used_gb': 13.3017578125, 'gpu_15_mem_used_gb': 13.2216796875}
2023-06-30 14:17:17 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2023-06-30 14:17:17 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.1.moe_layer.gate.wg_reduction.bias
2023-06-30 14:17:17 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.3.moe_layer.gate.wg_reduction.bias
2023-06-30 14:17:17 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.5.moe_layer.gate.wg_reduction.bias
2023-06-30 14:17:17 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.7.moe_layer.gate.wg_reduction.bias
2023-06-30 14:17:17 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.9.moe_layer.gate.wg_reduction.bias
2023-06-30 14:17:17 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.11.moe_layer.gate.wg_reduction.bias
2023-06-30 14:17:18 | INFO | fairseq.trainer | nvidia-smi stats: {'gpu_0_mem_used_gb': 18.9755859375, 'gpu_1_mem_used_gb': 19.3291015625, 'gpu_2_mem_used_gb': 13.3076171875, 'gpu_3_mem_used_gb': 13.2998046875, 'gpu_4_mem_used_gb': 13.3076171875, 'gpu_5_mem_used_gb': 13.2919921875, 'gpu_6_mem_used_gb': 13.3017578125, 'gpu_7_mem_used_gb': 13.2724609375, 'gpu_8_mem_used_gb': 13.1748046875, 'gpu_9_mem_used_gb': 13.3017578125, 'gpu_10_mem_used_gb': 13.3017578125, 'gpu_11_mem_used_gb': 13.2880859375, 'gpu_12_mem_used_gb': 13.2998046875, 'gpu_13_mem_used_gb': 13.3017578125, 'gpu_14_mem_used_gb': 13.3017578125, 'gpu_15_mem_used_gb': 13.2216796875}
2023-06-30 14:17:18 | INFO | fairseq.utils | ***********************CUDA enviroments for all 2 workers***********************
2023-06-30 14:17:18 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM3-32GB                    
2023-06-30 14:17:18 | INFO | fairseq.utils | rank   1: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM3-32GB                    
2023-06-30 14:17:18 | INFO | fairseq.utils | ***********************CUDA enviroments for all 2 workers***********************
2023-06-30 14:17:18 | INFO | fairseq_cli.train | training on 2 devices (GPUs/TPUs)
2023-06-30 14:17:18 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 2
2023-06-30 14:17:19 | INFO | fairseq_cli.train | nvidia-smi stats: {'gpu_0_mem_used_gb': 19.1474609375, 'gpu_1_mem_used_gb': 19.5009765625, 'gpu_2_mem_used_gb': 13.3076171875, 'gpu_3_mem_used_gb': 13.2998046875, 'gpu_4_mem_used_gb': 13.3076171875, 'gpu_5_mem_used_gb': 13.2919921875, 'gpu_6_mem_used_gb': 13.3017578125, 'gpu_7_mem_used_gb': 13.2724609375, 'gpu_8_mem_used_gb': 13.1748046875, 'gpu_9_mem_used_gb': 13.3017578125, 'gpu_10_mem_used_gb': 13.3017578125, 'gpu_11_mem_used_gb': 13.2880859375, 'gpu_12_mem_used_gb': 13.2998046875, 'gpu_13_mem_used_gb': 13.3017578125, 'gpu_14_mem_used_gb': 13.3017578125, 'gpu_15_mem_used_gb': 13.2216796875}
2023-06-30 14:17:19 | INFO | fairseq.trainer | No existing checkpoint found /mnt1/msranlpintern/wuxun/MoE/MoE_results/test_moe/small-baseline-redstone_v2-flash_attn-8experts/checkpoint_last-rank-0.pt
2023-06-30 14:17:19 | INFO | fairseq.trainer | loading train data for epoch 1
2023-06-30 14:17:35 | INFO | fairseq.optim.adam | using FusedAdam
2023-06-30 14:17:35 | INFO | fairseq.trainer | begin training epoch 1
2023-06-30 14:17:35 | INFO | fairseq_cli.train | Start iterating over samples
2023-06-30 14:17:36 | WARNING | infinibatch.iterators | trying to fetch item, but prefetch buffer is empty
2023-06-30 14:18:22 | WARNING | infinibatch.iterators | trying to fetch item, but prefetch buffer is empty
2023-06-30 14:18:22 | WARNING | infinibatch.iterators | trying to fetch item, but prefetch buffer is empty
2023-06-30 14:18:22 | WARNING | infinibatch.iterators | trying to fetch item, but prefetch buffer is empty
2023-06-30 14:18:22 | WARNING | infinibatch.iterators | trying to fetch item, but prefetch buffer is empty
2023-06-30 14:18:22 | WARNING | infinibatch.iterators | trying to fetch item, but prefetch buffer is empty
2023-06-30 14:18:22 | WARNING | infinibatch.iterators | trying to fetch item, but prefetch buffer is empty
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v2:dispatched_input_v3:  torch.Size([2, 4, 1024, 768])
torch.Size([16384, 384])
dispatched_input_v3: chunks:torch.Size([2, 4, 1024, 768])
 chunks: 4
4
self.num_local_experts self.num_local_experts 4
4
self.experts self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts expert_outputv2: torch.Size([2, 4, 1024, 768])
4self.all2all_size  2 d_model self.num_local_experts 768
4 expert_outputv3: d_model torch.Size([8, 1024, 768])
768
combine_weights expert_outputv3: torch.Size([8192, 8, 2048])
torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weightsdispatched_input_v1:  torch.Size([16384, 384])
torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: dispatched_input_v1: torch.Size([2, 4, 1024, 768])
torch.Size([16384, 384])
chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weightsdispatched_input_v1:  torch.Size([16384, 384])
torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v1:dispatched_input_v3:  torch.Size([2, 4, 1024, 768])
torch.Size([16384, 384])
chunks: 4
self.num_local_experts 4
self.experts dispatched_input_v2:ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
 torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2:expert_outputv1:  torch.Size([2, 4, 1024, 768])torch.Size([8192, 384])

expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) dispatch_maskreshaped_input  torch.Size([8192, 384])
torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux dispatched_input_v1: torch.Size([])torch.Size([16384, 384])

dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)torch.Size([2, 4, 1024, 768])

chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_maskdispatch_mask  torch.Size([8, 2048, 8192])torch.Size([8192, 8, 2048]) combine_weights  torch.Size([8192, 8, 2048]) reshaped_inputL_aux  torch.Size([])
torch.Size([8192, 384])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input dispatched_input_v1: torch.Size([8192, 384])
torch.Size([16384, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v2: dispatched_input_v3: torch.Size([16384, 384])
torch.Size([2, 4, 1024, 768])dispatched_input_v3: 
torch.Size([2, 4, 1024, 768])
chunks: chunks:4
self.num_local_experts  4
4
self.experts self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: expert_outputv1:torch.Size([2, 4, 1024, 768]) 
torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])expert_outputv2: 
torch.Size([2, 4, 1024, 768])self.all2all_size
 2self.all2all_size  self.num_local_experts2  self.num_local_experts 4 4 d_model d_model 768
768
expert_outputv3:expert_outputv3:  torch.Size([8, 1024, 768])
torch.Size([8, 1024, 768])combine_weights torch.Size([8192, 8, 2048])

combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) dispatch_maskL_aux  torch.Size([])
torch.Size([8192, 8, 2048]) combine_weights dispatch_mask torch.Size([8192, 8, 2048])torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
 L_aux torch.Size([])
dispatched_input_v1: dispatch_mask torch.Size([16384, 384])
torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: dispatched_input_v2:4
 self.num_local_experts torch.Size([16384, 384])
4
dispatched_input_v3: self.experts torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 expert_outputv2:d_model  768
torch.Size([2, 4, 1024, 768])
expert_outputv3: self.all2all_size torch.Size([8, 1024, 768])
2 combine_weightsself.num_local_experts  4 torch.Size([8192, 8, 2048])
d_model 768combined_output v2: torch.Size([8192, 384])

expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
2023-06-30 14:18:24 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 784.00 MiB (GPU 1; 31.75 GiB total capacity; 10.74 GiB already allocated; 403.25 MiB free; 11.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2023-06-30 14:18:24 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:24 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9427 MB |   10994 MB |   26998 MB |   17571 MB |
|       from large pool |    9395 MB |   10962 MB |   26766 MB |   17370 MB |
|       from small pool |      31 MB |      35 MB |     231 MB |     200 MB |
|---------------------------------------------------------------------------|
| Active memory         |    9427 MB |   10994 MB |   26998 MB |   17571 MB |
|       from large pool |    9395 MB |   10962 MB |   26766 MB |   17370 MB |
|       from small pool |      31 MB |      35 MB |     231 MB |     200 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   11628 MB |   11870 MB |   13438 MB |    1810 MB |
|       from large pool |   11594 MB |   11834 MB |   13402 MB |    1808 MB |
|       from small pool |      34 MB |      36 MB |      36 MB |       2 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  648093 KB |     859 MB |   10878 MB |   10245 MB |
|       from large pool |  645233 KB |     857 MB |   10642 MB |   10012 MB |
|       from small pool |    2859 KB |       5 MB |     235 MB |     232 MB |
|---------------------------------------------------------------------------|
| Allocations           |    1126    |    1133    |    2384    |    1258    |
|       from large pool |     492    |     497    |     876    |     384    |
|       from small pool |     634    |     642    |    1508    |     874    |
|---------------------------------------------------------------------------|
| Active allocs         |    1126    |    1133    |    2384    |    1258    |
|       from large pool |     492    |     498    |     876    |     384    |
|       from small pool |     634    |     642    |    1508    |     874    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      59    |      62    |      63    |       4    |
|       from large pool |      42    |      44    |      45    |       3    |
|       from small pool |      17    |      18    |      18    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      34    |      35    |     779    |     745    |
|       from large pool |      14    |      15    |     302    |     288    |
|       from small pool |      20    |      21    |     477    |     457    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:24 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2023-06-30 14:18:24 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 784.00 MiB (GPU 0; 31.75 GiB total capacity; 10.74 GiB already allocated; 765.25 MiB free; 11.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2023-06-30 14:18:24 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9429 MB |   10996 MB |   27001 MB |   17572 MB |
|       from large pool |    9397 MB |   10964 MB |   26769 MB |   17371 MB |
|       from small pool |      31 MB |      35 MB |     231 MB |     200 MB |
|---------------------------------------------------------------------------|
| Active memory         |    9429 MB |   10996 MB |   27001 MB |   17572 MB |
|       from large pool |    9397 MB |   10964 MB |   26769 MB |   17371 MB |
|       from small pool |      31 MB |      35 MB |     231 MB |     200 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   11628 MB |   11870 MB |   13438 MB |    1810 MB |
|       from large pool |   11594 MB |   11834 MB |   13402 MB |    1808 MB |
|       from small pool |      34 MB |      36 MB |      36 MB |       2 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  646045 KB |     857 MB |   10403 MB |    9772 MB |
|       from large pool |  643185 KB |     855 MB |   10167 MB |    9539 MB |
|       from small pool |    2859 KB |       5 MB |     235 MB |     232 MB |
|---------------------------------------------------------------------------|
| Allocations           |    1126    |    1133    |    2384    |    1258    |
|       from large pool |     492    |     497    |     876    |     384    |
|       from small pool |     634    |     642    |    1508    |     874    |
|---------------------------------------------------------------------------|
| Active allocs         |    1126    |    1133    |    2384    |    1258    |
|       from large pool |     492    |     497    |     876    |     384    |
|       from small pool |     634    |     642    |    1508    |     874    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      59    |      62    |      63    |       4    |
|       from large pool |      42    |      44    |      45    |       3    |
|       from small pool |      17    |      18    |      18    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      35    |      36    |     772    |     737    |
|       from large pool |      15    |      16    |     295    |     280    |
|       from small pool |      20    |      21    |     477    |     457    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:24 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:24 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input dispatch_masktorch.Size([8192, 384])
 torch.Size([8192, 8, 2048]) combine_weightsdispatched_input_v1:  torch.Size([16384, 384])torch.Size([8192, 8, 2048]) 
L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v2:dispatched_input_v1:  torch.Size([16384, 384])
torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
dispatched_input_v2: chunks:torch.Size([16384, 384])
 dispatched_input_v3: 4
torch.Size([2, 4, 1024, 768])
self.num_local_experts chunks:4
 self.experts 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1:expert_outputv1:  torch.Size([2, 4, 1024, 768])
torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
expert_outputv2:self.all2all_size  torch.Size([2, 4, 1024, 768])
2 self.all2all_sizeself.num_local_experts  2 self.num_local_experts 4 4d_model  768
d_modelexpert_outputv3:  torch.Size([8, 1024, 768])
768
combine_weights expert_outputv3: torch.Size([8192, 8, 2048])
torch.Size([8, 1024, 768])
combine_weightscombined_output v2:  torch.Size([8192, 384])
torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_maskdispatch_mask  torch.Size([8, 2048, 8192]) torch.Size([8192, 8, 2048]) reshaped_input combine_weights torch.Size([8192, 384])
torch.Size([8192, 8, 2048]) L_auxdispatched_input_v1: torch.Size([16384, 384])
 torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input dispatched_input_v2: torch.Size([8192, 384])
torch.Size([16384, 384])
dispatched_input_v3:dispatched_input_v1:  torch.Size([16384, 384])
torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 expert_outputv1:d_model  768
torch.Size([2, 4, 1024, 768])
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2:expert_outputv2: torch.Size([8192, 384])
 torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input dispatched_input_v2: torch.Size([8192, 384])
torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
dispatched_input_v1: torch.Size([16384, 384])chunks: 
4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: expert_outputv1: torch.Size([8192, 384])
torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1:dispatch_mask  torch.Size([16384, 384])
torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_inputdispatched_input_v2:  torch.Size([16384, 384])
torch.Size([8192, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
dispatched_input_v1: chunks:torch.Size([16384, 384]) 
4
self.num_local_experts 4
self.experts dispatched_input_v2: ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)torch.Size([16384, 384])

dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4expert_outputv1:  torch.Size([2, 4, 1024, 768])d_model 768

expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
expert_outputv2: torch.Size([2, 4, 1024, 768])
combined_output v2:self.all2all_size  2 torch.Size([8192, 384])
self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask expert_outputv2:torch.Size([8, 2048, 8192])  reshaped_inputtorch.Size([2, 4, 1024, 768])
 self.all2all_size torch.Size([8192, 384])
2 self.num_local_experts dispatched_input_v1:4  d_model torch.Size([16384, 384])768

expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
dispatched_input_v2:combined_output v2:  torch.Size([8192, 384])torch.Size([16384, 384])

dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts dispatch_mask ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts expert_outputv1: torch.Size([2, 4, 1024, 768])
ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
2023-06-30 14:18:24 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.53 GiB (GPU 0; 31.75 GiB total capacity; 9.21 GiB already allocated; 1.25 GiB free; 10.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2023-06-30 14:18:24 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 4         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9428 MB |   10996 MB |   50963 MB |   41535 MB |
|       from large pool |    9396 MB |   10964 MB |   50500 MB |   41103 MB |
|       from small pool |      31 MB |      35 MB |     462 MB |     431 MB |
|---------------------------------------------------------------------------|
| Active memory         |    9428 MB |   10996 MB |   50963 MB |   41535 MB |
|       from large pool |    9396 MB |   10964 MB |   50500 MB |   41103 MB |
|       from small pool |      31 MB |      35 MB |     462 MB |     431 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   11116 MB |   11900 MB |   15792 MB |    4676 MB |
|       from large pool |   11082 MB |   11866 MB |   15754 MB |    4672 MB |
|       from small pool |      34 MB |      36 MB |      38 MB |       4 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1687 MB |    2456 MB |   26833 MB |   25146 MB |
|       from large pool |    1685 MB |    2445 MB |   26350 MB |   24665 MB |
|       from small pool |       2 MB |      13 MB |     483 MB |     480 MB |
|---------------------------------------------------------------------------|
| Allocations           |    1129    |    1135    |    4357    |    3228    |
|       from large pool |     492    |     497    |    1627    |    1135    |
|       from small pool |     637    |     653    |    2730    |    2093    |
|---------------------------------------------------------------------------|
| Active allocs         |    1129    |    1135    |    4357    |    3228    |
|       from large pool |     492    |     499    |    1627    |    1135    |
|       from small pool |     637    |     653    |    2730    |    2093    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      58    |      62    |      66    |       8    |
|       from large pool |      41    |      44    |      47    |       6    |
|       from small pool |      17    |      18    |      19    |       2    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      33    |      62    |    1799    |    1766    |
|       from large pool |      15    |      21    |     668    |     653    |
|       from small pool |      18    |      42    |    1131    |    1113    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:24 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:24 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2023-06-30 14:18:24 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.53 GiB (GPU 1; 31.75 GiB total capacity; 9.21 GiB already allocated; 915.25 MiB free; 10.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2023-06-30 14:18:24 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:24 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 4         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9428 MB |   10994 MB |   50960 MB |   41532 MB |
|       from large pool |    9396 MB |   10962 MB |   50497 MB |   41100 MB |
|       from small pool |      31 MB |      35 MB |     462 MB |     431 MB |
|---------------------------------------------------------------------------|
| Active memory         |    9428 MB |   10994 MB |   50960 MB |   41532 MB |
|       from large pool |    9396 MB |   10962 MB |   50497 MB |   41100 MB |
|       from small pool |      31 MB |      35 MB |     462 MB |     431 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   11116 MB |   11900 MB |   15792 MB |    4676 MB |
|       from large pool |   11082 MB |   11866 MB |   15754 MB |    4672 MB |
|       from small pool |      34 MB |      36 MB |      38 MB |       4 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1687 MB |    2456 MB |   27306 MB |   25619 MB |
|       from large pool |    1685 MB |    2445 MB |   26823 MB |   25138 MB |
|       from small pool |       2 MB |      13 MB |     483 MB |     480 MB |
|---------------------------------------------------------------------------|
| Allocations           |    1129    |    1135    |    4357    |    3228    |
|       from large pool |     492    |     497    |    1627    |    1135    |
|       from small pool |     637    |     653    |    2730    |    2093    |
|---------------------------------------------------------------------------|
| Active allocs         |    1129    |    1135    |    4357    |    3228    |
|       from large pool |     492    |     499    |    1627    |    1135    |
|       from small pool |     637    |     653    |    2730    |    2093    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      58    |      62    |      66    |       8    |
|       from large pool |      41    |      44    |      47    |       6    |
|       from small pool |      17    |      18    |      19    |       2    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      33    |      62    |    1808    |    1775    |
|       from large pool |      15    |      21    |     677    |     662    |
|       from small pool |      18    |      42    |    1131    |    1113    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:24 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048])dispatch_mask  torch.Size([8192, 8, 2048])L_aux torch.Size([]) 
combine_weights dispatch_mask torch.Size([8192, 8, 2048])torch.Size([8, 2048, 8192]) reshaped_input  L_auxtorch.Size([8192, 384])
 torch.Size([])
dispatched_input_v1: dispatch_masktorch.Size([16384, 384])
 torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
dispatched_input_v2: self.num_local_experts torch.Size([16384, 384])
4
dispatched_input_v3: self.experts torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)self.experts
 ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: expert_outputv1:torch.Size([2, 4, 1024, 768]) 
torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model expert_outputv2:768 torch.Size([2, 4, 1024, 768])

self.all2all_sizeexpert_outputv3:  torch.Size([8, 1024, 768])
2combine_weights  self.num_local_expertstorch.Size([8192, 8, 2048])
 4 d_model combined_output v2: 768
torch.Size([8192, 384])
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weightsdispatched_input_v1:  torch.Size([16384, 384])
torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v1:dispatched_input_v3: torch.Size([16384, 384]) torch.Size([2, 4, 1024, 768])

chunks: 4
self.num_local_experts 4
self.experts dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)torch.Size([2, 4, 1024, 768])

chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
expert_outputv1:combine_weights  torch.Size([8192, 8, 2048])
torch.Size([2, 4, 1024, 768])
combined_output v2: torch.Size([8192, 384])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights dispatch_masktorch.Size([8192, 8, 2048])  L_aux torch.Size([8192, 8, 2048])torch.Size([])
 combine_weightsdispatch_mask  torch.Size([8, 2048, 8192]) torch.Size([8192, 8, 2048]) reshaped_inputL_aux torch.Size([])
 dispatch_mask torch.Size([8192, 384])
torch.Size([8, 2048, 8192]) reshaped_input dispatched_input_v1: torch.Size([8192, 384])
torch.Size([16384, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
dispatched_input_v2:chunks:  torch.Size([16384, 384])
dispatched_input_v3: 4
torch.Size([2, 4, 1024, 768])
self.num_local_experts 4chunks: 
4
self.experts self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768expert_outputv2:
expert_outputv3:  torch.Size([2, 4, 1024, 768])torch.Size([8, 1024, 768])
self.all2all_size 2
 self.num_local_expertscombine_weights 4 d_model  768
torch.Size([8192, 8, 2048])
expert_outputv3: torch.Size([8, 1024, 768])
combined_output v2: combine_weights torch.Size([8192, 384])
torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_maskdispatch_mask  torch.Size([8192, 8, 2048])torch.Size([8192, 8, 2048])  combine_weightscombine_weights  torch.Size([8192, 8, 2048]) torch.Size([8192, 8, 2048]) L_aux L_aux torch.Size([])torch.Size([])

dispatch_mask dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
torch.Size([8, 2048, 8192]) reshaped_input dispatched_input_v1: torch.Size([16384, 384])
torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
dispatched_input_v2: chunks: torch.Size([16384, 384])
4dispatched_input_v3: 
self.num_local_expertstorch.Size([2, 4, 1024, 768])
 4
chunks: self.experts 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv1:expert_outputv2:  torch.Size([2, 4, 1024, 768])
torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3:expert_outputv2:  torch.Size([8, 1024, 768])
combine_weights torch.Size([2, 4, 1024, 768])torch.Size([8192, 8, 2048])

self.all2all_size 2combined_output v2:  self.num_local_expertstorch.Size([8192, 384])
 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux dispatch_masktorch.Size([])
 torch.Size([8192, 8, 2048])dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input  combine_weightstorch.Size([8192, 384])
 torch.Size([8192, 8, 2048]) L_aux dispatched_input_v1: torch.Size([])torch.Size([16384, 384])

dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v2:dispatched_input_v1:  torch.Size([16384, 384])
torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
dispatched_input_v2:self.experts  torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: expert_outputv1:torch.Size([2, 4, 1024, 768]) 
torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weightsexpert_outputv2: torch.Size([8192, 8, 2048]) 
torch.Size([2, 4, 1024, 768])
self.all2all_size combined_output v2: 2 torch.Size([8192, 384])
self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
2023-06-30 14:18:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.53 GiB (GPU 1; 31.75 GiB total capacity; 9.21 GiB already allocated; 915.25 MiB free; 10.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2023-06-30 14:18:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.53 GiB (GPU 0; 31.75 GiB total capacity; 9.21 GiB already allocated; 1.25 GiB free; 10.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2023-06-30 14:18:25 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:25 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 5         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9429 MB |   10996 MB |   74926 MB |   65497 MB |
|       from large pool |    9397 MB |   10964 MB |   74233 MB |   64835 MB |
|       from small pool |      31 MB |      35 MB |     693 MB |     662 MB |
|---------------------------------------------------------------------------|
| Active memory         |    9429 MB |   10996 MB |   74926 MB |   65497 MB |
|       from large pool |    9397 MB |   10964 MB |   74233 MB |   64835 MB |
|       from small pool |      31 MB |      35 MB |     693 MB |     662 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   11116 MB |   11900 MB |   15794 MB |    4678 MB |
|       from large pool |   11082 MB |   11866 MB |   15754 MB |    4672 MB |
|       from small pool |      34 MB |      36 MB |      40 MB |       6 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1686 MB |    2549 MB |   45255 MB |   43568 MB |
|       from large pool |    1684 MB |    2544 MB |   44524 MB |   42840 MB |
|       from small pool |       2 MB |      13 MB |     730 MB |     728 MB |
|---------------------------------------------------------------------------|
| Allocations           |    1129    |    1135    |    6330    |    5201    |
|       from large pool |     492    |     497    |    2378    |    1886    |
|       from small pool |     637    |     653    |    3952    |    3315    |
|---------------------------------------------------------------------------|
| Active allocs         |    1129    |    1137    |    6330    |    5201    |
|       from large pool |     492    |     499    |    2378    |    1886    |
|       from small pool |     637    |     653    |    3952    |    3315    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      58    |      62    |      67    |       9    |
|       from large pool |      41    |      44    |      47    |       6    |
|       from small pool |      17    |      18    |      20    |       3    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      36    |      67    |    2810    |    2774    |
|       from large pool |      18    |      25    |    1050    |    1032    |
|       from small pool |      18    |      42    |    1760    |    1742    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:25 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 5         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9429 MB |   10994 MB |   74922 MB |   65493 MB |
|       from large pool |    9397 MB |   10962 MB |   74229 MB |   64831 MB |
|       from small pool |      31 MB |      35 MB |     693 MB |     662 MB |
|---------------------------------------------------------------------------|
| Active memory         |    9429 MB |   10994 MB |   74922 MB |   65493 MB |
|       from large pool |    9397 MB |   10962 MB |   74229 MB |   64831 MB |
|       from small pool |      31 MB |      35 MB |     693 MB |     662 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   11116 MB |   11900 MB |   15794 MB |    4678 MB |
|       from large pool |   11082 MB |   11866 MB |   15754 MB |    4672 MB |
|       from small pool |      34 MB |      36 MB |      40 MB |       6 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1686 MB |    2549 MB |   45727 MB |   44040 MB |
|       from large pool |    1684 MB |    2544 MB |   44996 MB |   43312 MB |
|       from small pool |       2 MB |      13 MB |     730 MB |     728 MB |
|---------------------------------------------------------------------------|
| Allocations           |    1129    |    1135    |    6330    |    5201    |
|       from large pool |     492    |     497    |    2378    |    1886    |
|       from small pool |     637    |     653    |    3952    |    3315    |
|---------------------------------------------------------------------------|
| Active allocs         |    1129    |    1137    |    6330    |    5201    |
|       from large pool |     492    |     499    |    2378    |    1886    |
|       from small pool |     637    |     653    |    3952    |    3315    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      58    |      62    |      67    |       9    |
|       from large pool |      41    |      44    |      47    |       6    |
|       from small pool |      17    |      18    |      20    |       3    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      36    |      67    |    2821    |    2785    |
|       from large pool |      18    |      25    |    1061    |    1043    |
|       from small pool |      18    |      42    |    1760    |    1742    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2023-06-30 14:18:25 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) dispatch_maskL_aux  torch.Size([])torch.Size([8192, 8, 2048])
 combine_weights dispatch_mask torch.Size([8192, 8, 2048]) torch.Size([8, 2048, 8192]) L_aux reshaped_input torch.Size([])torch.Size([8192, 384])

dispatch_mask dispatched_input_v1: torch.Size([8, 2048, 8192]) torch.Size([16384, 384])
reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4dispatched_input_v2:
 self.num_local_expertstorch.Size([16384, 384])
 dispatched_input_v3: 4torch.Size([2, 4, 1024, 768])

chunks: self.experts 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: expert_outputv1:torch.Size([2, 4, 1024, 768])
 self.all2all_sizetorch.Size([2, 4, 1024, 768]) 
2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
expert_outputv2: torch.Size([2, 4, 1024, 768])
combined_output v2: self.all2all_size torch.Size([8192, 384])
2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_auxdispatch_mask torch.Size([])
 dispatch_mask torch.Size([8192, 8, 2048]) torch.Size([8, 2048, 8192]) combine_weights reshaped_input torch.Size([8192, 8, 2048]) torch.Size([8192, 384])
L_aux torch.Size([])
dispatched_input_v1: dispatch_mask torch.Size([16384, 384])
torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: dispatched_input_v2: torch.Size([16384, 384])
torch.Size([16384, 384])dispatched_input_v3: 
torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts dispatched_input_v2: ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weightsdispatch_mask  torch.Size([8192, 8, 2048]) torch.Size([8192, 8, 2048])L_aux  torch.Size([])
combine_weights dispatch_mask torch.Size([8192, 8, 2048])torch.Size([8, 2048, 8192]) reshaped_input  L_auxtorch.Size([8192, 384])
 torch.Size([])
dispatched_input_v1:dispatch_mask  torch.Size([8, 2048, 8192]) torch.Size([16384, 384])
reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_expertsdispatched_input_v2:  4torch.Size([16384, 384])

dispatched_input_v3: self.experts torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)

self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: expert_outputv1:torch.Size([2, 4, 1024, 768]) 
torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv2:expert_outputv3:  torch.Size([8, 1024, 768])
torch.Size([2, 4, 1024, 768])
combine_weights self.all2all_size torch.Size([8192, 8, 2048])
2 self.num_local_experts 4combined_output v2:  torch.Size([8192, 384])
d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weightsdispatch_mask  torch.Size([8192, 8, 2048]) torch.Size([8192, 8, 2048])L_aux  combine_weightstorch.Size([])
 torch.Size([8192, 8, 2048]) dispatch_mask L_aux torch.Size([8, 2048, 8192]) torch.Size([])reshaped_input 
torch.Size([8192, 384])
dispatch_mask torch.Size([8, 2048, 8192]) dispatched_input_v1: reshaped_input torch.Size([16384, 384])
torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks:dispatched_input_v2:  torch.Size([16384, 384])
4
dispatched_input_v3:self.num_local_experts 4
 self.experts torch.Size([2, 4, 1024, 768])
chunks: ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)4

self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv1: expert_outputv2: torch.Size([2, 4, 1024, 768])torch.Size([2, 4, 1024, 768])

self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv2: expert_outputv3: torch.Size([2, 4, 1024, 768])
torch.Size([8, 1024, 768])self.all2all_size 
2 combine_weights self.num_local_experts torch.Size([8192, 8, 2048])
4 d_model 768combined_output v2: 
expert_outputv3:torch.Size([8192, 384])
 torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux dispatch_masktorch.Size([])
 dispatch_mask torch.Size([8192, 8, 2048])torch.Size([8, 2048, 8192])  combine_weightsreshaped_input  torch.Size([8192, 384])
torch.Size([8192, 8, 2048]) L_aux dispatched_input_v1: torch.Size([])torch.Size([16384, 384])

dispatch_mask torch.Size([8, 2048, 8192]) dispatched_input_v2: reshaped_inputtorch.Size([16384, 384])
 dispatched_input_v3: torch.Size([8192, 384])torch.Size([2, 4, 1024, 768])

chunks: 4
self.num_local_experts 4dispatched_input_v1: 
torch.Size([16384, 384])self.experts 
ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: dispatch_masktorch.Size([16384, 384])
 torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatched_input_v2: dispatch_mask torch.Size([16384, 384])
torch.Size([8, 2048, 8192]) dispatched_input_v3: torch.Size([2, 4, 1024, 768])
reshaped_inputchunks:  4
torch.Size([8192, 384])
self.num_local_experts 4
self.experts dispatched_input_v1: torch.Size([16384, 384])
ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
2023-06-30 14:18:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.53 GiB (GPU 1; 31.75 GiB total capacity; 9.21 GiB already allocated; 915.25 MiB free; 10.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2023-06-30 14:18:25 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:25 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 6         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9429 MB |   10994 MB |   98885 MB |   89456 MB |
|       from large pool |    9397 MB |   10962 MB |   97960 MB |   88562 MB |
|       from small pool |      31 MB |      35 MB |     924 MB |     893 MB |
|---------------------------------------------------------------------------|
| Active memory         |    9429 MB |   10994 MB |   98885 MB |   89456 MB |
|       from large pool |    9397 MB |   10962 MB |   97960 MB |   88562 MB |
|       from small pool |      31 MB |      35 MB |     924 MB |     893 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   11116 MB |   11900 MB |   15796 MB |    4680 MB |
|       from large pool |   11082 MB |   11866 MB |   15754 MB |    4672 MB |
|       from small pool |      34 MB |      36 MB |      42 MB |       8 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1686 MB |    2549 MB |   63916 MB |   62229 MB |
|       from large pool |    1684 MB |    2544 MB |   62937 MB |   61253 MB |
|       from small pool |       2 MB |      13 MB |     978 MB |     975 MB |
|---------------------------------------------------------------------------|
| Allocations           |    1129    |    1135    |    8303    |    7174    |
|       from large pool |     492    |     497    |    3129    |    2637    |
|       from small pool |     637    |     653    |    5174    |    4537    |
|---------------------------------------------------------------------------|
| Active allocs         |    1129    |    1138    |    8303    |    7174    |
|       from large pool |     492    |     499    |    3129    |    2637    |
|       from small pool |     637    |     653    |    5174    |    4537    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      58    |      62    |      68    |      10    |
|       from large pool |      41    |      44    |      47    |       6    |
|       from small pool |      17    |      18    |      21    |       4    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      33    |      68    |    3848    |    3815    |
|       from large pool |      18    |      25    |    1444    |    1426    |
|       from small pool |      15    |      44    |    2404    |    2389    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2023-06-30 14:18:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.53 GiB (GPU 0; 31.75 GiB total capacity; 9.21 GiB already allocated; 1.25 GiB free; 10.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2023-06-30 14:18:25 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 6         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9429 MB |   10996 MB |   98889 MB |   89460 MB |
|       from large pool |    9397 MB |   10964 MB |   97964 MB |   88566 MB |
|       from small pool |      31 MB |      35 MB |     924 MB |     893 MB |
|---------------------------------------------------------------------------|
| Active memory         |    9429 MB |   10996 MB |   98889 MB |   89460 MB |
|       from large pool |    9397 MB |   10964 MB |   97964 MB |   88566 MB |
|       from small pool |      31 MB |      35 MB |     924 MB |     893 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   11116 MB |   11900 MB |   15796 MB |    4680 MB |
|       from large pool |   11082 MB |   11866 MB |   15754 MB |    4672 MB |
|       from small pool |      34 MB |      36 MB |      42 MB |       8 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1686 MB |    2549 MB |   63444 MB |   61757 MB |
|       from large pool |    1684 MB |    2544 MB |   62465 MB |   60781 MB |
|       from small pool |       2 MB |      13 MB |     978 MB |     975 MB |
|---------------------------------------------------------------------------|
| Allocations           |    1129    |    1135    |    8303    |    7174    |
|       from large pool |     492    |     497    |    3129    |    2637    |
|       from small pool |     637    |     653    |    5174    |    4537    |
|---------------------------------------------------------------------------|
| Active allocs         |    1129    |    1137    |    8303    |    7174    |
|       from large pool |     492    |     499    |    3129    |    2637    |
|       from small pool |     637    |     653    |    5174    |    4537    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      58    |      62    |      68    |      10    |
|       from large pool |      41    |      44    |      47    |       6    |
|       from small pool |      17    |      18    |      21    |       4    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      33    |      68    |    3835    |    3802    |
|       from large pool |      18    |      25    |    1431    |    1413    |
|       from small pool |      15    |      44    |    2404    |    2389    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:25 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
dispatch_maskdispatch_mask  torch.Size([8192, 8, 2048]) torch.Size([8192, 8, 2048])combine_weights  torch.Size([8192, 8, 2048]) L_aux combine_weights torch.Size([])torch.Size([8192, 8, 2048]) 
L_aux dispatch_mask torch.Size([])torch.Size([8, 2048, 8192]) 
reshaped_input dispatch_mask torch.Size([8192, 384])
torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v2:dispatched_input_v3:  torch.Size([2, 4, 1024, 768])
torch.Size([16384, 384])
chunks: dispatched_input_v3: 4
torch.Size([2, 4, 1024, 768])self.num_local_experts 
4
chunks: self.experts 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2:expert_outputv1:  torch.Size([2, 4, 1024, 768])
torch.Size([2, 4, 1024, 768])self.all2all_size 2
 self.num_local_experts 4 d_model 768
expert_outputv3: expert_outputv2:torch.Size([8, 1024, 768])
 combine_weights torch.Size([2, 4, 1024, 768])torch.Size([8192, 8, 2048])

self.all2all_size 2combined_output v2:  self.num_local_expertstorch.Size([8192, 384])
 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
dispatch_mask torch.Size([8192, 8, 2048]) combined_output v2: combine_weights torch.Size([8192, 384])
torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048])dispatch_mask L_aux  torch.Size([])
torch.Size([8192, 8, 2048]) dispatch_mask combine_weightstorch.Size([8, 2048, 8192])  reshaped_input torch.Size([8192, 8, 2048]) torch.Size([8192, 384])
L_aux torch.Size([])
dispatched_input_v1: dispatch_mask torch.Size([16384, 384])
torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: dispatched_input_v2: torch.Size([16384, 384])
torch.Size([16384, 384])dispatched_input_v3: 
torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)torch.Size([2, 4, 1024, 768])

chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv1:expert_outputv3:  torch.Size([8, 1024, 768])
torch.Size([2, 4, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask dispatch_masktorch.Size([8192, 8, 2048]) torch.Size([8192, 8, 2048])  combine_weights combine_weightstorch.Size([8192, 8, 2048]) L_aux  torch.Size([])
torch.Size([8192, 8, 2048]) L_auxdispatch_mask  torch.Size([])torch.Size([8, 2048, 8192]) 
reshaped_input dispatch_mask torch.Size([8192, 384])
torch.Size([8, 2048, 8192]) reshaped_input dispatched_input_v1: torch.Size([8192, 384])
torch.Size([16384, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v2: dispatched_input_v3: torch.Size([16384, 384])
torch.Size([2, 4, 1024, 768])
dispatched_input_v3: chunks: torch.Size([2, 4, 1024, 768])4

self.num_local_experts 4chunks: 
4
self.experts self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: expert_outputv1: torch.Size([2, 4, 1024, 768])
torch.Size([2, 4, 1024, 768])self.all2all_size 
2 self.num_local_experts 4 d_model 768
expert_outputv3: expert_outputv2:torch.Size([8, 1024, 768])
 combine_weights torch.Size([2, 4, 1024, 768])
torch.Size([8192, 8, 2048])
self.all2all_size 2 combined_output v2: self.num_local_experts torch.Size([8192, 384])
4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask dispatch_masktorch.Size([8192, 8, 2048])  combine_weights torch.Size([8192, 8, 2048]) torch.Size([8192, 8, 2048]) combine_weights L_aux torch.Size([8192, 8, 2048]) torch.Size([])
L_aux torch.Size([])dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input
 torch.Size([8192, 384])
dispatch_mask dispatched_input_v1: torch.Size([8, 2048, 8192]) torch.Size([16384, 384])
reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
dispatched_input_v2:self.num_local_experts  4
torch.Size([16384, 384])
self.experts dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
self.num_local_experts
 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2:expert_outputv1:  torch.Size([2, 4, 1024, 768])
torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: expert_outputv2: torch.Size([8, 1024, 768])
torch.Size([2, 4, 1024, 768])
combine_weights self.all2all_size torch.Size([8192, 8, 2048])
2 self.num_local_experts 4combined_output v2:  d_modeltorch.Size([8192, 384])
 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weightsdispatched_input_v1:  torch.Size([16384, 384])
torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
dispatched_input_v1: chunks: torch.Size([16384, 384])
4
self.num_local_experts 4
self.experts dispatched_input_v2: torch.Size([16384, 384])
ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)dispatched_input_v3: 
torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])expert_outputv1:
 torch.Size([2, 4, 1024, 768])
combined_output v2: torch.Size([8192, 384])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
2023-06-30 14:18:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.53 GiB (GPU 0; 31.75 GiB total capacity; 9.21 GiB already allocated; 1.25 GiB free; 10.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2023-06-30 14:18:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.53 GiB (GPU 1; 31.75 GiB total capacity; 9.21 GiB already allocated; 915.25 MiB free; 10.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2023-06-30 14:18:26 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 7         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9429 MB |   10996 MB |  122851 MB |  113422 MB |
|       from large pool |    9397 MB |   10964 MB |  121696 MB |  112298 MB |
|       from small pool |      31 MB |      35 MB |    1155 MB |    1124 MB |
|---------------------------------------------------------------------------|
| Active memory         |    9429 MB |   10996 MB |  122851 MB |  113422 MB |
|       from large pool |    9397 MB |   10964 MB |  121696 MB |  112298 MB |
|       from small pool |      31 MB |      35 MB |    1155 MB |    1124 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   11116 MB |   11900 MB |   15798 MB |    4682 MB |
|       from large pool |   11082 MB |   11866 MB |   15754 MB |    4672 MB |
|       from small pool |      34 MB |      36 MB |      44 MB |      10 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1686 MB |    2549 MB |   81632 MB |   79945 MB |
|       from large pool |    1684 MB |    2544 MB |   80407 MB |   78723 MB |
|       from small pool |       2 MB |      13 MB |    1224 MB |    1222 MB |
|---------------------------------------------------------------------------|
| Allocations           |    1129    |    1135    |   10276    |    9147    |
|       from large pool |     492    |     497    |    3880    |    3388    |
|       from small pool |     637    |     653    |    6396    |    5759    |
|---------------------------------------------------------------------------|
| Active allocs         |    1129    |    1137    |   10276    |    9147    |
|       from large pool |     492    |     499    |    3880    |    3388    |
|       from small pool |     637    |     653    |    6396    |    5759    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      58    |      62    |      69    |      11    |
|       from large pool |      41    |      44    |      47    |       6    |
|       from small pool |      17    |      18    |      22    |       5    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      35    |      68    |    4843    |    4808    |
|       from large pool |      18    |      25    |    1814    |    1796    |
|       from small pool |      17    |      44    |    3029    |    3012    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:26 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:26 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2023-06-30 14:18:26 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 7         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9429 MB |   10994 MB |  122847 MB |  113418 MB |
|       from large pool |    9397 MB |   10962 MB |  121692 MB |  112294 MB |
|       from small pool |      31 MB |      35 MB |    1155 MB |    1124 MB |
|---------------------------------------------------------------------------|
| Active memory         |    9429 MB |   10994 MB |  122847 MB |  113418 MB |
|       from large pool |    9397 MB |   10962 MB |  121692 MB |  112294 MB |
|       from small pool |      31 MB |      35 MB |    1155 MB |    1124 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   11116 MB |   11900 MB |   15798 MB |    4682 MB |
|       from large pool |   11082 MB |   11866 MB |   15754 MB |    4672 MB |
|       from small pool |      34 MB |      36 MB |      44 MB |      10 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1686 MB |    2549 MB |   82104 MB |   80417 MB |
|       from large pool |    1684 MB |    2544 MB |   80879 MB |   79195 MB |
|       from small pool |       2 MB |      13 MB |    1224 MB |    1222 MB |
|---------------------------------------------------------------------------|
| Allocations           |    1129    |    1135    |   10276    |    9147    |
|       from large pool |     492    |     497    |    3880    |    3388    |
|       from small pool |     637    |     653    |    6396    |    5759    |
|---------------------------------------------------------------------------|
| Active allocs         |    1129    |    1138    |   10276    |    9147    |
|       from large pool |     492    |     499    |    3880    |    3388    |
|       from small pool |     637    |     653    |    6396    |    5759    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      58    |      62    |      69    |      11    |
|       from large pool |      41    |      44    |      47    |       6    |
|       from small pool |      17    |      18    |      22    |       5    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      35    |      68    |    4856    |    4821    |
|       from large pool |      18    |      25    |    1827    |    1809    |
|       from small pool |      17    |      44    |    3029    |    3012    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
dispatch_mask torch.Size([8192, 8, 2048]) dispatch_maskcombine_weights  torch.Size([8192, 8, 2048]) torch.Size([8192, 8, 2048]) L_aux combine_weights torch.Size([])torch.Size([8192, 8, 2048]) 
L_aux dispatch_mask torch.Size([])torch.Size([8, 2048, 8192]) 
reshaped_input torch.Size([8192, 384])
dispatch_mask torch.Size([8, 2048, 8192])dispatched_input_v1:  reshaped_inputtorch.Size([16384, 384])
 torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: dispatched_input_v2:4
 self.num_local_experts torch.Size([16384, 384])
4
dispatched_input_v3: self.experts torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)4

self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
expert_outputv1:self.all2all_size  2 torch.Size([2, 4, 1024, 768])self.num_local_experts 4 
d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
expert_outputv2: combined_output v2: torch.Size([2, 4, 1024, 768])torch.Size([8192, 384])

self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux dispatch_masktorch.Size([]) 
torch.Size([8192, 8, 2048]) dispatch_mask combine_weights torch.Size([8, 2048, 8192]) torch.Size([8192, 8, 2048])reshaped_input  L_auxtorch.Size([8192, 384])
 torch.Size([])
dispatched_input_v1: dispatch_mask torch.Size([16384, 384])
torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
dispatched_input_v2:self.experts  torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
expert_outputv1: torch.Size([2, 4, 1024, 768])
combined_output v2: torch.Size([8192, 384])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask dispatch_masktorch.Size([8, 2048, 8192])  reshaped_input torch.Size([8192, 8, 2048]) torch.Size([8192, 384])
combine_weights dispatched_input_v1: torch.Size([16384, 384])
torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input dispatched_input_v2: torch.Size([8192, 384])
torch.Size([16384, 384])
dispatched_input_v3: dispatched_input_v1: torch.Size([2, 4, 1024, 768])
torch.Size([16384, 384])chunks: 
4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768expert_outputv1:
expert_outputv3:  torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
torch.Size([2, 4, 1024, 768])
combined_output v2: torch.Size([8192, 384])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask dispatch_masktorch.Size([8192, 8, 2048])  combine_weights torch.Size([8192, 8, 2048]) torch.Size([8192, 8, 2048]) combine_weights L_aux torch.Size([8192, 8, 2048]) torch.Size([])
L_auxdispatch_mask  torch.Size([8, 2048, 8192]) torch.Size([])reshaped_input 
torch.Size([8192, 384])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_inputdispatched_input_v1:  torch.Size([16384, 384])
torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: dispatched_input_v2: 4
torch.Size([16384, 384])
self.num_local_experts dispatched_input_v3: 4
torch.Size([2, 4, 1024, 768])
self.expertschunks:  4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv1: expert_outputv2: torch.Size([2, 4, 1024, 768])torch.Size([2, 4, 1024, 768])

self.all2all_size 2 self.num_local_experts 4 d_model 768expert_outputv2:
expert_outputv3:  torch.Size([2, 4, 1024, 768])torch.Size([8, 1024, 768])

combine_weights self.all2all_size torch.Size([8192, 8, 2048])
2 self.num_local_experts 4combined_output v2:  d_modeltorch.Size([8192, 384])
 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weightsdispatched_input_v2: torch.Size([8192, 8, 2048])  L_aux torch.Size([16384, 384])
torch.Size([])
dispatched_input_v3: dispatch_mask torch.Size([2, 4, 1024, 768])
torch.Size([8, 2048, 8192]) chunks: reshaped_input 4
torch.Size([8192, 384])
self.num_local_experts 4
dispatched_input_v1: self.experts torch.Size([16384, 384])
ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: expert_outputv1:torch.Size([2, 4, 1024, 768])
 self.all2all_size torch.Size([2, 4, 1024, 768])2 
self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
expert_outputv2: combine_weights torch.Size([2, 4, 1024, 768])
torch.Size([8192, 8, 2048])self.all2all_size 2 
self.num_local_experts 4 d_modelcombined_output v2:  768torch.Size([8192, 384])

expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])dispatch_mask
 torch.Size([8192, 8, 2048]) dispatch_mask combine_weights torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 8, 2048]) torch.Size([8192, 384])
L_aux torch.Size([])
dispatched_input_v1: dispatch_mask torch.Size([16384, 384])
torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
dispatched_input_v2:self.experts  torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: expert_outputv1:torch.Size([8, 1024, 768])
 combine_weights torch.Size([2, 4, 1024, 768])torch.Size([8192, 8, 2048])

combined_output v2: torch.Size([8192, 384])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
2023-06-30 14:18:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.53 GiB (GPU 0; 31.75 GiB total capacity; 9.21 GiB already allocated; 1.25 GiB free; 10.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2023-06-30 14:18:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.53 GiB (GPU 1; 31.75 GiB total capacity; 9.21 GiB already allocated; 915.25 MiB free; 10.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2023-06-30 14:18:26 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 8         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9429 MB |   10996 MB |  146814 MB |  137385 MB |
|       from large pool |    9397 MB |   10964 MB |  145427 MB |  136029 MB |
|       from small pool |      31 MB |      35 MB |    1386 MB |    1355 MB |
|---------------------------------------------------------------------------|
| Active memory         |    9429 MB |   10996 MB |  146814 MB |  137385 MB |
|       from large pool |    9397 MB |   10964 MB |  145427 MB |  136029 MB |
|       from small pool |      31 MB |      35 MB |    1386 MB |    1355 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   11116 MB |   11900 MB |   15800 MB |    4684 MB |
|       from large pool |   11082 MB |   11866 MB |   15754 MB |    4672 MB |
|       from small pool |      34 MB |      36 MB |      46 MB |      12 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1686 MB |    2549 MB |   99825 MB |   98139 MB |
|       from large pool |    1684 MB |    2544 MB |   98348 MB |   96664 MB |
|       from small pool |       2 MB |      13 MB |    1477 MB |    1474 MB |
|---------------------------------------------------------------------------|
| Allocations           |    1129    |    1135    |   12249    |   11120    |
|       from large pool |     492    |     497    |    4631    |    4139    |
|       from small pool |     637    |     653    |    7618    |    6981    |
|---------------------------------------------------------------------------|
| Active allocs         |    1129    |    1137    |   12249    |   11120    |
|       from large pool |     492    |     499    |    4631    |    4139    |
|       from small pool |     637    |     653    |    7618    |    6981    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      58    |      62    |      70    |      12    |
|       from large pool |      41    |      44    |      47    |       6    |
|       from small pool |      17    |      18    |      23    |       6    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      35    |      68    |    5838    |    5803    |
|       from large pool |      18    |      25    |    2197    |    2179    |
|       from small pool |      17    |      44    |    3641    |    3624    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:26 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:26 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:26 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 8         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9429 MB |   10994 MB |  146810 MB |  137381 MB |
|       from large pool |    9397 MB |   10962 MB |  145423 MB |  136025 MB |
|       from small pool |      31 MB |      35 MB |    1386 MB |    1355 MB |
|---------------------------------------------------------------------------|
| Active memory         |    9429 MB |   10994 MB |  146810 MB |  137381 MB |
|       from large pool |    9397 MB |   10962 MB |  145423 MB |  136025 MB |
|       from small pool |      31 MB |      35 MB |    1386 MB |    1355 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   11116 MB |   11900 MB |   15800 MB |    4684 MB |
|       from large pool |   11082 MB |   11866 MB |   15754 MB |    4672 MB |
|       from small pool |      34 MB |      36 MB |      46 MB |      12 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1686 MB |    2549 MB |  100297 MB |   98611 MB |
|       from large pool |    1684 MB |    2544 MB |   98820 MB |   97136 MB |
|       from small pool |       2 MB |      13 MB |    1477 MB |    1474 MB |
|---------------------------------------------------------------------------|
| Allocations           |    1129    |    1135    |   12249    |   11120    |
|       from large pool |     492    |     497    |    4631    |    4139    |
|       from small pool |     637    |     653    |    7618    |    6981    |
|---------------------------------------------------------------------------|
| Active allocs         |    1129    |    1138    |   12249    |   11120    |
|       from large pool |     492    |     499    |    4631    |    4139    |
|       from small pool |     637    |     653    |    7618    |    6981    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      58    |      62    |      70    |      12    |
|       from large pool |      41    |      44    |      47    |       6    |
|       from small pool |      17    |      18    |      23    |       6    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      35    |      68    |    5851    |    5816    |
|       from large pool |      18    |      25    |    2210    |    2192    |
|       from small pool |      17    |      44    |    3641    |    3624    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2023-06-30 14:18:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])dispatch_mask
 torch.Size([8192, 8, 2048]) dispatch_maskcombine_weights  torch.Size([8, 2048, 8192]) torch.Size([8192, 8, 2048]) reshaped_input L_auxtorch.Size([8192, 384])
 torch.Size([])
dispatched_input_v1:dispatch_mask  torch.Size([16384, 384])
torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: dispatched_input_v2: torch.Size([16384, 384])
torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
dispatched_input_v2:self.experts  torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)

chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux dispatched_input_v2:torch.Size([])
 torch.Size([16384, 384])
dispatch_mask dispatched_input_v3: torch.Size([8, 2048, 8192]) torch.Size([2, 4, 1024, 768])
reshaped_input chunks: torch.Size([8192, 384])
4
self.num_local_experts 4
dispatched_input_v1: self.experts torch.Size([16384, 384])
ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatch_maskdispatched_input_v3:  torch.Size([2, 4, 1024, 768])
torch.Size([8192, 8, 2048])chunks:  combine_weights4 
self.num_local_expertstorch.Size([8192, 8, 2048])  4L_aux 
torch.Size([])
self.experts dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatch_maskdispatched_input_v2:  torch.Size([16384, 384])
dispatched_input_v3: torch.Size([8192, 8, 2048])torch.Size([2, 4, 1024, 768])
 combine_weightschunks:  4
torch.Size([8192, 8, 2048]) self.num_local_experts L_aux 4
torch.Size([])
self.experts dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask dispatch_masktorch.Size([8192, 8, 2048])  combine_weights torch.Size([8192, 8, 2048])torch.Size([8192, 8, 2048])  combine_weightsL_aux  torch.Size([])
torch.Size([8192, 8, 2048]) L_auxdispatch_mask  torch.Size([])torch.Size([8, 2048, 8192]) 
reshaped_input torch.Size([8192, 384])dispatch_mask 
torch.Size([8, 2048, 8192]) reshaped_input dispatched_input_v1: torch.Size([8192, 384])
torch.Size([16384, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v2: dispatched_input_v3: torch.Size([16384, 384])
torch.Size([2, 4, 1024, 768])
dispatched_input_v3: chunks: torch.Size([2, 4, 1024, 768])
4
self.num_local_expertschunks:  44

self.num_local_experts self.experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
expert_outputv2: self.all2all_size torch.Size([2, 4, 1024, 768])
2 self.all2all_size self.num_local_experts 2 4 self.num_local_experts d_model 4 768
d_model expert_outputv3: 768
torch.Size([8, 1024, 768])
expert_outputv3: combine_weights torch.Size([8, 1024, 768])
torch.Size([8192, 8, 2048])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
combined_output v2: torch.Size([8192, 384])
2023-06-30 14:18:27 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.53 GiB (GPU 0; 31.75 GiB total capacity; 9.21 GiB already allocated; 1.25 GiB free; 10.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2023-06-30 14:18:27 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.53 GiB (GPU 1; 31.75 GiB total capacity; 9.21 GiB already allocated; 915.25 MiB free; 10.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2023-06-30 14:18:27 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 9         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9429 MB |   10996 MB |  170777 MB |  161348 MB |
|       from large pool |    9397 MB |   10964 MB |  169159 MB |  159762 MB |
|       from small pool |      31 MB |      35 MB |    1617 MB |    1586 MB |
|---------------------------------------------------------------------------|
| Active memory         |    9429 MB |   10996 MB |  170777 MB |  161348 MB |
|       from large pool |    9397 MB |   10964 MB |  169159 MB |  159762 MB |
|       from small pool |      31 MB |      35 MB |    1617 MB |    1586 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   11116 MB |   11900 MB |   15802 MB |    4686 MB |
|       from large pool |   11082 MB |   11866 MB |   15754 MB |    4672 MB |
|       from small pool |      34 MB |      36 MB |      48 MB |      14 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1686 MB |    2549 MB |  118017 MB |  116330 MB |
|       from large pool |    1684 MB |    2544 MB |  116291 MB |  114607 MB |
|       from small pool |       2 MB |      13 MB |    1725 MB |    1722 MB |
|---------------------------------------------------------------------------|
| Allocations           |    1129    |    1135    |   14222    |   13093    |
|       from large pool |     492    |     497    |    5382    |    4890    |
|       from small pool |     637    |     653    |    8840    |    8203    |
|---------------------------------------------------------------------------|
| Active allocs         |    1129    |    1137    |   14222    |   13093    |
|       from large pool |     492    |     499    |    5382    |    4890    |
|       from small pool |     637    |     653    |    8840    |    8203    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      58    |      62    |      71    |      13    |
|       from large pool |      41    |      44    |      47    |       6    |
|       from small pool |      17    |      18    |      24    |       7    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      36    |      68    |    6841    |    6805    |
|       from large pool |      18    |      25    |    2578    |    2560    |
|       from small pool |      18    |      44    |    4263    |    4245    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:27 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:27 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:27 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2023-06-30 14:18:27 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 9         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9429 MB |   10994 MB |  170772 MB |  161343 MB |
|       from large pool |    9397 MB |   10962 MB |  169154 MB |  159757 MB |
|       from small pool |      31 MB |      35 MB |    1617 MB |    1586 MB |
|---------------------------------------------------------------------------|
| Active memory         |    9429 MB |   10994 MB |  170772 MB |  161343 MB |
|       from large pool |    9397 MB |   10962 MB |  169154 MB |  159757 MB |
|       from small pool |      31 MB |      35 MB |    1617 MB |    1586 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   11116 MB |   11900 MB |   15802 MB |    4686 MB |
|       from large pool |   11082 MB |   11866 MB |   15754 MB |    4672 MB |
|       from small pool |      34 MB |      36 MB |      48 MB |      14 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1686 MB |    2549 MB |  118488 MB |  116801 MB |
|       from large pool |    1684 MB |    2544 MB |  116762 MB |  115078 MB |
|       from small pool |       2 MB |      13 MB |    1725 MB |    1722 MB |
|---------------------------------------------------------------------------|
| Allocations           |    1129    |    1135    |   14222    |   13093    |
|       from large pool |     492    |     497    |    5382    |    4890    |
|       from small pool |     637    |     653    |    8840    |    8203    |
|---------------------------------------------------------------------------|
| Active allocs         |    1129    |    1138    |   14222    |   13093    |
|       from large pool |     492    |     499    |    5382    |    4890    |
|       from small pool |     637    |     653    |    8840    |    8203    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      58    |      62    |      71    |      13    |
|       from large pool |      41    |      44    |      47    |       6    |
|       from small pool |      17    |      18    |      24    |       7    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      36    |      68    |    6856    |    6820    |
|       from large pool |      18    |      25    |    2593    |    2575    |
|       from small pool |      18    |      44    |    4263    |    4245    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:27 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])dispatch_mask
 torch.Size([8192, 8, 2048])dispatch_mask  combine_weightstorch.Size([8, 2048, 8192])  reshaped_input torch.Size([8192, 8, 2048]) torch.Size([8192, 384])
L_aux torch.Size([])
dispatched_input_v1: dispatch_mask torch.Size([16384, 384])
torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_expertsdispatched_input_v2:  4torch.Size([16384, 384])

dispatched_input_v3: self.expertstorch.Size([2, 4, 1024, 768])
 chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_expertsexpert_outputv2: 4  d_modeltorch.Size([2, 4, 1024, 768])
 self.all2all_size 768
2 expert_outputv3: self.num_local_experts torch.Size([8, 1024, 768])
4 combine_weights d_model torch.Size([8192, 8, 2048])
768
expert_outputv3: combined_output v2: torch.Size([8, 1024, 768])
torch.Size([8192, 384])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_maskdispatch_mask  torch.Size([8192, 8, 2048]) torch.Size([8192, 8, 2048])combine_weights  combine_weightstorch.Size([8192, 8, 2048])  L_aux torch.Size([8192, 8, 2048]) torch.Size([])L_aux 
torch.Size([])
dispatch_mask dispatch_masktorch.Size([8, 2048, 8192])  reshaped_input torch.Size([8, 2048, 8192])torch.Size([8192, 384])
 reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v2: dispatched_input_v3: torch.Size([16384, 384])
torch.Size([2, 4, 1024, 768])
dispatched_input_v3:chunks:  4
torch.Size([2, 4, 1024, 768])
self.num_local_experts 4chunks: 
4
self.experts self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size expert_outputv2:2  self.num_local_experts torch.Size([2, 4, 1024, 768])4 
d_model self.all2all_size768
 expert_outputv3: 2 torch.Size([8, 1024, 768])
self.num_local_experts combine_weights 4 torch.Size([8192, 8, 2048])
d_model 768
combined_output v2: expert_outputv3: torch.Size([8192, 384])
torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_maskdispatch_mask  torch.Size([8, 2048, 8192]) torch.Size([8192, 8, 2048]) reshaped_input combine_weights torch.Size([8192, 384])
torch.Size([8192, 8, 2048]) L_aux dispatched_input_v1: torch.Size([])torch.Size([16384, 384])

dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v2: dispatched_input_v1: torch.Size([16384, 384])
torch.Size([16384, 384])dispatched_input_v3: 
torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)chunks: 
4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
expert_outputv1:self.all2all_size  2 torch.Size([2, 4, 1024, 768])self.num_local_experts 
4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weightsexpert_outputv2: torch.Size([8192, 8, 2048]) 
torch.Size([2, 4, 1024, 768])
self.all2all_size combined_output v2: 2 torch.Size([8192, 384])
self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) dispatch_maskcombine_weights  torch.Size([8192, 8, 2048]) torch.Size([8192, 8, 2048]) L_aux combine_weights torch.Size([])torch.Size([8192, 8, 2048]) 
L_aux torch.Size([])dispatch_mask 
torch.Size([8, 2048, 8192]) reshaped_inputdispatch_mask  torch.Size([8192, 384])
torch.Size([8, 2048, 8192]) reshaped_input dispatched_input_v1: torch.Size([8192, 384])
torch.Size([16384, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: dispatched_input_v2: torch.Size([2, 4, 1024, 768])
torch.Size([16384, 384])chunks: 
4
dispatched_input_v3: self.num_local_experts torch.Size([2, 4, 1024, 768])
4
chunks: self.experts 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_expertsexpert_outputv2:  4torch.Size([2, 4, 1024, 768])
 self.all2all_size d_model 2 768
self.num_local_experts expert_outputv3: 4 torch.Size([8, 1024, 768])
d_model combine_weights 768
torch.Size([8192, 8, 2048])
expert_outputv3: torch.Size([8, 1024, 768])combined_output v2: 
combine_weightstorch.Size([8192, 384])
 torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) combine_weights torch.Size([8192, 8, 2048]) L_aux torch.Size([])
dispatch_mask torch.Size([8, 2048, 8192]) reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v3: torch.Size([2, 4, 1024, 768])
chunks: 4
self.num_local_experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size 2 self.num_local_experts 4 d_model 768
expert_outputv3: torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
dispatch_mask torch.Size([8192, 8, 2048]) dispatch_maskcombine_weights  torch.Size([8192, 8, 2048]) torch.Size([8192, 8, 2048]) L_aux combine_weights torch.Size([])torch.Size([8192, 8, 2048]) 
L_aux torch.Size([])dispatch_mask 
torch.Size([8, 2048, 8192]) reshaped_inputdispatch_mask  torch.Size([8192, 384])torch.Size([8, 2048, 8192]) 
reshaped_input torch.Size([8192, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v1: torch.Size([16384, 384])
dispatched_input_v2: torch.Size([16384, 384])
dispatched_input_v2:dispatched_input_v3:  torch.Size([2, 4, 1024, 768])
torch.Size([16384, 384])
chunks: dispatched_input_v3: 4
torch.Size([2, 4, 1024, 768])
self.num_local_expertschunks:  44

self.num_local_experts self.experts 4
self.experts ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
ModuleList(
  (0): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (1): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (2): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
  (3): FeedForwardNetwork(
    (activation_dropout_module): Dropout(p=0.0, inplace=False)
    (dropout_module): Dropout(p=0.1, inplace=False)
    (fc1): Linear(in_features=768, out_features=3072, bias=True)
    (fc2): Linear(in_features=3072, out_features=768, bias=True)
    (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
  )
)
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv1: torch.Size([2, 4, 1024, 768])
expert_outputv2: torch.Size([2, 4, 1024, 768])
self.all2all_size expert_outputv2:2  self.num_local_experts torch.Size([2, 4, 1024, 768])
4 self.all2all_size d_model 2768
 self.num_local_expertsexpert_outputv3:  torch.Size([8, 1024, 768])
4combine_weights  d_modeltorch.Size([8192, 8, 2048])
 768
combined_output v2: expert_outputv3: torch.Size([8192, 384])
torch.Size([8, 1024, 768])
combine_weights torch.Size([8192, 8, 2048])
combined_output v2: torch.Size([8192, 384])
2023-06-30 14:18:28 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.53 GiB (GPU 1; 31.75 GiB total capacity; 9.21 GiB already allocated; 915.25 MiB free; 10.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2023-06-30 14:18:28 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.53 GiB (GPU 0; 31.75 GiB total capacity; 9.21 GiB already allocated; 1.25 GiB free; 10.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2023-06-30 14:18:28 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 8            |        cudaMalloc retries: 10        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9429 MB |   10996 MB |  194740 MB |  185311 MB |
|       from large pool |    9397 MB |   10964 MB |  192891 MB |  183493 MB |
|       from small pool |      31 MB |      35 MB |    1848 MB |    1817 MB |
|---------------------------------------------------------------------------|
| Active memory         |    9429 MB |   10996 MB |  194740 MB |  185311 MB |
|       from large pool |    9397 MB |   10964 MB |  192891 MB |  183493 MB |
|       from small pool |      31 MB |      35 MB |    1848 MB |    1817 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   11116 MB |   11900 MB |   15804 MB |    4688 MB |
|       from large pool |   11082 MB |   11866 MB |   15754 MB |    4672 MB |
|       from small pool |      34 MB |      36 MB |      50 MB |      16 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1686 MB |    2549 MB |  136210 MB |  134523 MB |
|       from large pool |    1684 MB |    2544 MB |  134232 MB |  132548 MB |
|       from small pool |       2 MB |      13 MB |    1977 MB |    1974 MB |
|---------------------------------------------------------------------------|
| Allocations           |    1129    |    1135    |   16195    |   15066    |
|       from large pool |     492    |     497    |    6133    |    5641    |
|       from small pool |     637    |     653    |   10062    |    9425    |
|---------------------------------------------------------------------------|
| Active allocs         |    1129    |    1137    |   16195    |   15066    |
|       from large pool |     492    |     499    |    6133    |    5641    |
|       from small pool |     637    |     653    |   10062    |    9425    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      58    |      62    |      72    |      14    |
|       from large pool |      41    |      44    |      47    |       6    |
|       from small pool |      17    |      18    |      25    |       8    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      35    |      68    |    7865    |    7830    |
|       from large pool |      18    |      25    |    2959    |    2941    |
|       from small pool |      17    |      44    |    4906    |    4889    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:28 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:28 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:28 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2023-06-30 14:18:28 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 8            |        cudaMalloc retries: 10        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9429 MB |   10994 MB |  194736 MB |  185307 MB |
|       from large pool |    9397 MB |   10962 MB |  192887 MB |  183489 MB |
|       from small pool |      31 MB |      35 MB |    1848 MB |    1817 MB |
|---------------------------------------------------------------------------|
| Active memory         |    9429 MB |   10994 MB |  194736 MB |  185307 MB |
|       from large pool |    9397 MB |   10962 MB |  192887 MB |  183489 MB |
|       from small pool |      31 MB |      35 MB |    1848 MB |    1817 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   11116 MB |   11900 MB |   15804 MB |    4688 MB |
|       from large pool |   11082 MB |   11866 MB |   15754 MB |    4672 MB |
|       from small pool |      34 MB |      36 MB |      50 MB |      16 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1686 MB |    2549 MB |  136682 MB |  134995 MB |
|       from large pool |    1684 MB |    2544 MB |  134704 MB |  133020 MB |
|       from small pool |       2 MB |      13 MB |    1977 MB |    1974 MB |
|---------------------------------------------------------------------------|
| Allocations           |    1129    |    1135    |   16195    |   15066    |
|       from large pool |     492    |     497    |    6133    |    5641    |
|       from small pool |     637    |     653    |   10062    |    9425    |
|---------------------------------------------------------------------------|
| Active allocs         |    1129    |    1138    |   16195    |   15066    |
|       from large pool |     492    |     499    |    6133    |    5641    |
|       from small pool |     637    |     653    |   10062    |    9425    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      58    |      62    |      72    |      14    |
|       from large pool |      41    |      44    |      47    |       6    |
|       from small pool |      17    |      18    |      25    |       8    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      35    |      68    |    7880    |    7845    |
|       from large pool |      18    |      25    |    2974    |    2956    |
|       from small pool |      17    |      44    |    4906    |    4889    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-06-30 14:18:28 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
