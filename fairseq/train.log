2023-06-28 18:33:30 | INFO | fairseq.distributed.utils | distributed init (rank 11): env://
2023-06-28 18:33:30 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 11
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | distributed init (rank 2): env://
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | distributed init (rank 9): env://
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 9
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | distributed init (rank 7): env://
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | distributed init (rank 15): env://
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 15
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | distributed init (rank 3): env://
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | distributed init (rank 1): env://
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | distributed init (rank 14): env://
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 14
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | distributed init (rank 0): env://
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | distributed init (rank 10): env://
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 10
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | distributed init (rank 12): env://
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 12
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | distributed init (rank 5): env://
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | distributed init (rank 6): env://
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | distributed init (rank 8): env://
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | distributed init (rank 4): env://
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | distributed init (rank 13): env://
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 8
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Rank 12: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 13
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Rank 13: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | initialized host GCRHYP3C308 as rank 12
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | initialized host GCRHYP3C308 as rank 13
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | initialized host GCRHYP3C308 as rank 0
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | initialized host GCRHYP3C308 as rank 2
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | initialized host GCRHYP3C308 as rank 6
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | initialized host GCRHYP3C308 as rank 5
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Rank 8: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | initialized host GCRHYP3C308 as rank 7
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | initialized host GCRHYP3C308 as rank 8
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | initialized host GCRHYP3C308 as rank 1
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | initialized host GCRHYP3C308 as rank 4
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Rank 11: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | initialized host GCRHYP3C308 as rank 3
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | initialized host GCRHYP3C308 as rank 11
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Rank 14: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Rank 10: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | initialized host GCRHYP3C308 as rank 14
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | initialized host GCRHYP3C308 as rank 10
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Rank 9: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | initialized host GCRHYP3C308 as rank 9
2023-06-28 18:33:32 | INFO | torch.distributed.distributed_c10d | Rank 15: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
2023-06-28 18:33:32 | INFO | fairseq.distributed.utils | initialized host GCRHYP3C308 as rank 15
2023-06-28 18:33:45 | INFO | fairseq_cli.train | nvidia-smi stats: {'gpu_0_mem_used_gb': 0.5712890625, 'gpu_1_mem_used_gb': 0.6884765625, 'gpu_2_mem_used_gb': 0.6884765625, 'gpu_3_mem_used_gb': 0.6884765625, 'gpu_4_mem_used_gb': 0.6884765625, 'gpu_5_mem_used_gb': 0.6884765625, 'gpu_6_mem_used_gb': 0.6884765625, 'gpu_7_mem_used_gb': 0.6884765625, 'gpu_8_mem_used_gb': 0.6884765625, 'gpu_9_mem_used_gb': 0.6884765625, 'gpu_10_mem_used_gb': 0.6884765625, 'gpu_11_mem_used_gb': 0.6884765625, 'gpu_12_mem_used_gb': 0.6884765625, 'gpu_13_mem_used_gb': 0.6884765625, 'gpu_14_mem_used_gb': 0.6884765625, 'gpu_15_mem_used_gb': 0.6650390625}
2023-06-28 18:33:45 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 50, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': '/mnt1/msranlpintern/wuxun/MoE/MoE_results/mhmoe_v1/small-baseline-redstone_v2-flash_attn-8experts-2heads/tb-logs', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'log_nvidia_smi': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None, 'is_moe': False}, 'distributed_training': {'_name': None, 'distributed_world_size': 16, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 16, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 2, 'num_workers_valid': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 2, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': None, 'batch_size_valid': 2, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 300000, 'stop_time_hours': 0.0, 'clip_norm': 2.0, 'sentence_avg': False, 'update_freq': [8], 'lr': [0.0006], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/mnt1/msranlpintern/wuxun/MoE/MoE_results/mhmoe_v1/small-baseline-redstone_v2-flash_attn-8experts-2heads', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_best_checkpoints': False, 'no_save_optimizer_state': False, 'no_save_optimizer_state_on_training_finished': False, 'symlink_best_and_last_checkpoints': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '-rank-0', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 's3_upload_path': None, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 16}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807, 'stats_path': None, 'max_valid_steps': None}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'gpt_small', 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 768, 'decoder_output_dim': 768, 'decoder_input_dim': 768, 'decoder_ffn_embed_dim': 3072, 'decoder_layers': 12, 'decoder_attention_heads': 12, 'decoder_normalize_before': True, 'no_token_positional_embeddings': True, 'share_decoder_input_output_embed': True, 'decoder_learned_pos': True, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'min_params_to_wrap': 100000000, 'moe_freq': 2, 'moe_expert_count': 8, 'moe_gating_use_fp32': True, 'moe_second_expert_policy': 'random', 'moe_normalize_gate_prob_before_dropping': True, 'moe_expert_ffn_dim': None, 'moe_top1_expert': False, 'moe_eval_capacity_token_fraction': -1.0, 'moe_normalize_expert_grad': 'world_size', 'record_a2a_perf_stats': False, 'dummy_a2a': False, 'moe_batch_prioritized_routing': False, 'use_xmoe': True, 'use_mhmoe': True, 'mhmoe_heads_number': 2, 'flash_attention': False, 'xpos_rel_pos': True, 'scale_length': 2048, 'add_bos_token': False, 'tokens_per_sample': 2048, 'max_target_positions': None, 'tpu': False, 'memory_efficient_fp16': True, 'fp16': True, 'fp16_no_flatten_grads': False, 'ddp_backend': 'c10d', 'world_size': 16, 'distributed_rank': 0, 'ddp_rank': 0, 'deepnorm': False, 'subln': True, 'rel_pos_buckets': 0, 'max_rel_pos': 0, 'group_norm_size': 1, 'model_parallel_size': 1}, 'task': {'_name': 'gpt', 'data': '/mnt/msranlp/shaohanh/data/redstone_v2_1_config/', 'sample_break_mode': 'none', 'tokens_per_sample': 2048, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_source_positions': None, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': 2, 'batch_size_valid': 2, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'spm_model': '', 'tiktoken_model': 'cl100k_base', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe', 'dict_path': '/mnt/msranlp/shaohanh/exp/unigpt_exp/data/tiktoken/cl100k_w_code_dict.txt', 'batch_read_ahead': 10000, 'pad_to_max_len': True, 'absolute_path': False, 'required_batch_size_multiple': 1}, 'criterion': {'_name': 'moe_cross_entropy', 'moe_gate_loss_wt': 0.01, 'moe_gate_loss_combine_method': 'sum', 'moe_gate_loss_transform': 'none', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0006], 'block_wise': False}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 375, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 300000.0, 'lr': [0.0006]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2023-06-28 18:33:53 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-06-28 18:33:53 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.
2023-06-28 18:33:54 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:3 to store for rank: 0
2023-06-28 18:33:54 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.
2023-06-28 18:33:54 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:4 to store for rank: 0
2023-06-28 18:33:54 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.
2023-06-28 18:33:54 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:5 to store for rank: 0
2023-06-28 18:33:54 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.
2023-06-28 18:33:54 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:6 to store for rank: 0
2023-06-28 18:33:54 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.
2023-06-28 18:33:54 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:7 to store for rank: 0
2023-06-28 18:33:54 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.
2023-06-28 18:33:54 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:8 to store for rank: 0
2023-06-28 18:33:54 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.
2023-06-28 18:33:54 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:9 to store for rank: 0
2023-06-28 18:33:54 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.
2023-06-28 18:33:54 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:10 to store for rank: 0
2023-06-28 18:33:54 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.
2023-06-28 18:33:54 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:11 to store for rank: 0
2023-06-28 18:33:54 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.
2023-06-28 18:33:54 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:12 to store for rank: 0
2023-06-28 18:33:54 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.
2023-06-28 18:33:56 | INFO | fairseq_cli.train | LanguageModel(
  (decoder): LMDecoder(
    (dropout_module): Dropout(p=0.1, inplace=False)
    (embed_tokens): Embedding(100287, 768, padding_idx=1)
    (output_projection): Linear(in_features=768, out_features=100287, bias=False)
    (layers): ModuleList(
      (0): DecoderLayer(
        (dropout_module): Dropout(p=0.1, inplace=False)
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
          (inner_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (xpos): XPOS()
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn): FeedForwardNetwork(
          (activation_dropout_module): Dropout(p=0.0, inplace=False)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (dropout_module): Dropout(p=0.1, inplace=False)
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
          (inner_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (xpos): XPOS()
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (moe_layer): MH_MOELayer(
          (gate): Top2Gate(
            (wg_reduction): Linear(in_features=384, out_features=16, bias=False)
          )
          (experts): ModuleList(
            (0): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
          )
          (multi_heads): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (dropout_module): Dropout(p=0.1, inplace=False)
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
          (inner_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (xpos): XPOS()
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn): FeedForwardNetwork(
          (activation_dropout_module): Dropout(p=0.0, inplace=False)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (dropout_module): Dropout(p=0.1, inplace=False)
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
          (inner_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (xpos): XPOS()
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (moe_layer): MH_MOELayer(
          (gate): Top2Gate(
            (wg_reduction): Linear(in_features=384, out_features=16, bias=False)
          )
          (experts): ModuleList(
            (0): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
          )
          (multi_heads): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (dropout_module): Dropout(p=0.1, inplace=False)
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
          (inner_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (xpos): XPOS()
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn): FeedForwardNetwork(
          (activation_dropout_module): Dropout(p=0.0, inplace=False)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (dropout_module): Dropout(p=0.1, inplace=False)
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
          (inner_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (xpos): XPOS()
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (moe_layer): MH_MOELayer(
          (gate): Top2Gate(
            (wg_reduction): Linear(in_features=384, out_features=16, bias=False)
          )
          (experts): ModuleList(
            (0): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
          )
          (multi_heads): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (dropout_module): Dropout(p=0.1, inplace=False)
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
          (inner_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (xpos): XPOS()
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn): FeedForwardNetwork(
          (activation_dropout_module): Dropout(p=0.0, inplace=False)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (dropout_module): Dropout(p=0.1, inplace=False)
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
          (inner_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (xpos): XPOS()
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (moe_layer): MH_MOELayer(
          (gate): Top2Gate(
            (wg_reduction): Linear(in_features=384, out_features=16, bias=False)
          )
          (experts): ModuleList(
            (0): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
          )
          (multi_heads): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (dropout_module): Dropout(p=0.1, inplace=False)
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
          (inner_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (xpos): XPOS()
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn): FeedForwardNetwork(
          (activation_dropout_module): Dropout(p=0.0, inplace=False)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (dropout_module): Dropout(p=0.1, inplace=False)
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
          (inner_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (xpos): XPOS()
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (moe_layer): MH_MOELayer(
          (gate): Top2Gate(
            (wg_reduction): Linear(in_features=384, out_features=16, bias=False)
          )
          (experts): ModuleList(
            (0): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
          )
          (multi_heads): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (dropout_module): Dropout(p=0.1, inplace=False)
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
          (inner_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (xpos): XPOS()
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn): FeedForwardNetwork(
          (activation_dropout_module): Dropout(p=0.0, inplace=False)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (dropout_module): Dropout(p=0.1, inplace=False)
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
          (inner_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (xpos): XPOS()
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (moe_layer): MH_MOELayer(
          (gate): Top2Gate(
            (wg_reduction): Linear(in_features=384, out_features=16, bias=False)
          )
          (experts): ModuleList(
            (0): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
            )
          )
          (multi_heads): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
  )
)
2023-06-28 18:33:56 | INFO | fairseq_cli.train | task: GPTPretrainingTask
2023-06-28 18:33:56 | INFO | fairseq_cli.train | model: LanguageModel
2023-06-28 18:33:56 | INFO | fairseq_cli.train | criterion: MoECrossEntropyCriterion
2023-06-28 18:33:56 | INFO | fairseq_cli.train | num. non-expert model params: 140,921,856 (num. trained: 140,921,856)
2023-06-28 18:33:56 | INFO | fairseq_cli.train | num. expert model params: 28,371,456 (num. trained: 28,371,456)
2023-06-28 18:33:58 | INFO | fairseq_cli.train | nvidia-smi stats: {'gpu_0_mem_used_gb': 0.5712890625, 'gpu_1_mem_used_gb': 0.6884765625, 'gpu_2_mem_used_gb': 0.6884765625, 'gpu_3_mem_used_gb': 0.6884765625, 'gpu_4_mem_used_gb': 0.6884765625, 'gpu_5_mem_used_gb': 0.6884765625, 'gpu_6_mem_used_gb': 0.6884765625, 'gpu_7_mem_used_gb': 0.6884765625, 'gpu_8_mem_used_gb': 0.6884765625, 'gpu_9_mem_used_gb': 0.6884765625, 'gpu_10_mem_used_gb': 0.6884765625, 'gpu_11_mem_used_gb': 0.6884765625, 'gpu_12_mem_used_gb': 0.6884765625, 'gpu_13_mem_used_gb': 0.6884765625, 'gpu_14_mem_used_gb': 0.6884765625, 'gpu_15_mem_used_gb': 0.6650390625}
2023-06-28 18:33:59 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2023-06-28 18:33:59 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.1.moe_layer.gate.wg_reduction.bias
2023-06-28 18:33:59 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.3.moe_layer.gate.wg_reduction.bias
2023-06-28 18:33:59 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.5.moe_layer.gate.wg_reduction.bias
2023-06-28 18:33:59 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.7.moe_layer.gate.wg_reduction.bias
2023-06-28 18:33:59 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.9.moe_layer.gate.wg_reduction.bias
2023-06-28 18:33:59 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.11.moe_layer.gate.wg_reduction.bias
2023-06-28 18:34:01 | INFO | fairseq.trainer | nvidia-smi stats: {'gpu_0_mem_used_gb': 0.6259765625, 'gpu_1_mem_used_gb': 0.6884765625, 'gpu_2_mem_used_gb': 0.7431640625, 'gpu_3_mem_used_gb': 0.7431640625, 'gpu_4_mem_used_gb': 0.7431640625, 'gpu_5_mem_used_gb': 0.7431640625, 'gpu_6_mem_used_gb': 0.7431640625, 'gpu_7_mem_used_gb': 0.6884765625, 'gpu_8_mem_used_gb': 0.7431640625, 'gpu_9_mem_used_gb': 0.7431640625, 'gpu_10_mem_used_gb': 0.7431640625, 'gpu_11_mem_used_gb': 0.6884765625, 'gpu_12_mem_used_gb': 0.6884765625, 'gpu_13_mem_used_gb': 0.6884765625, 'gpu_14_mem_used_gb': 0.7431640625, 'gpu_15_mem_used_gb': 0.6650390625}
2023-06-28 18:34:05 | INFO | fairseq.utils | ***********************CUDA enviroments for all 16 workers***********************
2023-06-28 18:34:05 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM3-32GB                    
2023-06-28 18:34:05 | INFO | fairseq.utils | rank   1: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM3-32GB                    
2023-06-28 18:34:05 | INFO | fairseq.utils | rank   2: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM3-32GB                    
2023-06-28 18:34:05 | INFO | fairseq.utils | rank   3: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM3-32GB                    
2023-06-28 18:34:05 | INFO | fairseq.utils | rank   4: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM3-32GB                    
2023-06-28 18:34:05 | INFO | fairseq.utils | rank   5: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM3-32GB                    
2023-06-28 18:34:05 | INFO | fairseq.utils | rank   6: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM3-32GB                    
2023-06-28 18:34:05 | INFO | fairseq.utils | rank   7: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM3-32GB                    
2023-06-28 18:34:05 | INFO | fairseq.utils | rank   8: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM3-32GB                    
2023-06-28 18:34:05 | INFO | fairseq.utils | rank   9: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM3-32GB                    
2023-06-28 18:34:05 | INFO | fairseq.utils | rank  10: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM3-32GB                    
2023-06-28 18:34:05 | INFO | fairseq.utils | rank  11: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM3-32GB                    
2023-06-28 18:34:05 | INFO | fairseq.utils | rank  12: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM3-32GB                    
2023-06-28 18:34:05 | INFO | fairseq.utils | rank  13: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM3-32GB                    
2023-06-28 18:34:05 | INFO | fairseq.utils | rank  14: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM3-32GB                    
2023-06-28 18:34:05 | INFO | fairseq.utils | rank  15: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM3-32GB                    
2023-06-28 18:34:05 | INFO | fairseq.utils | ***********************CUDA enviroments for all 16 workers***********************
2023-06-28 18:34:05 | INFO | fairseq_cli.train | training on 16 devices (GPUs/TPUs)
2023-06-28 18:34:05 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 2
2023-06-28 18:34:07 | INFO | fairseq_cli.train | nvidia-smi stats: {'gpu_0_mem_used_gb': 0.8212890625, 'gpu_1_mem_used_gb': 1.0556640625, 'gpu_2_mem_used_gb': 1.0556640625, 'gpu_3_mem_used_gb': 1.0556640625, 'gpu_4_mem_used_gb': 1.0556640625, 'gpu_5_mem_used_gb': 1.0556640625, 'gpu_6_mem_used_gb': 1.0556640625, 'gpu_7_mem_used_gb': 1.0556640625, 'gpu_8_mem_used_gb': 1.0556640625, 'gpu_9_mem_used_gb': 1.0556640625, 'gpu_10_mem_used_gb': 1.0556640625, 'gpu_11_mem_used_gb': 1.0556640625, 'gpu_12_mem_used_gb': 1.0556640625, 'gpu_13_mem_used_gb': 1.0556640625, 'gpu_14_mem_used_gb': 1.0556640625, 'gpu_15_mem_used_gb': 1.0087890625}
2023-06-28 18:34:07 | INFO | fairseq.trainer | No existing checkpoint found /mnt1/msranlpintern/wuxun/MoE/MoE_results/mhmoe_v1/small-baseline-redstone_v2-flash_attn-8experts-2heads/checkpoint_last-rank-0.pt
2023-06-28 18:34:07 | INFO | fairseq.trainer | loading train data for epoch 1
2023-06-28 18:34:36 | INFO | fairseq.optim.adam | using FusedAdam
2023-06-28 18:34:36 | INFO | fairseq.trainer | begin training epoch 1
2023-06-28 18:34:36 | INFO | fairseq_cli.train | Start iterating over samples
2023-06-28 18:34:36 | WARNING | infinibatch.iterators | trying to fetch item, but prefetch buffer is empty
2023-06-28 18:36:05 | WARNING | infinibatch.iterators | trying to fetch item, but prefetch buffer is empty
2023-06-28 18:36:05 | WARNING | infinibatch.iterators | trying to fetch item, but prefetch buffer is empty
2023-06-28 18:36:05 | WARNING | infinibatch.iterators | trying to fetch item, but prefetch buffer is empty
2023-06-28 18:36:05 | WARNING | infinibatch.iterators | trying to fetch item, but prefetch buffer is empty
2023-06-28 18:36:05 | WARNING | infinibatch.iterators | trying to fetch item, but prefetch buffer is empty
2023-06-28 18:36:05 | WARNING | infinibatch.iterators | trying to fetch item, but prefetch buffer is empty
2023-06-28 18:36:58 | INFO | torch.nn.parallel.distributed | Reducer buckets have been rebuilt in this iteration.
2023-06-28 18:40:31 | INFO | train_inner | epoch 001:     50 / 102400000 loss=15.321, moe_gate_loss=6.39545, overflow_expert1=4.418, overflow_expert2=32.817, entropy_gating=2.018, expert1_balance_top=48.485, expert1_balance_bottom=7.459, unused_expert1_count=0.002, expert2_balance_top=37.459, expert2_balance_bottom=12.928, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=15.229, ppl=38395.4, wps=120777, ups=0.23, wpb=524273, bsz=256, num_updates=50, lr=8e-05, gnorm=1.945, clip=36, loss_scale=4, train_wall=265, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=386
2023-06-28 18:44:06 | INFO | train_inner | epoch 001:    100 / 102400000 loss=10.998, moe_gate_loss=8.31164, overflow_expert1=20.629, overflow_expert2=21.405, entropy_gating=1.845, expert1_balance_top=68.268, expert1_balance_bottom=3.794, unused_expert1_count=0.648, expert2_balance_top=41.279, expert2_balance_bottom=9.599, unused_expert2_count=0.053, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=10.879, ppl=1882.66, wps=121801, ups=0.23, wpb=524271, bsz=256, num_updates=100, lr=0.00016, gnorm=1.842, clip=48, loss_scale=4, train_wall=215, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=601
2023-06-28 18:47:40 | INFO | train_inner | epoch 001:    150 / 102400000 loss=6.469, moe_gate_loss=10.8778, overflow_expert1=32.745, overflow_expert2=20.709, entropy_gating=1.602, expert1_balance_top=82.608, expert1_balance_bottom=0.778, unused_expert1_count=1.656, expert2_balance_top=47.865, expert2_balance_bottom=5.605, unused_expert2_count=0.145, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=6.312, ppl=79.45, wps=122762, ups=0.23, wpb=524298, bsz=256, num_updates=150, lr=0.00024, gnorm=3.169, clip=100, loss_scale=8, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=815
2023-06-28 18:51:14 | INFO | train_inner | epoch 001:    200 / 102400000 loss=3.49, moe_gate_loss=11.3777, overflow_expert1=32.921, overflow_expert2=22.467, entropy_gating=1.54, expert1_balance_top=82.863, expert1_balance_bottom=0.294, unused_expert1_count=1.498, expert2_balance_top=51.658, expert2_balance_bottom=4.587, unused_expert2_count=0.225, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=3.326, ppl=10.03, wps=122463, ups=0.23, wpb=524301, bsz=256, num_updates=200, lr=0.00032, gnorm=3.108, clip=100, loss_scale=8, train_wall=214, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=1030
2023-06-28 18:54:51 | INFO | train_inner | epoch 001:    250 / 102400000 loss=1.967, moe_gate_loss=10.2937, overflow_expert1=30.266, overflow_expert2=23.971, entropy_gating=1.626, expert1_balance_top=80.172, expert1_balance_bottom=0.633, unused_expert1_count=1.183, expert2_balance_top=55.58, expert2_balance_bottom=3.891, unused_expert2_count=0.257, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=1.818, ppl=3.53, wps=121350, ups=0.23, wpb=524144, bsz=256, num_updates=250, lr=0.0004, gnorm=2.282, clip=82, loss_scale=8, train_wall=216, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=1246
2023-06-28 18:58:24 | INFO | train_inner | epoch 001:    300 / 102400000 loss=1.348, moe_gate_loss=9.49271, overflow_expert1=29.493, overflow_expert2=27.274, entropy_gating=1.676, expert1_balance_top=79.264, expert1_balance_bottom=0.98, unused_expert1_count=1.078, expert2_balance_top=62.353, expert2_balance_bottom=2.878, unused_expert2_count=0.241, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=1.211, ppl=2.32, wps=122956, ups=0.23, wpb=524290, bsz=256, num_updates=300, lr=0.00048, gnorm=1.235, clip=6, loss_scale=16, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=1459
2023-06-28 19:02:00 | INFO | train_inner | epoch 001:    350 / 102400000 loss=1.244, moe_gate_loss=9.63122, overflow_expert1=29.702, overflow_expert2=27.295, entropy_gating=1.685, expert1_balance_top=79.399, expert1_balance_bottom=1.105, unused_expert1_count=1.069, expert2_balance_top=60.005, expert2_balance_bottom=3.344, unused_expert2_count=0.194, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=1.105, ppl=2.15, wps=121816, ups=0.23, wpb=524142, bsz=256, num_updates=350, lr=0.00056, gnorm=1.044, clip=4, loss_scale=16, train_wall=215, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=1675
2023-06-28 19:05:34 | INFO | train_inner | epoch 001:    400 / 102400000 loss=0.608, moe_gate_loss=8.37451, overflow_expert1=23.043, overflow_expert2=21.341, entropy_gating=1.776, expert1_balance_top=72.184, expert1_balance_bottom=2.218, unused_expert1_count=1.106, expert2_balance_top=58.766, expert2_balance_bottom=4.019, unused_expert2_count=0.256, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.487, ppl=1.4, wps=122299, ups=0.23, wpb=524170, bsz=256, num_updates=400, lr=0.00059995, gnorm=0.411, clip=0, loss_scale=32, train_wall=214, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=1889
2023-06-28 19:09:08 | INFO | train_inner | epoch 001:    450 / 102400000 loss=0.248, moe_gate_loss=7.33709, overflow_expert1=17.827, overflow_expert2=15.707, entropy_gating=1.794, expert1_balance_top=65.125, expert1_balance_bottom=3.91, unused_expert1_count=1.195, expert2_balance_top=56.094, expert2_balance_bottom=3.244, unused_expert2_count=0.333, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.142, ppl=1.1, wps=123201, ups=0.23, wpb=524326, bsz=256, num_updates=450, lr=0.00059985, gnorm=0.238, clip=0, loss_scale=32, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=2103
2023-06-28 19:12:43 | INFO | train_inner | epoch 001:    500 / 102400000 loss=0.213, moe_gate_loss=7.33156, overflow_expert1=13.821, overflow_expert2=22.763, entropy_gating=1.796, expert1_balance_top=60.856, expert1_balance_bottom=2.93, unused_expert1_count=1.155, expert2_balance_top=57.314, expert2_balance_bottom=3.461, unused_expert2_count=0.403, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.108, ppl=1.08, wps=121825, ups=0.23, wpb=524348, bsz=256, num_updates=500, lr=0.00059975, gnorm=0.193, clip=0, loss_scale=32, train_wall=215, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=2318
2023-06-28 19:16:16 | INFO | train_inner | epoch 001:    550 / 102400000 loss=0.167, moe_gate_loss=7.07204, overflow_expert1=9.435, overflow_expert2=19.451, entropy_gating=1.734, expert1_balance_top=53.289, expert1_balance_bottom=6.388, unused_expert1_count=1.109, expert2_balance_top=56.018, expert2_balance_bottom=3.536, unused_expert2_count=0.41, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.065, ppl=1.05, wps=123177, ups=0.23, wpb=524213, bsz=256, num_updates=550, lr=0.00059965, gnorm=0.161, clip=0, loss_scale=64, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=2531
2023-06-28 19:16:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-06-28 19:16:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-06-28 19:19:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-06-28 19:20:04 | INFO | train_inner | epoch 001:    603 / 102400000 loss=0.784, moe_gate_loss=8.3164, overflow_expert1=15.344, overflow_expert2=23.88, entropy_gating=1.654, expert1_balance_top=59.433, expert1_balance_bottom=6.508, unused_expert1_count=1.138, expert2_balance_top=54.337, expert2_balance_bottom=4.866, unused_expert2_count=0.322, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.664, ppl=1.58, wps=115572, ups=0.22, wpb=524357, bsz=256, num_updates=600, lr=0.000599549, gnorm=1.477, clip=20, loss_scale=8, train_wall=227, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=2759
2023-06-28 19:23:38 | INFO | train_inner | epoch 001:    653 / 102400000 loss=0.316, moe_gate_loss=7.99997, overflow_expert1=12.098, overflow_expert2=19.525, entropy_gating=1.654, expert1_balance_top=58.339, expert1_balance_bottom=4.315, unused_expert1_count=0.869, expert2_balance_top=53.192, expert2_balance_bottom=5.871, unused_expert2_count=0.229, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.2, ppl=1.15, wps=122552, ups=0.23, wpb=524440, bsz=256, num_updates=650, lr=0.000599449, gnorm=0.165, clip=0, loss_scale=8, train_wall=214, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=2973
2023-06-28 19:27:10 | INFO | train_inner | epoch 001:    703 / 102400000 loss=0.189, moe_gate_loss=7.27382, overflow_expert1=9.597, overflow_expert2=17.61, entropy_gating=1.666, expert1_balance_top=52.186, expert1_balance_bottom=7.9, unused_expert1_count=1.022, expert2_balance_top=52.699, expert2_balance_bottom=5.384, unused_expert2_count=0.298, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.084, ppl=1.06, wps=123859, ups=0.24, wpb=524052, bsz=256, num_updates=700, lr=0.000599349, gnorm=0.272, clip=2, loss_scale=8, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=3185
2023-06-28 19:30:45 | INFO | train_inner | epoch 001:    753 / 102400000 loss=0.228, moe_gate_loss=7.89025, overflow_expert1=11.829, overflow_expert2=18.925, entropy_gating=1.584, expert1_balance_top=57.137, expert1_balance_bottom=4.831, unused_expert1_count=1.055, expert2_balance_top=53.942, expert2_balance_bottom=4.401, unused_expert2_count=0.329, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.114, ppl=1.08, wps=121825, ups=0.23, wpb=524189, bsz=256, num_updates=750, lr=0.000599249, gnorm=0.212, clip=0, loss_scale=16, train_wall=215, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=3400
2023-06-28 19:34:17 | INFO | train_inner | epoch 001:    803 / 102400000 loss=0.168, moe_gate_loss=7.16275, overflow_expert1=7.163, overflow_expert2=19.925, entropy_gating=1.654, expert1_balance_top=49.945, expert1_balance_bottom=6.722, unused_expert1_count=0.59, expert2_balance_top=49.313, expert2_balance_bottom=6.443, unused_expert2_count=0.237, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.064, ppl=1.05, wps=123824, ups=0.24, wpb=524241, bsz=256, num_updates=800, lr=0.000599149, gnorm=0.128, clip=0, loss_scale=16, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=3612
2023-06-28 19:35:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-06-28 19:36:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-06-28 19:37:59 | INFO | train_inner | epoch 001:    855 / 102400000 loss=0.782, moe_gate_loss=8.89233, overflow_expert1=14.323, overflow_expert2=23.644, entropy_gating=1.555, expert1_balance_top=58.335, expert1_balance_bottom=5.59, unused_expert1_count=0.799, expert2_balance_top=52.369, expert2_balance_bottom=6.267, unused_expert2_count=0.299, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.654, ppl=1.57, wps=118356, ups=0.23, wpb=524132, bsz=256, num_updates=850, lr=0.000599049, gnorm=1.367, clip=20, loss_scale=4, train_wall=221, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=3834
2023-06-28 19:41:32 | INFO | train_inner | epoch 001:    905 / 102400000 loss=0.592, moe_gate_loss=9.08826, overflow_expert1=19.315, overflow_expert2=29.932, entropy_gating=1.647, expert1_balance_top=67.802, expert1_balance_bottom=1.769, unused_expert1_count=0.737, expert2_balance_top=54.234, expert2_balance_bottom=4.566, unused_expert2_count=0.084, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.461, ppl=1.38, wps=123353, ups=0.24, wpb=524363, bsz=256, num_updates=900, lr=0.000598949, gnorm=0.241, clip=0, loss_scale=4, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=4047
2023-06-28 19:45:04 | INFO | train_inner | epoch 001:    955 / 102400000 loss=0.174, moe_gate_loss=7.22839, overflow_expert1=9.619, overflow_expert2=21.156, entropy_gating=1.738, expert1_balance_top=54.998, expert1_balance_bottom=5.295, unused_expert1_count=1.008, expert2_balance_top=52.199, expert2_balance_bottom=4.684, unused_expert2_count=0.346, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.069, ppl=1.05, wps=123690, ups=0.24, wpb=524209, bsz=256, num_updates=950, lr=0.000598849, gnorm=0.072, clip=0, loss_scale=4, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=4259
2023-06-28 19:48:39 | INFO | train_inner | epoch 001:   1005 / 102400000 loss=0.16, moe_gate_loss=7.14019, overflow_expert1=9.013, overflow_expert2=20.632, entropy_gating=1.694, expert1_balance_top=52.94, expert1_balance_bottom=5.846, unused_expert1_count=0.938, expert2_balance_top=53.024, expert2_balance_bottom=5.141, unused_expert2_count=0.348, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.057, ppl=1.04, wps=122368, ups=0.23, wpb=524271, bsz=256, num_updates=1000, lr=0.000598748, gnorm=0.111, clip=0, loss_scale=8, train_wall=214, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=4474
2023-06-28 19:52:10 | INFO | train_inner | epoch 001:   1055 / 102400000 loss=0.264, moe_gate_loss=7.2465, overflow_expert1=10.176, overflow_expert2=18.782, entropy_gating=1.696, expert1_balance_top=53.365, expert1_balance_bottom=6.759, unused_expert1_count=1.014, expert2_balance_top=48.519, expert2_balance_bottom=5.837, unused_expert2_count=0.327, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.16, ppl=1.12, wps=124348, ups=0.24, wpb=524218, bsz=256, num_updates=1050, lr=0.000598648, gnorm=0.175, clip=0, loss_scale=8, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=4685
2023-06-28 19:55:44 | INFO | train_inner | epoch 001:   1105 / 102400000 loss=0.224, moe_gate_loss=7.65206, overflow_expert1=11.067, overflow_expert2=20.771, entropy_gating=1.683, expert1_balance_top=56.367, expert1_balance_bottom=5.145, unused_expert1_count=0.983, expert2_balance_top=50.397, expert2_balance_bottom=5.928, unused_expert2_count=0.283, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.114, ppl=1.08, wps=123046, ups=0.23, wpb=524242, bsz=256, num_updates=1100, lr=0.000598548, gnorm=0.129, clip=0, loss_scale=16, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=4899
2023-06-28 19:59:18 | INFO | train_inner | epoch 001:   1155 / 102400000 loss=0.313, moe_gate_loss=7.84552, overflow_expert1=12.659, overflow_expert2=21.116, entropy_gating=1.665, expert1_balance_top=57.915, expert1_balance_bottom=5.421, unused_expert1_count=0.875, expert2_balance_top=48.634, expert2_balance_bottom=6.123, unused_expert2_count=0.194, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.2, ppl=1.15, wps=122522, ups=0.23, wpb=524101, bsz=256, num_updates=1150, lr=0.000598448, gnorm=0.293, clip=4, loss_scale=16, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=5113
2023-06-28 20:02:50 | INFO | train_inner | epoch 001:   1205 / 102400000 loss=0.193, moe_gate_loss=7.53101, overflow_expert1=9.726, overflow_expert2=21.981, entropy_gating=1.73, expert1_balance_top=54.753, expert1_balance_bottom=4.932, unused_expert1_count=0.467, expert2_balance_top=48.916, expert2_balance_bottom=6.169, unused_expert2_count=0.053, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.085, ppl=1.06, wps=123956, ups=0.24, wpb=524151, bsz=256, num_updates=1200, lr=0.000598348, gnorm=0.102, clip=0, loss_scale=16, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=5325
2023-06-28 20:03:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-06-28 20:06:30 | INFO | train_inner | epoch 001:   1256 / 102400000 loss=0.245, moe_gate_loss=7.35096, overflow_expert1=10.395, overflow_expert2=20.105, entropy_gating=1.755, expert1_balance_top=55.577, expert1_balance_bottom=5.622, unused_expert1_count=0.461, expert2_balance_top=48.697, expert2_balance_bottom=6.355, unused_expert2_count=0.142, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.139, ppl=1.1, wps=119113, ups=0.23, wpb=524338, bsz=256, num_updates=1250, lr=0.000598248, gnorm=0.132, clip=0, loss_scale=16, train_wall=220, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=5545
Expecting value: line 1 column 1 (char 0)

2023-06-28 20:10:03 | INFO | train_inner | epoch 001:   1306 / 102400000 loss=0.141, moe_gate_loss=6.59709, overflow_expert1=5.947, overflow_expert2=17.314, entropy_gating=1.736, expert1_balance_top=46.663, expert1_balance_bottom=9.341, unused_expert1_count=0.05, expert2_balance_top=47.012, expert2_balance_bottom=6.827, unused_expert2_count=0.098, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.046, ppl=1.03, wps=123307, ups=0.24, wpb=524245, bsz=256, num_updates=1300, lr=0.000598148, gnorm=0.058, clip=0, loss_scale=16, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=5758
2023-06-28 20:13:37 | INFO | train_inner | epoch 001:   1356 / 102400000 loss=0.136, moe_gate_loss=6.6049, overflow_expert1=4.864, overflow_expert2=18.812, entropy_gating=1.726, expert1_balance_top=45.157, expert1_balance_bottom=9.29, unused_expert1_count=0.085, expert2_balance_top=47.271, expert2_balance_bottom=7.671, unused_expert2_count=0.074, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.04, ppl=1.03, wps=122806, ups=0.23, wpb=524248, bsz=256, num_updates=1350, lr=0.000598048, gnorm=0.064, clip=0, loss_scale=32, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=5972
2023-06-28 20:17:10 | INFO | train_inner | epoch 001:   1406 / 102400000 loss=0.143, moe_gate_loss=6.73084, overflow_expert1=5.655, overflow_expert2=19.271, entropy_gating=1.751, expert1_balance_top=46.799, expert1_balance_bottom=8.549, unused_expert1_count=0.053, expert2_balance_top=46.977, expert2_balance_bottom=7.41, unused_expert2_count=0.138, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.046, ppl=1.03, wps=123193, ups=0.23, wpb=524375, bsz=256, num_updates=1400, lr=0.000597947, gnorm=0.069, clip=0, loss_scale=32, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=6185
2023-06-28 20:20:42 | INFO | train_inner | epoch 001:   1456 / 102400000 loss=0.12, moe_gate_loss=6.27851, overflow_expert1=4.234, overflow_expert2=12.834, entropy_gating=1.722, expert1_balance_top=39.824, expert1_balance_bottom=14.187, unused_expert1_count=0.004, expert2_balance_top=42.525, expert2_balance_bottom=8.959, unused_expert2_count=0.151, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.03, ppl=1.02, wps=123888, ups=0.24, wpb=524280, bsz=256, num_updates=1450, lr=0.000597847, gnorm=0.038, clip=0, loss_scale=32, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=6397
2023-06-28 20:24:18 | INFO | train_inner | epoch 001:   1506 / 102400000 loss=0.105, moe_gate_loss=6.04171, overflow_expert1=3.764, overflow_expert2=9.668, entropy_gating=1.656, expert1_balance_top=35.241, expert1_balance_bottom=18.116, unused_expert1_count=0.001, expert2_balance_top=43.044, expert2_balance_bottom=10.014, unused_expert2_count=0.148, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.018, ppl=1.01, wps=121785, ups=0.23, wpb=524116, bsz=256, num_updates=1500, lr=0.000597747, gnorm=0.028, clip=0, loss_scale=64, train_wall=215, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=6613
2023-06-28 20:27:50 | INFO | train_inner | epoch 001:   1556 / 102400000 loss=0.102, moe_gate_loss=5.98859, overflow_expert1=3.811, overflow_expert2=8.243, entropy_gating=1.678, expert1_balance_top=34.574, expert1_balance_bottom=18.74, unused_expert1_count=0.001, expert2_balance_top=41.483, expert2_balance_bottom=10.111, unused_expert2_count=0.15, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.015, ppl=1.01, wps=123660, ups=0.24, wpb=524231, bsz=256, num_updates=1550, lr=0.000597647, gnorm=0.026, clip=0, loss_scale=64, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=6825
2023-06-28 20:31:24 | INFO | train_inner | epoch 001:   1606 / 102400000 loss=0.106, moe_gate_loss=6.09036, overflow_expert1=2.676, overflow_expert2=12.019, entropy_gating=1.705, expert1_balance_top=36.476, expert1_balance_bottom=16.051, unused_expert1_count=0.001, expert2_balance_top=42.235, expert2_balance_bottom=9.985, unused_expert2_count=0.009, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.018, ppl=1.01, wps=122548, ups=0.23, wpb=524322, bsz=256, num_updates=1600, lr=0.000597547, gnorm=0.04, clip=0, loss_scale=128, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=7039
2023-06-28 20:34:58 | INFO | train_inner | epoch 001:   1656 / 102400000 loss=0.116, moe_gate_loss=6.26594, overflow_expert1=0.915, overflow_expert2=18.224, entropy_gating=1.734, expert1_balance_top=37.242, expert1_balance_bottom=13.82, unused_expert1_count=0.012, expert2_balance_top=43.616, expert2_balance_bottom=10.231, unused_expert2_count=0.005, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.026, ppl=1.02, wps=122756, ups=0.23, wpb=524280, bsz=256, num_updates=1650, lr=0.000597447, gnorm=0.047, clip=0, loss_scale=128, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=7253
2023-06-28 20:38:30 | INFO | train_inner | epoch 001:   1706 / 102400000 loss=0.101, moe_gate_loss=6.06234, overflow_expert1=0.004, overflow_expert2=11.241, entropy_gating=1.617, expert1_balance_top=29.897, expert1_balance_bottom=19.704, unused_expert1_count=0.006, expert2_balance_top=40.992, expert2_balance_bottom=10.605, unused_expert2_count=0.002, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.013, ppl=1.01, wps=123868, ups=0.24, wpb=524211, bsz=256, num_updates=1700, lr=0.000597347, gnorm=0.021, clip=0, loss_scale=128, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=7465
2023-06-28 20:42:06 | INFO | train_inner | epoch 001:   1756 / 102400000 loss=0.097, moe_gate_loss=6.01482, overflow_expert1=0, overflow_expert2=9.214, entropy_gating=1.592, expert1_balance_top=28.205, expert1_balance_bottom=21.76, unused_expert1_count=0, expert2_balance_top=40.465, expert2_balance_bottom=11.236, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.01, ppl=1.01, wps=122022, ups=0.23, wpb=524240, bsz=256, num_updates=1750, lr=0.000597247, gnorm=0.016, clip=0, loss_scale=256, train_wall=214, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=7681
2023-06-28 20:45:38 | INFO | train_inner | epoch 001:   1806 / 102400000 loss=0.096, moe_gate_loss=6.01256, overflow_expert1=0.001, overflow_expert2=8.715, entropy_gating=1.606, expert1_balance_top=27.715, expert1_balance_bottom=22.334, unused_expert1_count=0, expert2_balance_top=40.364, expert2_balance_bottom=11.982, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.009, ppl=1.01, wps=123718, ups=0.24, wpb=524108, bsz=256, num_updates=1800, lr=0.000597146, gnorm=0.015, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=7893
2023-06-28 20:49:12 | INFO | train_inner | epoch 001:   1856 / 102400000 loss=0.096, moe_gate_loss=6.00595, overflow_expert1=0, overflow_expert2=8.371, entropy_gating=1.6, expert1_balance_top=27.929, expert1_balance_bottom=22.21, unused_expert1_count=0, expert2_balance_top=39.653, expert2_balance_bottom=10.295, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.009, ppl=1.01, wps=122870, ups=0.23, wpb=524163, bsz=256, num_updates=1850, lr=0.000597046, gnorm=0.015, clip=0, loss_scale=256, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=8107
2023-06-28 20:50:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-28 20:50:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-06-28 20:52:54 | INFO | train_inner | epoch 001:   1908 / 102400000 loss=0.097, moe_gate_loss=6.00208, overflow_expert1=0.001, overflow_expert2=9.34, entropy_gating=1.594, expert1_balance_top=27.959, expert1_balance_bottom=22.101, unused_expert1_count=0, expert2_balance_top=40.526, expert2_balance_bottom=11.025, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.01, ppl=1.01, wps=118114, ups=0.23, wpb=524266, bsz=256, num_updates=1900, lr=0.000596946, gnorm=0.017, clip=0, loss_scale=128, train_wall=221, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=8329
2023-06-28 20:56:26 | INFO | train_inner | epoch 001:   1958 / 102400000 loss=0.096, moe_gate_loss=6.00053, overflow_expert1=0, overflow_expert2=8.404, entropy_gating=1.59, expert1_balance_top=27.744, expert1_balance_bottom=22.271, unused_expert1_count=0, expert2_balance_top=40.318, expert2_balance_bottom=11.373, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.009, ppl=1.01, wps=124052, ups=0.24, wpb=524251, bsz=256, num_updates=1950, lr=0.000596846, gnorm=0.016, clip=0, loss_scale=128, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=8541
2023-06-28 21:00:01 | INFO | train_inner | epoch 001:   2008 / 102400000 loss=0.095, moe_gate_loss=6.00427, overflow_expert1=0, overflow_expert2=7.635, entropy_gating=1.564, expert1_balance_top=27.931, expert1_balance_bottom=22.156, unused_expert1_count=0, expert2_balance_top=39.824, expert2_balance_bottom=12.215, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.009, ppl=1.01, wps=122443, ups=0.23, wpb=524233, bsz=256, num_updates=2000, lr=0.000596746, gnorm=0.016, clip=0, loss_scale=256, train_wall=214, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=8756
2023-06-28 21:03:34 | INFO | train_inner | epoch 001:   2058 / 102400000 loss=0.095, moe_gate_loss=6.00506, overflow_expert1=0, overflow_expert2=6.542, entropy_gating=1.566, expert1_balance_top=27.668, expert1_balance_bottom=22.384, unused_expert1_count=0, expert2_balance_top=38.609, expert2_balance_bottom=12.632, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.009, ppl=1.01, wps=123422, ups=0.24, wpb=524222, bsz=256, num_updates=2050, lr=0.000596646, gnorm=0.016, clip=0, loss_scale=256, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=8968
2023-06-28 21:07:05 | INFO | train_inner | epoch 001:   2108 / 102400000 loss=0.095, moe_gate_loss=6.00587, overflow_expert1=0, overflow_expert2=6.829, entropy_gating=1.552, expert1_balance_top=27.916, expert1_balance_bottom=22.123, unused_expert1_count=0, expert2_balance_top=38.478, expert2_balance_bottom=12.563, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.009, ppl=1.01, wps=124226, ups=0.24, wpb=524362, bsz=256, num_updates=2100, lr=0.000596546, gnorm=0.016, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=9180
2023-06-28 21:10:39 | INFO | train_inner | epoch 001:   2158 / 102400000 loss=0.095, moe_gate_loss=6.00841, overflow_expert1=0, overflow_expert2=5.588, entropy_gating=1.545, expert1_balance_top=27.898, expert1_balance_bottom=22.205, unused_expert1_count=0, expert2_balance_top=36.576, expert2_balance_bottom=13.385, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=122719, ups=0.23, wpb=524211, bsz=256, num_updates=2150, lr=0.000596446, gnorm=0.015, clip=0, loss_scale=512, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=9394
2023-06-28 21:14:12 | INFO | train_inner | epoch 001:   2208 / 102400000 loss=0.095, moe_gate_loss=6.0067, overflow_expert1=0, overflow_expert2=5.759, entropy_gating=1.56, expert1_balance_top=28.312, expert1_balance_bottom=21.735, unused_expert1_count=0, expert2_balance_top=36.132, expert2_balance_bottom=13.788, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.009, ppl=1.01, wps=123276, ups=0.24, wpb=524191, bsz=256, num_updates=2200, lr=0.000596345, gnorm=0.017, clip=0, loss_scale=512, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=9607
2023-06-28 21:17:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-28 21:17:51 | INFO | train_inner | epoch 001:   2259 / 102400000 loss=0.095, moe_gate_loss=6.00098, overflow_expert1=0, overflow_expert2=3.989, entropy_gating=1.568, expert1_balance_top=27.752, expert1_balance_bottom=22.286, unused_expert1_count=0, expert2_balance_top=34.702, expert2_balance_bottom=14.778, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=120080, ups=0.23, wpb=524146, bsz=256, num_updates=2250, lr=0.000596245, gnorm=0.015, clip=0, loss_scale=256, train_wall=218, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=9826
2023-06-28 21:21:23 | INFO | train_inner | epoch 001:   2309 / 102400000 loss=0.097, moe_gate_loss=6.02061, overflow_expert1=0, overflow_expert2=5.19, entropy_gating=1.57, expert1_balance_top=28.986, expert1_balance_bottom=21.056, unused_expert1_count=0, expert2_balance_top=35.965, expert2_balance_bottom=15.274, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.01, ppl=1.01, wps=124274, ups=0.24, wpb=524189, bsz=256, num_updates=2300, lr=0.000596145, gnorm=0.02, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=10038
2023-06-28 21:24:56 | INFO | train_inner | epoch 001:   2359 / 102400000 loss=0.095, moe_gate_loss=5.99765, overflow_expert1=0, overflow_expert2=3.71, entropy_gating=1.584, expert1_balance_top=27.885, expert1_balance_bottom=22.151, unused_expert1_count=0, expert2_balance_top=34.709, expert2_balance_bottom=17.232, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=123242, ups=0.24, wpb=524263, bsz=256, num_updates=2350, lr=0.000596045, gnorm=0.015, clip=0, loss_scale=256, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=10251
2023-06-28 21:26:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-06-28 21:28:35 | INFO | train_inner | epoch 001:   2410 / 102400000 loss=0.095, moe_gate_loss=5.99544, overflow_expert1=0, overflow_expert2=4.561, entropy_gating=1.581, expert1_balance_top=28.229, expert1_balance_bottom=21.863, unused_expert1_count=0, expert2_balance_top=34.659, expert2_balance_bottom=16.45, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.009, ppl=1.01, wps=120155, ups=0.23, wpb=524243, bsz=256, num_updates=2400, lr=0.000595945, gnorm=0.017, clip=0, loss_scale=128, train_wall=218, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=10469
2023-06-28 21:32:07 | INFO | train_inner | epoch 001:   2460 / 102400000 loss=0.095, moe_gate_loss=5.99866, overflow_expert1=0.002, overflow_expert2=5.401, entropy_gating=1.588, expert1_balance_top=28.419, expert1_balance_bottom=21.596, unused_expert1_count=0, expert2_balance_top=35.027, expert2_balance_bottom=15.808, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.009, ppl=1.01, wps=123783, ups=0.24, wpb=524170, bsz=256, num_updates=2450, lr=0.000595845, gnorm=0.017, clip=0, loss_scale=128, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=10682
2023-06-28 21:35:43 | INFO | train_inner | epoch 001:   2510 / 102400000 loss=0.095, moe_gate_loss=6.00321, overflow_expert1=0.001, overflow_expert2=5.097, entropy_gating=1.595, expert1_balance_top=28.372, expert1_balance_bottom=21.671, unused_expert1_count=0, expert2_balance_top=33.469, expert2_balance_bottom=16.913, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=121566, ups=0.23, wpb=524209, bsz=256, num_updates=2500, lr=0.000595745, gnorm=0.017, clip=0, loss_scale=256, train_wall=215, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=10898
2023-06-28 21:39:16 | INFO | train_inner | epoch 001:   2560 / 102400000 loss=0.095, moe_gate_loss=6.0021, overflow_expert1=0.001, overflow_expert2=4.945, entropy_gating=1.596, expert1_balance_top=28.077, expert1_balance_bottom=21.961, unused_expert1_count=0, expert2_balance_top=33.09, expert2_balance_bottom=17.38, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.009, ppl=1.01, wps=123254, ups=0.24, wpb=524260, bsz=256, num_updates=2550, lr=0.000595645, gnorm=0.017, clip=0, loss_scale=256, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=11111
2023-06-28 21:42:49 | INFO | train_inner | epoch 001:   2610 / 102400000 loss=0.095, moe_gate_loss=6.00618, overflow_expert1=0, overflow_expert2=4.898, entropy_gating=1.611, expert1_balance_top=28.172, expert1_balance_bottom=21.838, unused_expert1_count=0, expert2_balance_top=32.812, expert2_balance_bottom=17.554, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.009, ppl=1.01, wps=123762, ups=0.24, wpb=524096, bsz=256, num_updates=2600, lr=0.000595544, gnorm=0.017, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=11323
2023-06-28 21:46:23 | INFO | train_inner | epoch 001:   2660 / 102400000 loss=0.097, moe_gate_loss=6.01778, overflow_expert1=0.002, overflow_expert2=4.801, entropy_gating=1.645, expert1_balance_top=29.163, expert1_balance_bottom=20.861, unused_expert1_count=0, expert2_balance_top=33.486, expert2_balance_bottom=16.882, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.01, ppl=1.01, wps=122516, ups=0.23, wpb=524334, bsz=256, num_updates=2650, lr=0.000595444, gnorm=0.021, clip=0, loss_scale=512, train_wall=214, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=11538
2023-06-28 21:47:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-28 21:49:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-06-28 21:50:05 | INFO | train_inner | epoch 001:   2712 / 102400000 loss=0.095, moe_gate_loss=6.00654, overflow_expert1=0, overflow_expert2=3.537, entropy_gating=1.67, expert1_balance_top=28.196, expert1_balance_bottom=21.826, unused_expert1_count=0, expert2_balance_top=32.106, expert2_balance_bottom=18.385, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.009, ppl=1.01, wps=118477, ups=0.23, wpb=524219, bsz=256, num_updates=2700, lr=0.000595344, gnorm=0.017, clip=0, loss_scale=128, train_wall=221, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=11760
2023-06-28 21:53:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-06-28 21:53:44 | INFO | train_inner | epoch 001:   2763 / 102400000 loss=0.096, moe_gate_loss=6.01165, overflow_expert1=0, overflow_expert2=2.761, entropy_gating=1.69, expert1_balance_top=28.922, expert1_balance_bottom=21.095, unused_expert1_count=0, expert2_balance_top=31.137, expert2_balance_bottom=18.933, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.009, ppl=1.01, wps=119717, ups=0.23, wpb=524317, bsz=256, num_updates=2750, lr=0.000595244, gnorm=0.019, clip=0, loss_scale=64, train_wall=219, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=11979
2023-06-28 21:57:16 | INFO | train_inner | epoch 001:   2813 / 102400000 loss=0.096, moe_gate_loss=6.00607, overflow_expert1=0, overflow_expert2=2.325, entropy_gating=1.698, expert1_balance_top=28.582, expert1_balance_bottom=21.451, unused_expert1_count=0, expert2_balance_top=30.507, expert2_balance_bottom=19.463, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.009, ppl=1.01, wps=124054, ups=0.24, wpb=524225, bsz=256, num_updates=2800, lr=0.000595144, gnorm=0.018, clip=0, loss_scale=64, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=12191
2023-06-28 22:00:50 | INFO | train_inner | epoch 001:   2863 / 102400000 loss=0.095, moe_gate_loss=6.00599, overflow_expert1=0, overflow_expert2=2.229, entropy_gating=1.709, expert1_balance_top=28.779, expert1_balance_bottom=21.22, unused_expert1_count=0, expert2_balance_top=30.602, expert2_balance_bottom=19.499, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=122780, ups=0.23, wpb=524285, bsz=256, num_updates=2850, lr=0.000595044, gnorm=0.018, clip=0, loss_scale=64, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=12405
2023-06-28 22:04:24 | INFO | train_inner | epoch 001:   2913 / 102400000 loss=0.095, moe_gate_loss=6.00274, overflow_expert1=0.001, overflow_expert2=2.133, entropy_gating=1.709, expert1_balance_top=28.592, expert1_balance_bottom=21.415, unused_expert1_count=0, expert2_balance_top=30.32, expert2_balance_bottom=19.85, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.009, ppl=1.01, wps=122796, ups=0.23, wpb=524242, bsz=256, num_updates=2900, lr=0.000594944, gnorm=0.018, clip=0, loss_scale=128, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=12619
2023-06-28 22:07:56 | INFO | train_inner | epoch 001:   2963 / 102400000 loss=0.095, moe_gate_loss=6.00653, overflow_expert1=0.001, overflow_expert2=2.69, entropy_gating=1.692, expert1_balance_top=28.702, expert1_balance_bottom=21.319, unused_expert1_count=0, expert2_balance_top=30.558, expert2_balance_bottom=19.844, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=124181, ups=0.24, wpb=524071, bsz=256, num_updates=2950, lr=0.000594844, gnorm=0.018, clip=0, loss_scale=128, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=12831
2023-06-28 22:08:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-06-28 22:11:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-06-28 22:11:39 | INFO | train_inner | epoch 001:   3015 / 102400000 loss=0.095, moe_gate_loss=6.00505, overflow_expert1=0, overflow_expert2=4.303, entropy_gating=1.696, expert1_balance_top=28.782, expert1_balance_bottom=21.201, unused_expert1_count=0, expert2_balance_top=32.695, expert2_balance_bottom=17.349, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=117583, ups=0.22, wpb=524423, bsz=256, num_updates=3000, lr=0.000594743, gnorm=0.018, clip=0, loss_scale=32, train_wall=223, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=13054
2023-06-28 22:15:12 | INFO | train_inner | epoch 001:   3065 / 102400000 loss=0.094, moe_gate_loss=6.00561, overflow_expert1=0.001, overflow_expert2=4.027, entropy_gating=1.702, expert1_balance_top=28.615, expert1_balance_bottom=21.367, unused_expert1_count=0, expert2_balance_top=32.564, expert2_balance_bottom=17.559, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=123791, ups=0.24, wpb=524094, bsz=256, num_updates=3050, lr=0.000594643, gnorm=0.016, clip=0, loss_scale=32, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=13266
2023-06-28 22:18:45 | INFO | train_inner | epoch 001:   3115 / 102400000 loss=0.094, moe_gate_loss=6.00448, overflow_expert1=0, overflow_expert2=4.352, entropy_gating=1.699, expert1_balance_top=28.453, expert1_balance_bottom=21.538, unused_expert1_count=0, expert2_balance_top=32.966, expert2_balance_bottom=17.565, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=122990, ups=0.23, wpb=524265, bsz=256, num_updates=3100, lr=0.000594543, gnorm=0.016, clip=0, loss_scale=32, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=13480
2023-06-28 22:22:19 | INFO | train_inner | epoch 001:   3165 / 102400000 loss=0.095, moe_gate_loss=6.00889, overflow_expert1=0.001, overflow_expert2=5.707, entropy_gating=1.71, expert1_balance_top=29.114, expert1_balance_bottom=20.888, unused_expert1_count=0, expert2_balance_top=34.434, expert2_balance_bottom=16.29, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.009, ppl=1.01, wps=122846, ups=0.23, wpb=524258, bsz=256, num_updates=3150, lr=0.000594443, gnorm=0.019, clip=0, loss_scale=64, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=13694
2023-06-28 22:25:50 | INFO | train_inner | epoch 001:   3215 / 102400000 loss=0.095, moe_gate_loss=6.00545, overflow_expert1=0, overflow_expert2=4.639, entropy_gating=1.72, expert1_balance_top=28.528, expert1_balance_bottom=21.486, unused_expert1_count=0, expert2_balance_top=33.081, expert2_balance_bottom=17.176, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=124698, ups=0.24, wpb=524215, bsz=256, num_updates=3200, lr=0.000594343, gnorm=0.017, clip=0, loss_scale=64, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=13905
2023-06-28 22:29:24 | INFO | train_inner | epoch 001:   3265 / 102400000 loss=0.095, moe_gate_loss=6.00408, overflow_expert1=0.001, overflow_expert2=5.149, entropy_gating=1.715, expert1_balance_top=28.319, expert1_balance_bottom=21.682, unused_expert1_count=0, expert2_balance_top=33.617, expert2_balance_bottom=16.929, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=122687, ups=0.23, wpb=524324, bsz=256, num_updates=3250, lr=0.000594243, gnorm=0.017, clip=0, loss_scale=64, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=14119
2023-06-28 22:32:55 | INFO | train_inner | epoch 001:   3315 / 102400000 loss=0.094, moe_gate_loss=6.00657, overflow_expert1=0.001, overflow_expert2=5.522, entropy_gating=1.72, expert1_balance_top=28.576, expert1_balance_bottom=21.428, unused_expert1_count=0, expert2_balance_top=34.204, expert2_balance_bottom=16.342, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=124594, ups=0.24, wpb=524017, bsz=256, num_updates=3300, lr=0.000594143, gnorm=0.017, clip=0, loss_scale=128, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=14330
2023-06-28 22:36:28 | INFO | train_inner | epoch 001:   3365 / 102400000 loss=0.095, moe_gate_loss=6.00622, overflow_expert1=0.001, overflow_expert2=5.441, entropy_gating=1.73, expert1_balance_top=28.711, expert1_balance_bottom=21.308, unused_expert1_count=0, expert2_balance_top=33.694, expert2_balance_bottom=16.058, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=123294, ups=0.24, wpb=524245, bsz=256, num_updates=3350, lr=0.000594043, gnorm=0.018, clip=0, loss_scale=128, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=14543
2023-06-28 22:40:03 | INFO | train_inner | epoch 001:   3415 / 102400000 loss=0.095, moe_gate_loss=6.00683, overflow_expert1=0.001, overflow_expert2=5.892, entropy_gating=1.733, expert1_balance_top=28.437, expert1_balance_bottom=21.598, unused_expert1_count=0, expert2_balance_top=34.081, expert2_balance_bottom=16.061, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=122312, ups=0.23, wpb=524071, bsz=256, num_updates=3400, lr=0.000593942, gnorm=0.017, clip=0, loss_scale=256, train_wall=214, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=14758
2023-06-28 22:43:35 | INFO | train_inner | epoch 001:   3465 / 102400000 loss=0.095, moe_gate_loss=6.00525, overflow_expert1=0, overflow_expert2=5.845, entropy_gating=1.739, expert1_balance_top=28.507, expert1_balance_bottom=21.482, unused_expert1_count=0, expert2_balance_top=34.212, expert2_balance_bottom=16.226, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=123833, ups=0.24, wpb=524306, bsz=256, num_updates=3450, lr=0.000593842, gnorm=0.017, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=14970
2023-06-28 22:47:10 | INFO | train_inner | epoch 001:   3515 / 102400000 loss=0.094, moe_gate_loss=6.00572, overflow_expert1=0, overflow_expert2=5.588, entropy_gating=1.73, expert1_balance_top=28.469, expert1_balance_bottom=21.55, unused_expert1_count=0, expert2_balance_top=33.741, expert2_balance_bottom=16.629, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=122713, ups=0.23, wpb=524316, bsz=256, num_updates=3500, lr=0.000593742, gnorm=0.017, clip=0, loss_scale=256, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=15184
2023-06-28 22:50:42 | INFO | train_inner | epoch 001:   3565 / 102400000 loss=0.095, moe_gate_loss=6.00759, overflow_expert1=0, overflow_expert2=6.591, entropy_gating=1.74, expert1_balance_top=28.61, expert1_balance_bottom=21.368, unused_expert1_count=0, expert2_balance_top=34.702, expert2_balance_bottom=16.191, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=123810, ups=0.24, wpb=524392, bsz=256, num_updates=3550, lr=0.000593642, gnorm=0.018, clip=0, loss_scale=512, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=15397
2023-06-28 22:54:15 | INFO | train_inner | epoch 001:   3615 / 102400000 loss=0.095, moe_gate_loss=6.00593, overflow_expert1=0, overflow_expert2=7.053, entropy_gating=1.762, expert1_balance_top=28.569, expert1_balance_bottom=21.436, unused_expert1_count=0, expert2_balance_top=35.115, expert2_balance_bottom=16.027, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=123220, ups=0.24, wpb=524205, bsz=256, num_updates=3600, lr=0.000593542, gnorm=0.017, clip=0, loss_scale=512, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=15610
2023-06-28 22:57:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-06-28 22:57:53 | INFO | train_inner | epoch 001:   3666 / 102400000 loss=0.094, moe_gate_loss=6.00329, overflow_expert1=0.001, overflow_expert2=7.264, entropy_gating=1.759, expert1_balance_top=28.023, expert1_balance_bottom=21.978, unused_expert1_count=0, expert2_balance_top=35.388, expert2_balance_bottom=16.251, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=120942, ups=0.23, wpb=524230, bsz=256, num_updates=3650, lr=0.000593442, gnorm=0.015, clip=0, loss_scale=512, train_wall=216, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=15827
2023-06-28 23:01:26 | INFO | train_inner | epoch 001:   3716 / 102400000 loss=0.095, moe_gate_loss=6.00445, overflow_expert1=0, overflow_expert2=7.734, entropy_gating=1.755, expert1_balance_top=28.033, expert1_balance_bottom=21.968, unused_expert1_count=0, expert2_balance_top=35.786, expert2_balance_bottom=15.911, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=123598, ups=0.24, wpb=524331, bsz=256, num_updates=3700, lr=0.000593342, gnorm=0.015, clip=0, loss_scale=512, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=16040
2023-06-28 23:05:02 | INFO | train_inner | epoch 001:   3766 / 102400000 loss=0.095, moe_gate_loss=6.00508, overflow_expert1=0.001, overflow_expert2=8.352, entropy_gating=1.752, expert1_balance_top=28.411, expert1_balance_bottom=21.585, unused_expert1_count=0, expert2_balance_top=36.875, expert2_balance_bottom=15.52, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=121452, ups=0.23, wpb=524318, bsz=256, num_updates=3750, lr=0.000593242, gnorm=0.016, clip=0, loss_scale=512, train_wall=215, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=16257
2023-06-28 23:06:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-28 23:07:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-06-28 23:08:43 | INFO | train_inner | epoch 001:   3818 / 102400000 loss=0.095, moe_gate_loss=6.00391, overflow_expert1=0.002, overflow_expert2=8.876, entropy_gating=1.738, expert1_balance_top=28.17, expert1_balance_bottom=21.842, unused_expert1_count=0, expert2_balance_top=37.385, expert2_balance_bottom=15.276, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=118957, ups=0.23, wpb=524204, bsz=256, num_updates=3800, lr=0.000593141, gnorm=0.016, clip=0, loss_scale=128, train_wall=220, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=16478
2023-06-28 23:12:15 | INFO | train_inner | epoch 001:   3868 / 102400000 loss=0.096, moe_gate_loss=6.00948, overflow_expert1=0.002, overflow_expert2=9.163, entropy_gating=1.75, expert1_balance_top=28.666, expert1_balance_bottom=21.363, unused_expert1_count=0, expert2_balance_top=37.287, expert2_balance_bottom=14.906, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.009, ppl=1.01, wps=124146, ups=0.24, wpb=524212, bsz=256, num_updates=3850, lr=0.000593041, gnorm=0.018, clip=0, loss_scale=128, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=16689
2023-06-28 23:15:48 | INFO | train_inner | epoch 001:   3918 / 102400000 loss=0.095, moe_gate_loss=6.00476, overflow_expert1=0, overflow_expert2=9.742, entropy_gating=1.745, expert1_balance_top=28.212, expert1_balance_bottom=21.784, unused_expert1_count=0, expert2_balance_top=38.041, expert2_balance_bottom=14.938, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=123085, ups=0.23, wpb=524237, bsz=256, num_updates=3900, lr=0.000592941, gnorm=0.016, clip=0, loss_scale=128, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=16903
2023-06-28 23:19:20 | INFO | train_inner | epoch 001:   3968 / 102400000 loss=0.095, moe_gate_loss=6.00227, overflow_expert1=0, overflow_expert2=10.477, entropy_gating=1.744, expert1_balance_top=28.053, expert1_balance_bottom=21.944, unused_expert1_count=0, expert2_balance_top=38.937, expert2_balance_bottom=14.579, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.009, ppl=1.01, wps=124252, ups=0.24, wpb=524243, bsz=256, num_updates=3950, lr=0.000592841, gnorm=0.016, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=17115
2023-06-28 23:22:53 | INFO | train_inner | epoch 001:   4018 / 102400000 loss=0.095, moe_gate_loss=6.00481, overflow_expert1=0.003, overflow_expert2=12.577, entropy_gating=1.729, expert1_balance_top=28.363, expert1_balance_bottom=21.577, unused_expert1_count=0, expert2_balance_top=40.944, expert2_balance_bottom=13.653, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=123540, ups=0.24, wpb=524266, bsz=256, num_updates=4000, lr=0.000592741, gnorm=0.017, clip=0, loss_scale=256, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=17328
2023-06-28 23:26:25 | INFO | train_inner | epoch 001:   4068 / 102400000 loss=0.096, moe_gate_loss=6.00809, overflow_expert1=0.001, overflow_expert2=12.942, entropy_gating=1.74, expert1_balance_top=28.42, expert1_balance_bottom=21.579, unused_expert1_count=0, expert2_balance_top=41.384, expert2_balance_bottom=13.099, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.009, ppl=1.01, wps=124120, ups=0.24, wpb=524382, bsz=256, num_updates=4050, lr=0.000592641, gnorm=0.018, clip=0, loss_scale=512, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=17539
2023-06-28 23:29:58 | INFO | train_inner | epoch 001:   4118 / 102400000 loss=0.096, moe_gate_loss=6.00352, overflow_expert1=0, overflow_expert2=13.922, entropy_gating=1.741, expert1_balance_top=28.267, expert1_balance_bottom=21.701, unused_expert1_count=0, expert2_balance_top=42.64, expert2_balance_bottom=11.938, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.009, ppl=1.01, wps=123280, ups=0.24, wpb=524148, bsz=256, num_updates=4100, lr=0.000592541, gnorm=0.016, clip=0, loss_scale=512, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=17753
2023-06-28 23:31:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-28 23:33:37 | INFO | train_inner | epoch 001:   4169 / 102400000 loss=0.096, moe_gate_loss=6.00582, overflow_expert1=0.001, overflow_expert2=13.619, entropy_gating=1.742, expert1_balance_top=28.194, expert1_balance_bottom=21.787, unused_expert1_count=0, expert2_balance_top=41.991, expert2_balance_bottom=12.384, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.009, ppl=1.01, wps=120276, ups=0.23, wpb=524361, bsz=256, num_updates=4150, lr=0.000592441, gnorm=0.017, clip=0, loss_scale=256, train_wall=218, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=17971
2023-06-28 23:37:07 | INFO | train_inner | epoch 001:   4219 / 102400000 loss=0.095, moe_gate_loss=6.00637, overflow_expert1=0, overflow_expert2=14.904, entropy_gating=1.743, expert1_balance_top=28.47, expert1_balance_bottom=21.509, unused_expert1_count=0, expert2_balance_top=42.68, expert2_balance_bottom=12.568, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.009, ppl=1.01, wps=124840, ups=0.24, wpb=524215, bsz=256, num_updates=4200, lr=0.00059234, gnorm=0.017, clip=0, loss_scale=256, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=18182
2023-06-28 23:40:42 | INFO | train_inner | epoch 001:   4269 / 102400000 loss=0.096, moe_gate_loss=6.00514, overflow_expert1=0.002, overflow_expert2=14.755, entropy_gating=1.744, expert1_balance_top=28.419, expert1_balance_bottom=21.602, unused_expert1_count=0, expert2_balance_top=42.17, expert2_balance_bottom=12.224, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.01, ppl=1.01, wps=122534, ups=0.23, wpb=524037, bsz=256, num_updates=4250, lr=0.00059224, gnorm=0.018, clip=0, loss_scale=256, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=18396
2023-06-28 23:44:14 | INFO | train_inner | epoch 001:   4319 / 102400000 loss=0.096, moe_gate_loss=6.00742, overflow_expert1=0.001, overflow_expert2=15.253, entropy_gating=1.756, expert1_balance_top=28.447, expert1_balance_bottom=21.58, unused_expert1_count=0, expert2_balance_top=42.382, expert2_balance_bottom=11.892, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.009, ppl=1.01, wps=123848, ups=0.24, wpb=524183, bsz=256, num_updates=4300, lr=0.00059214, gnorm=0.017, clip=0, loss_scale=512, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=18609
2023-06-28 23:47:46 | INFO | train_inner | epoch 001:   4369 / 102400000 loss=0.095, moe_gate_loss=6.00668, overflow_expert1=0.001, overflow_expert2=13.671, entropy_gating=1.752, expert1_balance_top=28.232, expert1_balance_bottom=21.771, unused_expert1_count=0, expert2_balance_top=41.224, expert2_balance_bottom=12.005, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.009, ppl=1.01, wps=124280, ups=0.24, wpb=524214, bsz=256, num_updates=4350, lr=0.00059204, gnorm=0.016, clip=0, loss_scale=512, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=18820
2023-06-28 23:50:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-06-28 23:51:25 | INFO | train_inner | epoch 001:   4420 / 102400000 loss=0.095, moe_gate_loss=6.00837, overflow_expert1=0, overflow_expert2=11.13, entropy_gating=1.769, expert1_balance_top=28.196, expert1_balance_bottom=21.826, unused_expert1_count=0, expert2_balance_top=38.629, expert2_balance_bottom=12.521, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=119883, ups=0.23, wpb=524220, bsz=256, num_updates=4400, lr=0.00059194, gnorm=0.016, clip=0, loss_scale=512, train_wall=218, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=19040
2023-06-28 23:52:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-28 23:55:00 | INFO | train_inner | epoch 001:   4471 / 102400000 loss=0.095, moe_gate_loss=6.01032, overflow_expert1=0, overflow_expert2=8.025, entropy_gating=1.761, expert1_balance_top=28.21, expert1_balance_bottom=21.803, unused_expert1_count=0, expert2_balance_top=36.089, expert2_balance_bottom=14.492, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=122056, ups=0.23, wpb=524213, bsz=256, num_updates=4450, lr=0.00059184, gnorm=0.016, clip=0, loss_scale=256, train_wall=214, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=19255
2023-06-28 23:58:35 | INFO | train_inner | epoch 001:   4521 / 102400000 loss=0.095, moe_gate_loss=6.01603, overflow_expert1=0, overflow_expert2=6.299, entropy_gating=1.755, expert1_balance_top=28.998, expert1_balance_bottom=21.05, unused_expert1_count=0, expert2_balance_top=34.162, expert2_balance_bottom=15.795, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.009, ppl=1.01, wps=122707, ups=0.23, wpb=524190, bsz=256, num_updates=4500, lr=0.00059174, gnorm=0.019, clip=0, loss_scale=256, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=19469
2023-06-29 00:02:05 | INFO | train_inner | epoch 001:   4571 / 102400000 loss=0.094, moe_gate_loss=6.01019, overflow_expert1=0, overflow_expert2=4.412, entropy_gating=1.765, expert1_balance_top=28.465, expert1_balance_bottom=21.571, unused_expert1_count=0, expert2_balance_top=32.279, expert2_balance_bottom=17.392, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=124985, ups=0.24, wpb=524136, bsz=256, num_updates=4550, lr=0.00059164, gnorm=0.014, clip=0, loss_scale=512, train_wall=209, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=19680
2023-06-29 00:05:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 00:05:40 | INFO | train_inner | epoch 001:   4622 / 102400000 loss=0.093, moe_gate_loss=6.00734, overflow_expert1=0, overflow_expert2=4.177, entropy_gating=1.791, expert1_balance_top=28.142, expert1_balance_bottom=21.871, unused_expert1_count=0, expert2_balance_top=31.773, expert2_balance_bottom=17.636, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=121966, ups=0.23, wpb=524223, bsz=256, num_updates=4600, lr=0.000591539, gnorm=0.013, clip=0, loss_scale=256, train_wall=215, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=19895
2023-06-29 00:09:14 | INFO | train_inner | epoch 001:   4672 / 102400000 loss=0.093, moe_gate_loss=6.00754, overflow_expert1=0, overflow_expert2=3.4, entropy_gating=1.788, expert1_balance_top=28.215, expert1_balance_bottom=21.808, unused_expert1_count=0, expert2_balance_top=30.984, expert2_balance_bottom=18.159, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=123240, ups=0.24, wpb=524205, bsz=256, num_updates=4650, lr=0.000591439, gnorm=0.013, clip=0, loss_scale=256, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=20109
2023-06-29 00:09:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-06-29 00:12:48 | INFO | train_inner | epoch 001:   4723 / 102400000 loss=0.093, moe_gate_loss=6.00648, overflow_expert1=0, overflow_expert2=2.642, entropy_gating=1.789, expert1_balance_top=28.175, expert1_balance_bottom=21.836, unused_expert1_count=0, expert2_balance_top=30.269, expert2_balance_bottom=18.896, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=122720, ups=0.23, wpb=524256, bsz=256, num_updates=4700, lr=0.000591339, gnorm=0.013, clip=0, loss_scale=128, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=20323
2023-06-29 00:16:21 | INFO | train_inner | epoch 001:   4773 / 102400000 loss=0.093, moe_gate_loss=6.0072, overflow_expert1=0, overflow_expert2=3.396, entropy_gating=1.782, expert1_balance_top=28.15, expert1_balance_bottom=21.862, unused_expert1_count=0, expert2_balance_top=30.876, expert2_balance_bottom=18.485, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=123236, ups=0.24, wpb=524229, bsz=256, num_updates=4750, lr=0.000591239, gnorm=0.013, clip=0, loss_scale=128, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=20536
2023-06-29 00:19:53 | INFO | train_inner | epoch 001:   4823 / 102400000 loss=0.093, moe_gate_loss=6.00719, overflow_expert1=0, overflow_expert2=4.229, entropy_gating=1.778, expert1_balance_top=28.278, expert1_balance_bottom=21.747, unused_expert1_count=0, expert2_balance_top=31.791, expert2_balance_bottom=17.933, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=124486, ups=0.24, wpb=524217, bsz=256, num_updates=4800, lr=0.000591139, gnorm=0.013, clip=0, loss_scale=256, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=20747
2023-06-29 00:23:23 | INFO | train_inner | epoch 001:   4873 / 102400000 loss=0.093, moe_gate_loss=6.00532, overflow_expert1=0, overflow_expert2=4.454, entropy_gating=1.788, expert1_balance_top=27.877, expert1_balance_bottom=22.134, unused_expert1_count=0, expert2_balance_top=32.013, expert2_balance_bottom=17.775, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=125001, ups=0.24, wpb=524186, bsz=256, num_updates=4850, lr=0.000591039, gnorm=0.012, clip=0, loss_scale=256, train_wall=209, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=20958
2023-06-29 00:26:57 | INFO | train_inner | epoch 001:   4923 / 102400000 loss=0.094, moe_gate_loss=6.00776, overflow_expert1=0, overflow_expert2=4.182, entropy_gating=1.785, expert1_balance_top=28.39, expert1_balance_bottom=21.616, unused_expert1_count=0, expert2_balance_top=31.946, expert2_balance_bottom=17.957, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=122833, ups=0.23, wpb=524217, bsz=256, num_updates=4900, lr=0.000590939, gnorm=0.014, clip=0, loss_scale=256, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=21172
2023-06-29 00:30:28 | INFO | train_inner | epoch 001:   4973 / 102400000 loss=0.094, moe_gate_loss=6.00747, overflow_expert1=0, overflow_expert2=4.5, entropy_gating=1.784, expert1_balance_top=28.405, expert1_balance_bottom=21.618, unused_expert1_count=0, expert2_balance_top=32.324, expert2_balance_bottom=17.27, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=124392, ups=0.24, wpb=524181, bsz=256, num_updates=4950, lr=0.000590839, gnorm=0.014, clip=0, loss_scale=512, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=21383
2023-06-29 00:30:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 00:34:08 | INFO | train_inner | epoch 001:   5024 / 102400000 loss=0.094, moe_gate_loss=6.00519, overflow_expert1=0, overflow_expert2=4.451, entropy_gating=1.792, expert1_balance_top=28.134, expert1_balance_bottom=21.881, unused_expert1_count=0, expert2_balance_top=32.119, expert2_balance_bottom=17.468, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=119986, ups=0.23, wpb=524236, bsz=256, num_updates=5000, lr=0.000590738, gnorm=0.013, clip=0, loss_scale=256, train_wall=218, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=21602
2023-06-29 00:34:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 5000 updates
2023-06-29 00:34:08 | INFO | fairseq.trainer | Saving checkpoint to /mnt1/msranlpintern/wuxun/MoE/MoE_results/mhmoe_v1/small-baseline-redstone_v2-flash_attn-8experts-2heads/checkpoint_1_5000-rank-0.pt
2023-06-29 00:34:13 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt1/msranlpintern/wuxun/MoE/MoE_results/mhmoe_v1/small-baseline-redstone_v2-flash_attn-8experts-2heads/checkpoint_1_5000-rank-0.pt
2023-06-29 00:34:13 | INFO | fairseq.trainer | Saving checkpoint to /mnt1/msranlpintern/wuxun/MoE/MoE_results/mhmoe_v1/small-baseline-redstone_v2-flash_attn-8experts-2heads/checkpoint_1_5000-shared.pt
2023-06-29 00:34:22 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt1/msranlpintern/wuxun/MoE/MoE_results/mhmoe_v1/small-baseline-redstone_v2-flash_attn-8experts-2heads/checkpoint_1_5000-shared.pt
2023-06-29 00:34:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /mnt1/msranlpintern/wuxun/MoE/MoE_results/mhmoe_v1/small-baseline-redstone_v2-flash_attn-8experts-2heads/checkpoint_1_5000-rank-0.pt (epoch 1 @ 5000 updates, score None) (writing took 25.073870624997653 seconds)
2023-06-29 00:38:04 | INFO | train_inner | epoch 001:   5074 / 102400000 loss=0.094, moe_gate_loss=6.00696, overflow_expert1=0, overflow_expert2=4.98, entropy_gating=1.788, expert1_balance_top=28.58, expert1_balance_bottom=21.432, unused_expert1_count=0, expert2_balance_top=33.129, expert2_balance_bottom=16.55, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1.01, wps=111280, ups=0.21, wpb=524328, bsz=256, num_updates=5050, lr=0.000590638, gnorm=0.015, clip=0, loss_scale=256, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=21838
2023-06-29 00:40:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 00:41:40 | INFO | train_inner | epoch 001:   5125 / 102400000 loss=0.093, moe_gate_loss=6.00632, overflow_expert1=0, overflow_expert2=4.795, entropy_gating=1.79, expert1_balance_top=28.38, expert1_balance_bottom=21.648, unused_expert1_count=0, expert2_balance_top=32.835, expert2_balance_bottom=16.905, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=121384, ups=0.23, wpb=524322, bsz=256, num_updates=5100, lr=0.000590538, gnorm=0.014, clip=0, loss_scale=256, train_wall=216, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=22055
2023-06-29 00:45:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-06-29 00:45:18 | INFO | train_inner | epoch 001:   5176 / 102400000 loss=0.093, moe_gate_loss=6.00468, overflow_expert1=0, overflow_expert2=5.191, entropy_gating=1.782, expert1_balance_top=28.085, expert1_balance_bottom=21.934, unused_expert1_count=0, expert2_balance_top=33.067, expert2_balance_bottom=16.573, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=120929, ups=0.23, wpb=524168, bsz=256, num_updates=5150, lr=0.000590438, gnorm=0.013, clip=0, loss_scale=128, train_wall=216, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=22273
2023-06-29 00:48:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-06-29 00:48:54 | INFO | train_inner | epoch 001:   5227 / 102400000 loss=0.093, moe_gate_loss=6.00669, overflow_expert1=0, overflow_expert2=6.003, entropy_gating=1.785, expert1_balance_top=28.323, expert1_balance_bottom=21.69, unused_expert1_count=0, expert2_balance_top=33.858, expert2_balance_bottom=15.789, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=121479, ups=0.23, wpb=524333, bsz=256, num_updates=5200, lr=0.000590338, gnorm=0.014, clip=0, loss_scale=64, train_wall=215, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=22489
2023-06-29 00:52:29 | INFO | train_inner | epoch 001:   5277 / 102400000 loss=0.094, moe_gate_loss=6.00582, overflow_expert1=0, overflow_expert2=6.961, entropy_gating=1.792, expert1_balance_top=28.1, expert1_balance_bottom=21.925, unused_expert1_count=0, expert2_balance_top=34.825, expert2_balance_bottom=15.19, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=122572, ups=0.23, wpb=524080, bsz=256, num_updates=5250, lr=0.000590238, gnorm=0.013, clip=0, loss_scale=64, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=22704
2023-06-29 00:56:00 | INFO | train_inner | epoch 001:   5327 / 102400000 loss=0.094, moe_gate_loss=6.00841, overflow_expert1=0.001, overflow_expert2=7.25, entropy_gating=1.788, expert1_balance_top=28.416, expert1_balance_bottom=21.62, unused_expert1_count=0, expert2_balance_top=35.338, expert2_balance_bottom=14.411, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=124465, ups=0.24, wpb=524233, bsz=256, num_updates=5300, lr=0.000590138, gnorm=0.014, clip=0, loss_scale=64, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=22915
2023-06-29 00:59:34 | INFO | train_inner | epoch 001:   5377 / 102400000 loss=0.093, moe_gate_loss=6.00618, overflow_expert1=0, overflow_expert2=7.533, entropy_gating=1.787, expert1_balance_top=27.943, expert1_balance_bottom=22.096, unused_expert1_count=0, expert2_balance_top=35.529, expert2_balance_bottom=13.944, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=122993, ups=0.23, wpb=524233, bsz=256, num_updates=5350, lr=0.000590038, gnorm=0.011, clip=0, loss_scale=128, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=23129
2023-06-29 01:03:06 | INFO | train_inner | epoch 001:   5427 / 102400000 loss=0.093, moe_gate_loss=6.00685, overflow_expert1=0.001, overflow_expert2=7.923, entropy_gating=1.777, expert1_balance_top=28.39, expert1_balance_bottom=21.618, unused_expert1_count=0, expert2_balance_top=36.062, expert2_balance_bottom=13.813, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=123868, ups=0.24, wpb=524182, bsz=256, num_updates=5400, lr=0.000589937, gnorm=0.013, clip=0, loss_scale=128, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=23341
2023-06-29 01:06:38 | INFO | train_inner | epoch 001:   5477 / 102400000 loss=0.094, moe_gate_loss=6.00721, overflow_expert1=0, overflow_expert2=8.268, entropy_gating=1.78, expert1_balance_top=28.421, expert1_balance_bottom=21.603, unused_expert1_count=0, expert2_balance_top=36.237, expert2_balance_bottom=13.725, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=124272, ups=0.24, wpb=524137, bsz=256, num_updates=5450, lr=0.000589837, gnorm=0.014, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=23553
2023-06-29 01:10:13 | INFO | train_inner | epoch 001:   5527 / 102400000 loss=0.094, moe_gate_loss=6.00631, overflow_expert1=0, overflow_expert2=7.929, entropy_gating=1.802, expert1_balance_top=28.272, expert1_balance_bottom=21.752, unused_expert1_count=0, expert2_balance_top=35.802, expert2_balance_bottom=14.273, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=122458, ups=0.23, wpb=524322, bsz=256, num_updates=5500, lr=0.000589737, gnorm=0.013, clip=0, loss_scale=256, train_wall=214, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=23767
2023-06-29 01:13:45 | INFO | train_inner | epoch 001:   5577 / 102400000 loss=0.093, moe_gate_loss=6.00642, overflow_expert1=0, overflow_expert2=8.228, entropy_gating=1.806, expert1_balance_top=28.431, expert1_balance_bottom=21.574, unused_expert1_count=0, expert2_balance_top=36.199, expert2_balance_bottom=14.398, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=124049, ups=0.24, wpb=524257, bsz=256, num_updates=5550, lr=0.000589637, gnorm=0.014, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=23979
2023-06-29 01:16:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 01:17:22 | INFO | train_inner | epoch 001:   5628 / 102400000 loss=0.093, moe_gate_loss=6.0056, overflow_expert1=0, overflow_expert2=7.974, entropy_gating=1.818, expert1_balance_top=28.142, expert1_balance_bottom=21.876, unused_expert1_count=0, expert2_balance_top=35.697, expert2_balance_bottom=14.493, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=121313, ups=0.23, wpb=524343, bsz=256, num_updates=5600, lr=0.000589537, gnorm=0.012, clip=0, loss_scale=256, train_wall=216, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=24196
2023-06-29 01:19:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-06-29 01:21:00 | INFO | train_inner | epoch 001:   5679 / 102400000 loss=0.093, moe_gate_loss=6.00565, overflow_expert1=0, overflow_expert2=8.264, entropy_gating=1.803, expert1_balance_top=28.232, expert1_balance_bottom=21.795, unused_expert1_count=0, expert2_balance_top=35.92, expert2_balance_bottom=13.926, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=120314, ups=0.23, wpb=524190, bsz=256, num_updates=5650, lr=0.000589437, gnorm=0.013, clip=0, loss_scale=128, train_wall=218, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=24415
2023-06-29 01:24:32 | INFO | train_inner | epoch 001:   5729 / 102400000 loss=0.093, moe_gate_loss=6.00588, overflow_expert1=0, overflow_expert2=9.098, entropy_gating=1.8, expert1_balance_top=28.302, expert1_balance_bottom=21.713, unused_expert1_count=0, expert2_balance_top=36.625, expert2_balance_bottom=12.939, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=124458, ups=0.24, wpb=524270, bsz=256, num_updates=5700, lr=0.000589337, gnorm=0.012, clip=0, loss_scale=128, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=24626
2023-06-29 01:28:05 | INFO | train_inner | epoch 001:   5779 / 102400000 loss=0.093, moe_gate_loss=6.00433, overflow_expert1=0.001, overflow_expert2=9.341, entropy_gating=1.79, expert1_balance_top=27.994, expert1_balance_bottom=22.01, unused_expert1_count=0, expert2_balance_top=36.957, expert2_balance_bottom=12.99, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=123354, ups=0.24, wpb=524244, bsz=256, num_updates=5750, lr=0.000589237, gnorm=0.012, clip=0, loss_scale=128, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=24839
2023-06-29 01:31:36 | INFO | train_inner | epoch 001:   5829 / 102400000 loss=0.093, moe_gate_loss=6.00572, overflow_expert1=0.001, overflow_expert2=10.249, entropy_gating=1.796, expert1_balance_top=27.949, expert1_balance_bottom=22.055, unused_expert1_count=0, expert2_balance_top=37.816, expert2_balance_bottom=12.276, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=124167, ups=0.24, wpb=524234, bsz=256, num_updates=5800, lr=0.000589136, gnorm=0.011, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=25051
2023-06-29 01:35:09 | INFO | train_inner | epoch 001:   5879 / 102400000 loss=0.094, moe_gate_loss=6.00732, overflow_expert1=0, overflow_expert2=11.54, entropy_gating=1.8, expert1_balance_top=28.248, expert1_balance_bottom=21.763, unused_expert1_count=0, expert2_balance_top=39.009, expert2_balance_bottom=11.829, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=123927, ups=0.24, wpb=524230, bsz=256, num_updates=5850, lr=0.000589036, gnorm=0.013, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=25263
2023-06-29 01:38:42 | INFO | train_inner | epoch 001:   5929 / 102400000 loss=0.094, moe_gate_loss=6.00836, overflow_expert1=0, overflow_expert2=11.876, entropy_gating=1.818, expert1_balance_top=28.541, expert1_balance_bottom=21.483, unused_expert1_count=0, expert2_balance_top=39.2, expert2_balance_bottom=11.84, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=123267, ups=0.24, wpb=524219, bsz=256, num_updates=5900, lr=0.000588936, gnorm=0.014, clip=0, loss_scale=512, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=25477
2023-06-29 01:39:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 01:42:17 | INFO | train_inner | epoch 001:   5980 / 102400000 loss=0.094, moe_gate_loss=6.00585, overflow_expert1=0, overflow_expert2=11.671, entropy_gating=1.827, expert1_balance_top=28.166, expert1_balance_bottom=21.858, unused_expert1_count=0, expert2_balance_top=39.415, expert2_balance_bottom=12.258, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1.01, wps=122383, ups=0.23, wpb=524237, bsz=256, num_updates=5950, lr=0.000588836, gnorm=0.013, clip=0, loss_scale=256, train_wall=214, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=25692
2023-06-29 01:45:51 | INFO | train_inner | epoch 001:   6030 / 102400000 loss=0.094, moe_gate_loss=6.00957, overflow_expert1=0.001, overflow_expert2=11.873, entropy_gating=1.828, expert1_balance_top=28.62, expert1_balance_bottom=21.428, unused_expert1_count=0, expert2_balance_top=39.399, expert2_balance_bottom=12.399, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=122654, ups=0.23, wpb=524271, bsz=256, num_updates=6000, lr=0.000588736, gnorm=0.014, clip=0, loss_scale=256, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=25906
2023-06-29 01:48:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 01:49:28 | INFO | train_inner | epoch 001:   6081 / 102400000 loss=0.093, moe_gate_loss=6.00547, overflow_expert1=0, overflow_expert2=11.81, entropy_gating=1.831, expert1_balance_top=28.074, expert1_balance_bottom=21.947, unused_expert1_count=0, expert2_balance_top=39.592, expert2_balance_bottom=12.383, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=121489, ups=0.23, wpb=524146, bsz=256, num_updates=6050, lr=0.000588636, gnorm=0.012, clip=0, loss_scale=256, train_wall=215, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=26122
2023-06-29 01:53:00 | INFO | train_inner | epoch 001:   6131 / 102400000 loss=0.093, moe_gate_loss=6.00413, overflow_expert1=0, overflow_expert2=11.851, entropy_gating=1.836, expert1_balance_top=27.848, expert1_balance_bottom=22.181, unused_expert1_count=0, expert2_balance_top=39.904, expert2_balance_bottom=12.4, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=123850, ups=0.24, wpb=524127, bsz=256, num_updates=6100, lr=0.000588536, gnorm=0.012, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=26335
2023-06-29 01:56:34 | INFO | train_inner | epoch 001:   6181 / 102400000 loss=0.094, moe_gate_loss=6.00805, overflow_expert1=0, overflow_expert2=11.646, entropy_gating=1.845, expert1_balance_top=28.332, expert1_balance_bottom=21.729, unused_expert1_count=0, expert2_balance_top=39.28, expert2_balance_bottom=12.661, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=122902, ups=0.23, wpb=524175, bsz=256, num_updates=6150, lr=0.000588436, gnorm=0.013, clip=0, loss_scale=256, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=26549
2023-06-29 01:59:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 02:00:10 | INFO | train_inner | epoch 001:   6232 / 102400000 loss=0.094, moe_gate_loss=6.00857, overflow_expert1=0.001, overflow_expert2=10.322, entropy_gating=1.861, expert1_balance_top=28.303, expert1_balance_bottom=21.766, unused_expert1_count=0, expert2_balance_top=37.644, expert2_balance_bottom=14.045, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=121517, ups=0.23, wpb=524322, bsz=256, num_updates=6200, lr=0.000588335, gnorm=0.013, clip=0, loss_scale=256, train_wall=215, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=26765
2023-06-29 02:03:46 | INFO | train_inner | epoch 001:   6282 / 102400000 loss=0.094, moe_gate_loss=6.0089, overflow_expert1=0.001, overflow_expert2=9.756, entropy_gating=1.876, expert1_balance_top=28.41, expert1_balance_bottom=21.624, unused_expert1_count=0, expert2_balance_top=36.618, expert2_balance_bottom=15.01, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=122077, ups=0.23, wpb=524111, bsz=256, num_updates=6250, lr=0.000588235, gnorm=0.013, clip=0, loss_scale=256, train_wall=214, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=26980
2023-06-29 02:05:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-06-29 02:07:20 | INFO | train_inner | epoch 001:   6333 / 102400000 loss=0.093, moe_gate_loss=6.00693, overflow_expert1=0, overflow_expert2=8.463, entropy_gating=1.894, expert1_balance_top=28.348, expert1_balance_bottom=21.693, unused_expert1_count=0, expert2_balance_top=35.182, expert2_balance_bottom=15.818, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=122778, ups=0.23, wpb=524193, bsz=256, num_updates=6300, lr=0.000588135, gnorm=0.013, clip=0, loss_scale=128, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=27195
2023-06-29 02:10:53 | INFO | train_inner | epoch 001:   6383 / 102400000 loss=0.093, moe_gate_loss=6.00749, overflow_expert1=0, overflow_expert2=8.599, entropy_gating=1.892, expert1_balance_top=28.217, expert1_balance_bottom=21.809, unused_expert1_count=0, expert2_balance_top=35.239, expert2_balance_bottom=15.96, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=123575, ups=0.24, wpb=523990, bsz=256, num_updates=6350, lr=0.000588035, gnorm=0.013, clip=0, loss_scale=128, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=27407
2023-06-29 02:14:26 | INFO | train_inner | epoch 001:   6433 / 102400000 loss=0.094, moe_gate_loss=6.01085, overflow_expert1=0.001, overflow_expert2=10.08, entropy_gating=1.896, expert1_balance_top=28.788, expert1_balance_bottom=21.245, unused_expert1_count=0, expert2_balance_top=36.766, expert2_balance_bottom=14.887, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=123311, ups=0.24, wpb=524194, bsz=256, num_updates=6400, lr=0.000587935, gnorm=0.015, clip=0, loss_scale=128, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=27621
2023-06-29 02:17:58 | INFO | train_inner | epoch 001:   6483 / 102400000 loss=0.095, moe_gate_loss=6.0095, overflow_expert1=0, overflow_expert2=11.55, entropy_gating=1.902, expert1_balance_top=28.627, expert1_balance_bottom=21.386, unused_expert1_count=0, expert2_balance_top=38.093, expert2_balance_bottom=13.67, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=124572, ups=0.24, wpb=524299, bsz=256, num_updates=6450, lr=0.000587835, gnorm=0.014, clip=0, loss_scale=256, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=27832
2023-06-29 02:21:32 | INFO | train_inner | epoch 001:   6533 / 102400000 loss=0.094, moe_gate_loss=6.00699, overflow_expert1=0, overflow_expert2=11.691, entropy_gating=1.906, expert1_balance_top=28.26, expert1_balance_bottom=21.766, unused_expert1_count=0, expert2_balance_top=38.196, expert2_balance_bottom=13.399, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=122658, ups=0.23, wpb=524457, bsz=256, num_updates=6500, lr=0.000587735, gnorm=0.013, clip=0, loss_scale=256, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=28047
2023-06-29 02:23:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-06-29 02:25:07 | INFO | train_inner | epoch 001:   6584 / 102400000 loss=0.094, moe_gate_loss=6.00765, overflow_expert1=0.001, overflow_expert2=11.505, entropy_gating=1.906, expert1_balance_top=28.428, expert1_balance_bottom=21.615, unused_expert1_count=0, expert2_balance_top=38.166, expert2_balance_bottom=13.268, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=122109, ups=0.23, wpb=524322, bsz=256, num_updates=6550, lr=0.000587635, gnorm=0.014, clip=0, loss_scale=128, train_wall=214, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=28262
2023-06-29 02:28:41 | INFO | train_inner | epoch 001:   6634 / 102400000 loss=0.094, moe_gate_loss=6.0083, overflow_expert1=0, overflow_expert2=11.931, entropy_gating=1.918, expert1_balance_top=28.61, expert1_balance_bottom=21.443, unused_expert1_count=0, expert2_balance_top=38.607, expert2_balance_bottom=12.877, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=123297, ups=0.24, wpb=524222, bsz=256, num_updates=6600, lr=0.000587534, gnorm=0.014, clip=0, loss_scale=128, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=28475
2023-06-29 02:32:14 | INFO | train_inner | epoch 001:   6684 / 102400000 loss=0.094, moe_gate_loss=6.00872, overflow_expert1=0.001, overflow_expert2=13.373, entropy_gating=1.92, expert1_balance_top=28.476, expert1_balance_bottom=21.593, unused_expert1_count=0, expert2_balance_top=40.137, expert2_balance_bottom=12.43, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1.01, wps=123395, ups=0.24, wpb=524234, bsz=256, num_updates=6650, lr=0.000587434, gnorm=0.014, clip=0, loss_scale=128, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=28689
2023-06-29 02:35:45 | INFO | train_inner | epoch 001:   6734 / 102400000 loss=0.094, moe_gate_loss=6.00838, overflow_expert1=0, overflow_expert2=14.877, entropy_gating=1.914, expert1_balance_top=28.504, expert1_balance_bottom=21.555, unused_expert1_count=0, expert2_balance_top=41.853, expert2_balance_bottom=12.042, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.008, ppl=1.01, wps=124392, ups=0.24, wpb=524231, bsz=256, num_updates=6700, lr=0.000587334, gnorm=0.014, clip=0, loss_scale=256, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=28900
2023-06-29 02:39:20 | INFO | train_inner | epoch 001:   6784 / 102400000 loss=0.094, moe_gate_loss=6.00998, overflow_expert1=0.001, overflow_expert2=13.446, entropy_gating=1.92, expert1_balance_top=28.667, expert1_balance_bottom=21.405, unused_expert1_count=0, expert2_balance_top=40.036, expert2_balance_bottom=13.807, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1.01, wps=122356, ups=0.23, wpb=524391, bsz=256, num_updates=6750, lr=0.000587234, gnorm=0.014, clip=0, loss_scale=256, train_wall=214, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=29115
2023-06-29 02:41:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 02:42:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-06-29 02:42:59 | INFO | train_inner | epoch 001:   6836 / 102400000 loss=0.094, moe_gate_loss=6.00918, overflow_expert1=0.001, overflow_expert2=11.901, entropy_gating=1.919, expert1_balance_top=28.734, expert1_balance_bottom=21.353, unused_expert1_count=0, expert2_balance_top=38.569, expert2_balance_bottom=14.099, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=119995, ups=0.23, wpb=524166, bsz=256, num_updates=6800, lr=0.000587134, gnorm=0.014, clip=0, loss_scale=128, train_wall=218, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=29334
2023-06-29 02:46:33 | INFO | train_inner | epoch 001:   6886 / 102400000 loss=0.093, moe_gate_loss=6.00826, overflow_expert1=0, overflow_expert2=9.976, entropy_gating=1.92, expert1_balance_top=28.509, expert1_balance_bottom=21.603, unused_expert1_count=0, expert2_balance_top=36.371, expert2_balance_bottom=15.466, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=123112, ups=0.23, wpb=524324, bsz=256, num_updates=6850, lr=0.000587034, gnorm=0.013, clip=0, loss_scale=128, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=29548
2023-06-29 02:50:07 | INFO | train_inner | epoch 001:   6936 / 102400000 loss=0.093, moe_gate_loss=6.00887, overflow_expert1=0.001, overflow_expert2=7.556, entropy_gating=1.913, expert1_balance_top=28.557, expert1_balance_bottom=21.526, unused_expert1_count=0, expert2_balance_top=33.635, expert2_balance_bottom=16.789, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=122970, ups=0.23, wpb=524293, bsz=256, num_updates=6900, lr=0.000586934, gnorm=0.013, clip=0, loss_scale=128, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=29762
2023-06-29 02:53:39 | INFO | train_inner | epoch 001:   6986 / 102400000 loss=0.093, moe_gate_loss=6.01105, overflow_expert1=0, overflow_expert2=5.113, entropy_gating=1.922, expert1_balance_top=28.817, expert1_balance_bottom=21.266, unused_expert1_count=0, expert2_balance_top=30.742, expert2_balance_bottom=18.808, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=124088, ups=0.24, wpb=524289, bsz=256, num_updates=6950, lr=0.000586834, gnorm=0.014, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=29974
2023-06-29 02:56:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-06-29 02:57:17 | INFO | train_inner | epoch 001:   7037 / 102400000 loss=0.093, moe_gate_loss=6.00802, overflow_expert1=0, overflow_expert2=3.895, entropy_gating=1.916, expert1_balance_top=28.483, expert1_balance_bottom=21.576, unused_expert1_count=0, expert2_balance_top=29.847, expert2_balance_bottom=19.493, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=120460, ups=0.23, wpb=524393, bsz=256, num_updates=7000, lr=0.000586733, gnorm=0.013, clip=0, loss_scale=128, train_wall=217, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=30192
2023-06-29 03:00:49 | INFO | train_inner | epoch 001:   7087 / 102400000 loss=0.093, moe_gate_loss=6.00786, overflow_expert1=0.001, overflow_expert2=3.988, entropy_gating=1.922, expert1_balance_top=28.726, expert1_balance_bottom=21.362, unused_expert1_count=0, expert2_balance_top=29.89, expert2_balance_bottom=19.469, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=124014, ups=0.24, wpb=524312, bsz=256, num_updates=7050, lr=0.000586633, gnorm=0.014, clip=0, loss_scale=128, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=30404
2023-06-29 03:04:22 | INFO | train_inner | epoch 001:   7137 / 102400000 loss=0.093, moe_gate_loss=6.0076, overflow_expert1=0.001, overflow_expert2=4.139, entropy_gating=1.919, expert1_balance_top=28.699, expert1_balance_bottom=21.381, unused_expert1_count=0, expert2_balance_top=30.168, expert2_balance_bottom=19.433, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=123645, ups=0.24, wpb=524250, bsz=256, num_updates=7100, lr=0.000586533, gnorm=0.013, clip=0, loss_scale=128, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=30617
2023-06-29 03:07:55 | INFO | train_inner | epoch 001:   7187 / 102400000 loss=0.092, moe_gate_loss=6.00728, overflow_expert1=0, overflow_expert2=4.676, entropy_gating=1.917, expert1_balance_top=28.679, expert1_balance_bottom=21.38, unused_expert1_count=0, expert2_balance_top=30.485, expert2_balance_bottom=19.062, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=123795, ups=0.24, wpb=524276, bsz=256, num_updates=7150, lr=0.000586433, gnorm=0.013, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=30829
2023-06-29 03:11:26 | INFO | train_inner | epoch 001:   7237 / 102400000 loss=0.093, moe_gate_loss=6.00638, overflow_expert1=0.001, overflow_expert2=4.68, entropy_gating=1.922, expert1_balance_top=28.621, expert1_balance_bottom=21.406, unused_expert1_count=0, expert2_balance_top=30.806, expert2_balance_bottom=18.75, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=124074, ups=0.24, wpb=523994, bsz=256, num_updates=7200, lr=0.000586333, gnorm=0.012, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=31041
2023-06-29 03:15:01 | INFO | train_inner | epoch 001:   7287 / 102400000 loss=0.093, moe_gate_loss=6.00925, overflow_expert1=0.001, overflow_expert2=5.409, entropy_gating=1.92, expert1_balance_top=29.001, expert1_balance_bottom=21.077, unused_expert1_count=0, expert2_balance_top=31.161, expert2_balance_bottom=18.678, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=122710, ups=0.23, wpb=524131, bsz=256, num_updates=7250, lr=0.000586233, gnorm=0.014, clip=0, loss_scale=512, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=31255
2023-06-29 03:15:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 03:18:36 | INFO | train_inner | epoch 001:   7338 / 102400000 loss=0.093, moe_gate_loss=6.00871, overflow_expert1=0.001, overflow_expert2=6.73, entropy_gating=1.921, expert1_balance_top=28.878, expert1_balance_bottom=21.171, unused_expert1_count=0, expert2_balance_top=32.555, expert2_balance_bottom=17.179, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=121932, ups=0.23, wpb=524178, bsz=256, num_updates=7300, lr=0.000586133, gnorm=0.014, clip=0, loss_scale=256, train_wall=215, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=31471
2023-06-29 03:22:07 | INFO | train_inner | epoch 001:   7388 / 102400000 loss=0.093, moe_gate_loss=6.00534, overflow_expert1=0, overflow_expert2=8.632, entropy_gating=1.923, expert1_balance_top=28.593, expert1_balance_bottom=21.431, unused_expert1_count=0, expert2_balance_top=34.311, expert2_balance_bottom=15.303, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=124668, ups=0.24, wpb=524351, bsz=256, num_updates=7350, lr=0.000586033, gnorm=0.012, clip=0, loss_scale=256, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=31682
2023-06-29 03:25:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 03:25:45 | INFO | train_inner | epoch 001:   7439 / 102400000 loss=0.093, moe_gate_loss=6.0069, overflow_expert1=0, overflow_expert2=9.16, entropy_gating=1.923, expert1_balance_top=28.578, expert1_balance_bottom=21.462, unused_expert1_count=0, expert2_balance_top=34.437, expert2_balance_bottom=14.565, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=120622, ups=0.23, wpb=524194, bsz=256, num_updates=7400, lr=0.000585932, gnorm=0.012, clip=0, loss_scale=256, train_wall=217, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=31900
2023-06-29 03:29:17 | INFO | train_inner | epoch 001:   7489 / 102400000 loss=0.093, moe_gate_loss=6.00756, overflow_expert1=0, overflow_expert2=9.393, entropy_gating=1.93, expert1_balance_top=28.596, expert1_balance_bottom=21.444, unused_expert1_count=0, expert2_balance_top=34.706, expert2_balance_bottom=14.388, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=124322, ups=0.24, wpb=524246, bsz=256, num_updates=7450, lr=0.000585832, gnorm=0.012, clip=0, loss_scale=256, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=32112
2023-06-29 03:31:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-06-29 03:32:56 | INFO | train_inner | epoch 001:   7540 / 102400000 loss=0.093, moe_gate_loss=6.00701, overflow_expert1=0, overflow_expert2=10.75, entropy_gating=1.927, expert1_balance_top=28.326, expert1_balance_bottom=21.708, unused_expert1_count=0, expert2_balance_top=36.508, expert2_balance_bottom=13.241, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=120309, ups=0.23, wpb=524222, bsz=256, num_updates=7500, lr=0.000585732, gnorm=0.012, clip=0, loss_scale=128, train_wall=218, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=32330
2023-06-29 03:36:27 | INFO | train_inner | epoch 001:   7590 / 102400000 loss=0.093, moe_gate_loss=6.00537, overflow_expert1=0, overflow_expert2=10.824, entropy_gating=1.923, expert1_balance_top=28.217, expert1_balance_bottom=21.825, unused_expert1_count=0, expert2_balance_top=36.41, expert2_balance_bottom=12.443, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=124555, ups=0.24, wpb=524249, bsz=256, num_updates=7550, lr=0.000585632, gnorm=0.011, clip=0, loss_scale=128, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=32541
2023-06-29 03:39:59 | INFO | train_inner | epoch 001:   7640 / 102400000 loss=0.093, moe_gate_loss=6.00629, overflow_expert1=0, overflow_expert2=12.272, entropy_gating=1.914, expert1_balance_top=28.273, expert1_balance_bottom=21.75, unused_expert1_count=0, expert2_balance_top=38.108, expert2_balance_bottom=12.534, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=123718, ups=0.24, wpb=524087, bsz=256, num_updates=7600, lr=0.000585532, gnorm=0.011, clip=0, loss_scale=128, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=32754
2023-06-29 03:43:33 | INFO | train_inner | epoch 001:   7690 / 102400000 loss=0.093, moe_gate_loss=6.00845, overflow_expert1=0.001, overflow_expert2=11.859, entropy_gating=1.916, expert1_balance_top=28.671, expert1_balance_bottom=21.405, unused_expert1_count=0, expert2_balance_top=37.518, expert2_balance_bottom=12.577, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=123268, ups=0.24, wpb=524067, bsz=256, num_updates=7650, lr=0.000585432, gnorm=0.013, clip=0, loss_scale=256, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=32967
2023-06-29 03:47:04 | INFO | train_inner | epoch 001:   7740 / 102400000 loss=0.093, moe_gate_loss=6.00801, overflow_expert1=0, overflow_expert2=10.149, entropy_gating=1.931, expert1_balance_top=28.533, expert1_balance_bottom=21.534, unused_expert1_count=0, expert2_balance_top=35.435, expert2_balance_bottom=13.873, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=124496, ups=0.24, wpb=524263, bsz=256, num_updates=7700, lr=0.000585332, gnorm=0.012, clip=0, loss_scale=256, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=33179
2023-06-29 03:50:38 | INFO | train_inner | epoch 001:   7790 / 102400000 loss=0.093, moe_gate_loss=6.0109, overflow_expert1=0, overflow_expert2=10.786, entropy_gating=1.942, expert1_balance_top=28.802, expert1_balance_bottom=21.274, unused_expert1_count=0, expert2_balance_top=34.947, expert2_balance_bottom=14.046, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=123040, ups=0.23, wpb=524379, bsz=256, num_updates=7750, lr=0.000585232, gnorm=0.013, clip=0, loss_scale=512, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=33392
2023-06-29 03:50:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 03:54:14 | INFO | train_inner | epoch 001:   7841 / 102400000 loss=0.093, moe_gate_loss=6.00714, overflow_expert1=0, overflow_expert2=8.734, entropy_gating=1.951, expert1_balance_top=28.399, expert1_balance_bottom=21.678, unused_expert1_count=0, expert2_balance_top=33.445, expert2_balance_bottom=15.441, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=121904, ups=0.23, wpb=524206, bsz=256, num_updates=7800, lr=0.000585131, gnorm=0.012, clip=0, loss_scale=256, train_wall=215, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=33608
2023-06-29 03:54:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-06-29 03:57:49 | INFO | train_inner | epoch 001:   7892 / 102400000 loss=0.093, moe_gate_loss=6.00588, overflow_expert1=0.001, overflow_expert2=7.078, entropy_gating=1.956, expert1_balance_top=28.341, expert1_balance_bottom=21.708, unused_expert1_count=0, expert2_balance_top=32.456, expert2_balance_bottom=16.569, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=121929, ups=0.23, wpb=524084, bsz=256, num_updates=7850, lr=0.000585031, gnorm=0.012, clip=0, loss_scale=128, train_wall=214, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=33824
2023-06-29 04:01:22 | INFO | train_inner | epoch 001:   7942 / 102400000 loss=0.092, moe_gate_loss=6.00525, overflow_expert1=0, overflow_expert2=7.007, entropy_gating=1.954, expert1_balance_top=28.204, expert1_balance_bottom=21.84, unused_expert1_count=0, expert2_balance_top=32.343, expert2_balance_bottom=16.92, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=123744, ups=0.24, wpb=524007, bsz=256, num_updates=7900, lr=0.000584931, gnorm=0.011, clip=0, loss_scale=128, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=34036
2023-06-29 04:02:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-06-29 04:04:58 | INFO | train_inner | epoch 001:   7993 / 102400000 loss=0.092, moe_gate_loss=6.00469, overflow_expert1=0, overflow_expert2=6.548, entropy_gating=1.962, expert1_balance_top=28.184, expert1_balance_bottom=21.843, unused_expert1_count=0, expert2_balance_top=31.73, expert2_balance_bottom=17.657, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=121800, ups=0.23, wpb=524232, bsz=256, num_updates=7950, lr=0.000584831, gnorm=0.011, clip=0, loss_scale=64, train_wall=215, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=34252
2023-06-29 04:08:32 | INFO | train_inner | epoch 001:   8043 / 102400000 loss=0.093, moe_gate_loss=6.00498, overflow_expert1=0, overflow_expert2=5.999, entropy_gating=1.962, expert1_balance_top=28.094, expert1_balance_bottom=21.956, unused_expert1_count=0, expert2_balance_top=31.583, expert2_balance_bottom=17.924, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=122720, ups=0.23, wpb=524217, bsz=256, num_updates=8000, lr=0.000584731, gnorm=0.011, clip=0, loss_scale=64, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=34467
2023-06-29 04:12:02 | INFO | train_inner | epoch 001:   8093 / 102400000 loss=0.093, moe_gate_loss=6.00524, overflow_expert1=0, overflow_expert2=4.863, entropy_gating=1.97, expert1_balance_top=28.218, expert1_balance_bottom=21.812, unused_expert1_count=0, expert2_balance_top=30.907, expert2_balance_bottom=18.742, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=125102, ups=0.24, wpb=524236, bsz=256, num_updates=8050, lr=0.000584631, gnorm=0.011, clip=0, loss_scale=128, train_wall=209, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=34677
2023-06-29 04:15:36 | INFO | train_inner | epoch 001:   8143 / 102400000 loss=0.092, moe_gate_loss=6.00479, overflow_expert1=0, overflow_expert2=5.684, entropy_gating=1.968, expert1_balance_top=27.996, expert1_balance_bottom=22.032, unused_expert1_count=0, expert2_balance_top=31.378, expert2_balance_bottom=18.45, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=123140, ups=0.23, wpb=524312, bsz=256, num_updates=8100, lr=0.000584531, gnorm=0.01, clip=0, loss_scale=128, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=34891
2023-06-29 04:19:09 | INFO | train_inner | epoch 001:   8193 / 102400000 loss=0.092, moe_gate_loss=6.00402, overflow_expert1=0, overflow_expert2=5.537, entropy_gating=1.969, expert1_balance_top=27.896, expert1_balance_bottom=22.124, unused_expert1_count=0, expert2_balance_top=31.315, expert2_balance_bottom=18.417, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=123318, ups=0.24, wpb=524254, bsz=256, num_updates=8150, lr=0.000584431, gnorm=0.01, clip=0, loss_scale=128, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=35104
2023-06-29 04:22:40 | INFO | train_inner | epoch 001:   8243 / 102400000 loss=0.092, moe_gate_loss=6.00408, overflow_expert1=0, overflow_expert2=6.061, entropy_gating=1.976, expert1_balance_top=27.987, expert1_balance_bottom=22.034, unused_expert1_count=0, expert2_balance_top=31.617, expert2_balance_bottom=17.866, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=124985, ups=0.24, wpb=524085, bsz=256, num_updates=8200, lr=0.00058433, gnorm=0.01, clip=0, loss_scale=256, train_wall=209, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=35314
2023-06-29 04:26:14 | INFO | train_inner | epoch 001:   8293 / 102400000 loss=0.093, moe_gate_loss=6.00721, overflow_expert1=0, overflow_expert2=6.202, entropy_gating=1.976, expert1_balance_top=28.522, expert1_balance_bottom=21.524, unused_expert1_count=0, expert2_balance_top=31.524, expert2_balance_bottom=18.114, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=123009, ups=0.23, wpb=524304, bsz=256, num_updates=8250, lr=0.00058423, gnorm=0.012, clip=0, loss_scale=256, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=35528
2023-06-29 04:29:46 | INFO | train_inner | epoch 001:   8343 / 102400000 loss=0.092, moe_gate_loss=6.00496, overflow_expert1=0, overflow_expert2=6.414, entropy_gating=1.974, expert1_balance_top=28.094, expert1_balance_bottom=21.935, unused_expert1_count=0, expert2_balance_top=31.588, expert2_balance_bottom=17.628, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=124119, ups=0.24, wpb=524359, bsz=256, num_updates=8300, lr=0.00058413, gnorm=0.01, clip=0, loss_scale=512, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=35740
2023-06-29 04:30:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 04:33:22 | INFO | train_inner | epoch 001:   8394 / 102400000 loss=0.092, moe_gate_loss=6.00425, overflow_expert1=0, overflow_expert2=7.537, entropy_gating=1.973, expert1_balance_top=27.892, expert1_balance_bottom=22.155, unused_expert1_count=0, expert2_balance_top=32.506, expert2_balance_bottom=17.02, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=121531, ups=0.23, wpb=524142, bsz=256, num_updates=8350, lr=0.00058403, gnorm=0.009, clip=0, loss_scale=256, train_wall=215, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=35957
2023-06-29 04:36:56 | INFO | train_inner | epoch 001:   8444 / 102400000 loss=0.093, moe_gate_loss=6.00437, overflow_expert1=0, overflow_expert2=7.734, entropy_gating=1.977, expert1_balance_top=28.149, expert1_balance_bottom=21.877, unused_expert1_count=0, expert2_balance_top=32.96, expert2_balance_bottom=16.721, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=123037, ups=0.23, wpb=524088, bsz=256, num_updates=8400, lr=0.00058393, gnorm=0.01, clip=0, loss_scale=256, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=36171
2023-06-29 04:40:27 | INFO | train_inner | epoch 001:   8494 / 102400000 loss=0.093, moe_gate_loss=6.00769, overflow_expert1=0, overflow_expert2=8.091, entropy_gating=1.98, expert1_balance_top=28.571, expert1_balance_bottom=21.459, unused_expert1_count=0, expert2_balance_top=33.561, expert2_balance_bottom=16.816, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=124550, ups=0.24, wpb=524218, bsz=256, num_updates=8450, lr=0.00058383, gnorm=0.012, clip=0, loss_scale=512, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=36382
2023-06-29 04:41:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 04:44:07 | INFO | train_inner | epoch 001:   8545 / 102400000 loss=0.092, moe_gate_loss=6.00367, overflow_expert1=0, overflow_expert2=8.59, entropy_gating=1.979, expert1_balance_top=27.71, expert1_balance_bottom=22.317, unused_expert1_count=0, expert2_balance_top=34.028, expert2_balance_bottom=16.061, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=119720, ups=0.23, wpb=524381, bsz=256, num_updates=8500, lr=0.00058373, gnorm=0.009, clip=0, loss_scale=256, train_wall=219, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=36602
2023-06-29 04:47:38 | INFO | train_inner | epoch 001:   8595 / 102400000 loss=0.093, moe_gate_loss=6.00441, overflow_expert1=0, overflow_expert2=7.88, entropy_gating=1.981, expert1_balance_top=27.95, expert1_balance_bottom=22.076, unused_expert1_count=0, expert2_balance_top=33.132, expert2_balance_bottom=16.991, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=124880, ups=0.24, wpb=524310, bsz=256, num_updates=8550, lr=0.00058363, gnorm=0.01, clip=0, loss_scale=256, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=36812
2023-06-29 04:50:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 04:51:16 | INFO | train_inner | epoch 001:   8646 / 102400000 loss=0.093, moe_gate_loss=6.00448, overflow_expert1=0, overflow_expert2=7.704, entropy_gating=1.979, expert1_balance_top=28.11, expert1_balance_bottom=21.923, unused_expert1_count=0, expert2_balance_top=33.078, expert2_balance_bottom=16.946, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=120746, ups=0.23, wpb=524158, bsz=256, num_updates=8600, lr=0.000583529, gnorm=0.011, clip=0, loss_scale=256, train_wall=217, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=37030
2023-06-29 04:54:50 | INFO | train_inner | epoch 001:   8696 / 102400000 loss=0.093, moe_gate_loss=6.00421, overflow_expert1=0, overflow_expert2=8.605, entropy_gating=1.977, expert1_balance_top=28.052, expert1_balance_bottom=21.978, unused_expert1_count=0, expert2_balance_top=33.983, expert2_balance_bottom=16.511, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=122867, ups=0.23, wpb=524254, bsz=256, num_updates=8650, lr=0.000583429, gnorm=0.01, clip=0, loss_scale=256, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=37244
2023-06-29 04:58:20 | INFO | train_inner | epoch 001:   8746 / 102400000 loss=0.093, moe_gate_loss=6.00693, overflow_expert1=0, overflow_expert2=8.714, entropy_gating=1.982, expert1_balance_top=28.425, expert1_balance_bottom=21.634, unused_expert1_count=0, expert2_balance_top=33.913, expert2_balance_bottom=16.686, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=124874, ups=0.24, wpb=524105, bsz=256, num_updates=8700, lr=0.000583329, gnorm=0.011, clip=0, loss_scale=256, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=37455
2023-06-29 04:59:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 05:02:00 | INFO | train_inner | epoch 001:   8797 / 102400000 loss=0.092, moe_gate_loss=6.00384, overflow_expert1=0, overflow_expert2=9.248, entropy_gating=1.981, expert1_balance_top=28.075, expert1_balance_bottom=21.965, unused_expert1_count=0, expert2_balance_top=34.641, expert2_balance_bottom=15.917, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=119740, ups=0.23, wpb=524120, bsz=256, num_updates=8750, lr=0.000583229, gnorm=0.01, clip=0, loss_scale=256, train_wall=218, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=37675
2023-06-29 05:05:31 | INFO | train_inner | epoch 001:   8847 / 102400000 loss=0.092, moe_gate_loss=6.00532, overflow_expert1=0, overflow_expert2=9.643, entropy_gating=1.981, expert1_balance_top=28.361, expert1_balance_bottom=21.703, unused_expert1_count=0, expert2_balance_top=34.98, expert2_balance_bottom=15.743, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=124523, ups=0.24, wpb=524358, bsz=256, num_updates=8800, lr=0.000583129, gnorm=0.01, clip=0, loss_scale=256, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=37886
2023-06-29 05:09:04 | INFO | train_inner | epoch 001:   8897 / 102400000 loss=0.092, moe_gate_loss=6.00441, overflow_expert1=0.002, overflow_expert2=9.746, entropy_gating=1.979, expert1_balance_top=28.175, expert1_balance_bottom=21.88, unused_expert1_count=0, expert2_balance_top=34.984, expert2_balance_bottom=15.84, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123917, ups=0.24, wpb=524371, bsz=256, num_updates=8850, lr=0.000583029, gnorm=0.009, clip=0, loss_scale=512, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=38098
2023-06-29 05:12:37 | INFO | train_inner | epoch 001:   8947 / 102400000 loss=0.092, moe_gate_loss=6.00509, overflow_expert1=0, overflow_expert2=11.114, entropy_gating=1.977, expert1_balance_top=28.179, expert1_balance_bottom=21.856, unused_expert1_count=0, expert2_balance_top=36.332, expert2_balance_bottom=15.249, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=123197, ups=0.23, wpb=524246, bsz=256, num_updates=8900, lr=0.000582929, gnorm=0.01, clip=0, loss_scale=512, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=38312
2023-06-29 05:12:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 05:16:13 | INFO | train_inner | epoch 001:   8998 / 102400000 loss=0.092, moe_gate_loss=6.00456, overflow_expert1=0, overflow_expert2=10.529, entropy_gating=1.977, expert1_balance_top=28.248, expert1_balance_bottom=21.802, unused_expert1_count=0, expert2_balance_top=35.856, expert2_balance_bottom=15.618, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=121752, ups=0.23, wpb=524285, bsz=256, num_updates=8950, lr=0.000582829, gnorm=0.01, clip=0, loss_scale=256, train_wall=215, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=38528
2023-06-29 05:19:48 | INFO | train_inner | epoch 001:   9048 / 102400000 loss=0.093, moe_gate_loss=6.00517, overflow_expert1=0, overflow_expert2=10.798, entropy_gating=1.976, expert1_balance_top=28.255, expert1_balance_bottom=21.804, unused_expert1_count=0, expert2_balance_top=35.377, expert2_balance_bottom=15.082, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=122352, ups=0.23, wpb=524150, bsz=256, num_updates=9000, lr=0.000582728, gnorm=0.011, clip=0, loss_scale=256, train_wall=214, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=38743
2023-06-29 05:22:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 05:23:23 | INFO | train_inner | epoch 001:   9099 / 102400000 loss=0.093, moe_gate_loss=6.00485, overflow_expert1=0, overflow_expert2=11.913, entropy_gating=1.977, expert1_balance_top=28.252, expert1_balance_bottom=21.808, unused_expert1_count=0, expert2_balance_top=36.773, expert2_balance_bottom=14.941, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=122328, ups=0.23, wpb=524194, bsz=256, num_updates=9050, lr=0.000582628, gnorm=0.011, clip=0, loss_scale=256, train_wall=214, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=38958
2023-06-29 05:26:56 | INFO | train_inner | epoch 001:   9149 / 102400000 loss=0.093, moe_gate_loss=6.00822, overflow_expert1=0.001, overflow_expert2=11.392, entropy_gating=1.98, expert1_balance_top=28.727, expert1_balance_bottom=21.367, unused_expert1_count=0, expert2_balance_top=36.049, expert2_balance_bottom=15.354, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=123470, ups=0.24, wpb=524126, bsz=256, num_updates=9100, lr=0.000582528, gnorm=0.012, clip=0, loss_scale=256, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=39171
2023-06-29 05:30:30 | INFO | train_inner | epoch 001:   9199 / 102400000 loss=0.092, moe_gate_loss=6.00622, overflow_expert1=0, overflow_expert2=9.878, entropy_gating=1.985, expert1_balance_top=28.258, expert1_balance_bottom=21.807, unused_expert1_count=0, expert2_balance_top=35.186, expert2_balance_bottom=15.434, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=123439, ups=0.24, wpb=524167, bsz=256, num_updates=9150, lr=0.000582428, gnorm=0.01, clip=0, loss_scale=256, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=39384
2023-06-29 05:31:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 05:34:06 | INFO | train_inner | epoch 001:   9250 / 102400000 loss=0.093, moe_gate_loss=6.00891, overflow_expert1=0, overflow_expert2=8.987, entropy_gating=1.99, expert1_balance_top=28.702, expert1_balance_bottom=21.39, unused_expert1_count=0, expert2_balance_top=34.091, expert2_balance_bottom=16.463, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=121518, ups=0.23, wpb=524131, bsz=256, num_updates=9200, lr=0.000582328, gnorm=0.012, clip=0, loss_scale=256, train_wall=215, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=39601
2023-06-29 05:37:41 | INFO | train_inner | epoch 001:   9300 / 102400000 loss=0.092, moe_gate_loss=6.00707, overflow_expert1=0, overflow_expert2=9.183, entropy_gating=1.992, expert1_balance_top=28.662, expert1_balance_bottom=21.391, unused_expert1_count=0, expert2_balance_top=34.6, expert2_balance_bottom=16.391, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=122606, ups=0.23, wpb=524306, bsz=256, num_updates=9250, lr=0.000582228, gnorm=0.011, clip=0, loss_scale=256, train_wall=214, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=39815
2023-06-29 05:41:12 | INFO | train_inner | epoch 001:   9350 / 102400000 loss=0.093, moe_gate_loss=6.0093, overflow_expert1=0, overflow_expert2=10.166, entropy_gating=1.991, expert1_balance_top=29.008, expert1_balance_bottom=21.104, unused_expert1_count=0, expert2_balance_top=35.258, expert2_balance_bottom=16.6, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=124500, ups=0.24, wpb=524302, bsz=256, num_updates=9300, lr=0.000582128, gnorm=0.013, clip=0, loss_scale=512, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=40027
2023-06-29 05:44:46 | INFO | train_inner | epoch 001:   9400 / 102400000 loss=0.092, moe_gate_loss=6.00832, overflow_expert1=0, overflow_expert2=9.733, entropy_gating=1.993, expert1_balance_top=28.779, expert1_balance_bottom=21.32, unused_expert1_count=0, expert2_balance_top=34.778, expert2_balance_bottom=17.163, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=122863, ups=0.23, wpb=524295, bsz=256, num_updates=9350, lr=0.000582028, gnorm=0.012, clip=0, loss_scale=512, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=40241
2023-06-29 05:47:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 05:48:23 | INFO | train_inner | epoch 001:   9451 / 102400000 loss=0.092, moe_gate_loss=6.00529, overflow_expert1=0, overflow_expert2=9.824, entropy_gating=1.996, expert1_balance_top=28.2, expert1_balance_bottom=21.864, unused_expert1_count=0, expert2_balance_top=35.346, expert2_balance_bottom=16.315, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=121289, ups=0.23, wpb=524124, bsz=256, num_updates=9400, lr=0.000581927, gnorm=0.01, clip=0, loss_scale=256, train_wall=216, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=40458
2023-06-29 05:51:55 | INFO | train_inner | epoch 001:   9501 / 102400000 loss=0.092, moe_gate_loss=6.00498, overflow_expert1=0, overflow_expert2=10.801, entropy_gating=1.995, expert1_balance_top=28.414, expert1_balance_bottom=21.661, unused_expert1_count=0, expert2_balance_top=35.906, expert2_balance_bottom=14.56, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=124540, ups=0.24, wpb=524139, bsz=256, num_updates=9450, lr=0.000581827, gnorm=0.01, clip=0, loss_scale=256, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=40669
2023-06-29 05:55:29 | INFO | train_inner | epoch 001:   9551 / 102400000 loss=0.093, moe_gate_loss=6.00489, overflow_expert1=0, overflow_expert2=11.391, entropy_gating=2.001, expert1_balance_top=28.336, expert1_balance_bottom=21.714, unused_expert1_count=0, expert2_balance_top=36.837, expert2_balance_bottom=14.159, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=122878, ups=0.23, wpb=524440, bsz=256, num_updates=9500, lr=0.000581727, gnorm=0.01, clip=0, loss_scale=256, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=40884
2023-06-29 05:59:02 | INFO | train_inner | epoch 001:   9601 / 102400000 loss=0.093, moe_gate_loss=6.00534, overflow_expert1=0.001, overflow_expert2=12.817, entropy_gating=1.994, expert1_balance_top=28.527, expert1_balance_bottom=21.545, unused_expert1_count=0, expert2_balance_top=38.374, expert2_balance_bottom=13.172, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=123742, ups=0.24, wpb=524241, bsz=256, num_updates=9550, lr=0.000581627, gnorm=0.011, clip=0, loss_scale=512, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=41096
2023-06-29 06:00:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 06:02:38 | INFO | train_inner | epoch 001:   9652 / 102400000 loss=0.092, moe_gate_loss=6.00635, overflow_expert1=0.001, overflow_expert2=11.886, entropy_gating=1.997, expert1_balance_top=28.277, expert1_balance_bottom=21.807, unused_expert1_count=0, expert2_balance_top=36.924, expert2_balance_bottom=13.786, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=121678, ups=0.23, wpb=523955, bsz=256, num_updates=9600, lr=0.000581527, gnorm=0.01, clip=0, loss_scale=256, train_wall=215, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=41312
2023-06-29 06:06:12 | INFO | train_inner | epoch 001:   9702 / 102400000 loss=0.092, moe_gate_loss=6.00512, overflow_expert1=0, overflow_expert2=11.842, entropy_gating=1.992, expert1_balance_top=28.259, expert1_balance_bottom=21.807, unused_expert1_count=0, expert2_balance_top=36.987, expert2_balance_bottom=12.995, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=122802, ups=0.23, wpb=524220, bsz=256, num_updates=9650, lr=0.000581427, gnorm=0.01, clip=0, loss_scale=256, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=41527
2023-06-29 06:09:44 | INFO | train_inner | epoch 001:   9752 / 102400000 loss=0.092, moe_gate_loss=6.00532, overflow_expert1=0, overflow_expert2=12.028, entropy_gating=1.996, expert1_balance_top=28.468, expert1_balance_bottom=21.617, unused_expert1_count=0, expert2_balance_top=37.01, expert2_balance_bottom=13.616, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=124150, ups=0.24, wpb=524228, bsz=256, num_updates=9700, lr=0.000581327, gnorm=0.01, clip=0, loss_scale=512, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=41739
2023-06-29 06:13:18 | INFO | train_inner | epoch 001:   9802 / 102400000 loss=0.092, moe_gate_loss=6.00647, overflow_expert1=0, overflow_expert2=11.678, entropy_gating=1.999, expert1_balance_top=28.443, expert1_balance_bottom=21.64, unused_expert1_count=0, expert2_balance_top=37.141, expert2_balance_bottom=14.756, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=123090, ups=0.23, wpb=524336, bsz=256, num_updates=9750, lr=0.000581227, gnorm=0.011, clip=0, loss_scale=512, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=41953
2023-06-29 06:14:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 06:16:55 | INFO | train_inner | epoch 001:   9853 / 102400000 loss=0.092, moe_gate_loss=6.00616, overflow_expert1=0, overflow_expert2=9.483, entropy_gating=2.002, expert1_balance_top=28.423, expert1_balance_bottom=21.676, unused_expert1_count=0, expert2_balance_top=34.866, expert2_balance_bottom=16.249, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=121660, ups=0.23, wpb=524261, bsz=256, num_updates=9800, lr=0.000581126, gnorm=0.01, clip=0, loss_scale=256, train_wall=215, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=42169
2023-06-29 06:20:27 | INFO | train_inner | epoch 001:   9903 / 102400000 loss=0.092, moe_gate_loss=6.00591, overflow_expert1=0, overflow_expert2=9.451, entropy_gating=2.005, expert1_balance_top=28.368, expert1_balance_bottom=21.704, unused_expert1_count=0, expert2_balance_top=34.714, expert2_balance_bottom=15.857, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123998, ups=0.24, wpb=524264, bsz=256, num_updates=9850, lr=0.000581026, gnorm=0.01, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=42381
2023-06-29 06:22:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-06-29 06:24:05 | INFO | train_inner | epoch 001:   9954 / 102400000 loss=0.092, moe_gate_loss=6.00474, overflow_expert1=0, overflow_expert2=8.969, entropy_gating=2.004, expert1_balance_top=28.119, expert1_balance_bottom=21.951, unused_expert1_count=0, expert2_balance_top=34.128, expert2_balance_bottom=15.859, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=120764, ups=0.23, wpb=524255, bsz=256, num_updates=9900, lr=0.000580926, gnorm=0.009, clip=0, loss_scale=128, train_wall=217, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=42599
2023-06-29 06:27:35 | INFO | train_inner | epoch 001:  10004 / 102400000 loss=0.092, moe_gate_loss=6.00352, overflow_expert1=0, overflow_expert2=9.573, entropy_gating=2.006, expert1_balance_top=27.786, expert1_balance_bottom=22.263, unused_expert1_count=0, expert2_balance_top=35.083, expert2_balance_bottom=15.25, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=125112, ups=0.24, wpb=524239, bsz=256, num_updates=9950, lr=0.000580826, gnorm=0.007, clip=0, loss_scale=128, train_wall=209, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=42810
2023-06-29 06:31:10 | INFO | train_inner | epoch 001:  10054 / 102400000 loss=0.092, moe_gate_loss=6.00342, overflow_expert1=0, overflow_expert2=10.183, entropy_gating=2.002, expert1_balance_top=27.806, expert1_balance_bottom=22.245, unused_expert1_count=0, expert2_balance_top=35.023, expert2_balance_bottom=15.012, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=122594, ups=0.23, wpb=524168, bsz=256, num_updates=10000, lr=0.000580726, gnorm=0.007, clip=0, loss_scale=128, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=43025
2023-06-29 06:31:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 10000 updates
2023-06-29 06:31:10 | INFO | fairseq.trainer | Saving checkpoint to /mnt1/msranlpintern/wuxun/MoE/MoE_results/mhmoe_v1/small-baseline-redstone_v2-flash_attn-8experts-2heads/checkpoint_1_10000-rank-0.pt
2023-06-29 06:31:15 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt1/msranlpintern/wuxun/MoE/MoE_results/mhmoe_v1/small-baseline-redstone_v2-flash_attn-8experts-2heads/checkpoint_1_10000-rank-0.pt
2023-06-29 06:31:15 | INFO | fairseq.trainer | Saving checkpoint to /mnt1/msranlpintern/wuxun/MoE/MoE_results/mhmoe_v1/small-baseline-redstone_v2-flash_attn-8experts-2heads/checkpoint_1_10000-shared.pt
2023-06-29 06:31:26 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt1/msranlpintern/wuxun/MoE/MoE_results/mhmoe_v1/small-baseline-redstone_v2-flash_attn-8experts-2heads/checkpoint_1_10000-shared.pt
2023-06-29 06:31:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /mnt1/msranlpintern/wuxun/MoE/MoE_results/mhmoe_v1/small-baseline-redstone_v2-flash_attn-8experts-2heads/checkpoint_1_10000-rank-0.pt (epoch 1 @ 10000 updates, score None) (writing took 31.298284268006682 seconds)
2023-06-29 06:35:12 | INFO | train_inner | epoch 001:  10104 / 102400000 loss=0.092, moe_gate_loss=6.00373, overflow_expert1=0, overflow_expert2=10.15, entropy_gating=2.001, expert1_balance_top=27.965, expert1_balance_bottom=22.051, unused_expert1_count=0, expert2_balance_top=35.393, expert2_balance_bottom=14.762, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=108638, ups=0.21, wpb=524413, bsz=256, num_updates=10050, lr=0.000580626, gnorm=0.008, clip=0, loss_scale=256, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=43267
2023-06-29 06:38:45 | INFO | train_inner | epoch 001:  10154 / 102400000 loss=0.092, moe_gate_loss=6.00431, overflow_expert1=0, overflow_expert2=9.816, entropy_gating=2.001, expert1_balance_top=27.981, expert1_balance_bottom=22.05, unused_expert1_count=0, expert2_balance_top=35.496, expert2_balance_bottom=14.788, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123570, ups=0.24, wpb=524244, bsz=256, num_updates=10100, lr=0.000580526, gnorm=0.008, clip=0, loss_scale=256, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=43480
2023-06-29 06:41:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 06:42:23 | INFO | train_inner | epoch 001:  10205 / 102400000 loss=0.092, moe_gate_loss=6.00377, overflow_expert1=0, overflow_expert2=10.348, entropy_gating=2.004, expert1_balance_top=27.841, expert1_balance_bottom=22.187, unused_expert1_count=0, expert2_balance_top=36.194, expert2_balance_bottom=14.978, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=120996, ups=0.23, wpb=524264, bsz=256, num_updates=10150, lr=0.000580426, gnorm=0.008, clip=0, loss_scale=256, train_wall=216, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=43697
2023-06-29 06:45:55 | INFO | train_inner | epoch 001:  10255 / 102400000 loss=0.092, moe_gate_loss=6.00369, overflow_expert1=0, overflow_expert2=10.168, entropy_gating=2.002, expert1_balance_top=27.845, expert1_balance_bottom=22.192, unused_expert1_count=0, expert2_balance_top=36.341, expert2_balance_bottom=15.38, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=124265, ups=0.24, wpb=524177, bsz=256, num_updates=10200, lr=0.000580325, gnorm=0.008, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=43909
2023-06-29 06:49:30 | INFO | train_inner | epoch 001:  10305 / 102400000 loss=0.092, moe_gate_loss=6.00319, overflow_expert1=0, overflow_expert2=10.37, entropy_gating=1.998, expert1_balance_top=27.664, expert1_balance_bottom=22.36, unused_expert1_count=0, expert2_balance_top=36.581, expert2_balance_bottom=15.141, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=122418, ups=0.23, wpb=524253, bsz=256, num_updates=10250, lr=0.000580225, gnorm=0.007, clip=0, loss_scale=256, train_wall=214, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=44124
2023-06-29 06:52:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 06:53:05 | INFO | train_inner | epoch 001:  10356 / 102400000 loss=0.092, moe_gate_loss=6.00421, overflow_expert1=0, overflow_expert2=10.802, entropy_gating=2, expert1_balance_top=27.895, expert1_balance_bottom=22.134, unused_expert1_count=0, expert2_balance_top=37.244, expert2_balance_bottom=14.713, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=122333, ups=0.23, wpb=524323, bsz=256, num_updates=10300, lr=0.000580125, gnorm=0.008, clip=0, loss_scale=256, train_wall=214, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=44339
2023-06-29 06:56:39 | INFO | train_inner | epoch 001:  10406 / 102400000 loss=0.092, moe_gate_loss=6.00497, overflow_expert1=0, overflow_expert2=10.528, entropy_gating=2.001, expert1_balance_top=27.913, expert1_balance_bottom=22.12, unused_expert1_count=0, expert2_balance_top=36.931, expert2_balance_bottom=14.861, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123112, ups=0.23, wpb=524180, bsz=256, num_updates=10350, lr=0.000580025, gnorm=0.008, clip=0, loss_scale=256, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=44553
2023-06-29 07:00:13 | INFO | train_inner | epoch 001:  10456 / 102400000 loss=0.092, moe_gate_loss=6.00358, overflow_expert1=0, overflow_expert2=11.475, entropy_gating=2.004, expert1_balance_top=27.715, expert1_balance_bottom=22.316, unused_expert1_count=0, expert2_balance_top=37.988, expert2_balance_bottom=14.316, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=122978, ups=0.23, wpb=524164, bsz=256, num_updates=10400, lr=0.000579925, gnorm=0.008, clip=0, loss_scale=256, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=44767
2023-06-29 07:03:46 | INFO | train_inner | epoch 001:  10506 / 102400000 loss=0.092, moe_gate_loss=6.00402, overflow_expert1=0, overflow_expert2=11.504, entropy_gating=2, expert1_balance_top=27.81, expert1_balance_bottom=22.233, unused_expert1_count=0, expert2_balance_top=38.091, expert2_balance_bottom=13.704, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=123772, ups=0.24, wpb=524362, bsz=256, num_updates=10450, lr=0.000579825, gnorm=0.008, clip=0, loss_scale=512, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=44980
2023-06-29 07:05:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 07:07:23 | INFO | train_inner | epoch 001:  10557 / 102400000 loss=0.092, moe_gate_loss=6.00411, overflow_expert1=0, overflow_expert2=12.081, entropy_gating=1.996, expert1_balance_top=27.904, expert1_balance_bottom=22.13, unused_expert1_count=0, expert2_balance_top=38.465, expert2_balance_bottom=12.829, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=120894, ups=0.23, wpb=524312, bsz=256, num_updates=10500, lr=0.000579725, gnorm=0.008, clip=0, loss_scale=256, train_wall=216, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=45198
2023-06-29 07:10:55 | INFO | train_inner | epoch 001:  10607 / 102400000 loss=0.092, moe_gate_loss=6.00487, overflow_expert1=0, overflow_expert2=11.405, entropy_gating=1.995, expert1_balance_top=27.93, expert1_balance_bottom=22.115, unused_expert1_count=0, expert2_balance_top=37.813, expert2_balance_bottom=13, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=124231, ups=0.24, wpb=524240, bsz=256, num_updates=10550, lr=0.000579625, gnorm=0.008, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=45410
2023-06-29 07:14:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 07:14:33 | INFO | train_inner | epoch 001:  10658 / 102400000 loss=0.092, moe_gate_loss=6.00381, overflow_expert1=0, overflow_expert2=10.882, entropy_gating=1.999, expert1_balance_top=27.865, expert1_balance_bottom=22.179, unused_expert1_count=0, expert2_balance_top=37.155, expert2_balance_bottom=13.026, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=120593, ups=0.23, wpb=524138, bsz=256, num_updates=10600, lr=0.000579524, gnorm=0.008, clip=0, loss_scale=256, train_wall=217, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=45628
2023-06-29 07:18:06 | INFO | train_inner | epoch 001:  10708 / 102400000 loss=0.092, moe_gate_loss=6.00412, overflow_expert1=0, overflow_expert2=10.562, entropy_gating=2.003, expert1_balance_top=27.838, expert1_balance_bottom=22.189, unused_expert1_count=0, expert2_balance_top=36.838, expert2_balance_bottom=13.347, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=124101, ups=0.24, wpb=524222, bsz=256, num_updates=10650, lr=0.000579424, gnorm=0.008, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=45840
2023-06-29 07:21:37 | INFO | train_inner | epoch 001:  10758 / 102400000 loss=0.092, moe_gate_loss=6.00359, overflow_expert1=0.001, overflow_expert2=10.452, entropy_gating=2.002, expert1_balance_top=27.558, expert1_balance_bottom=22.483, unused_expert1_count=0, expert2_balance_top=36.869, expert2_balance_bottom=13.704, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=124478, ups=0.24, wpb=524280, bsz=256, num_updates=10700, lr=0.000579324, gnorm=0.007, clip=0, loss_scale=256, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=46052
2023-06-29 07:25:11 | INFO | train_inner | epoch 001:  10808 / 102400000 loss=0.092, moe_gate_loss=6.00379, overflow_expert1=0, overflow_expert2=9.788, entropy_gating=2.006, expert1_balance_top=27.755, expert1_balance_bottom=22.286, unused_expert1_count=0, expert2_balance_top=35.924, expert2_balance_bottom=14.181, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123137, ups=0.23, wpb=524334, bsz=256, num_updates=10750, lr=0.000579224, gnorm=0.007, clip=0, loss_scale=512, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=46265
2023-06-29 07:28:44 | INFO | train_inner | epoch 001:  10858 / 102400000 loss=0.092, moe_gate_loss=6.00601, overflow_expert1=0, overflow_expert2=8.9, entropy_gating=2.013, expert1_balance_top=28.451, expert1_balance_bottom=21.633, unused_expert1_count=0, expert2_balance_top=34.398, expert2_balance_bottom=15.39, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123760, ups=0.24, wpb=524154, bsz=256, num_updates=10800, lr=0.000579124, gnorm=0.009, clip=0, loss_scale=512, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=46478
2023-06-29 07:32:16 | INFO | train_inner | epoch 001:  10908 / 102400000 loss=0.091, moe_gate_loss=6.00351, overflow_expert1=0, overflow_expert2=9.115, entropy_gating=2.017, expert1_balance_top=27.695, expert1_balance_bottom=22.365, unused_expert1_count=0, expert2_balance_top=34.782, expert2_balance_bottom=15.224, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123703, ups=0.24, wpb=524200, bsz=256, num_updates=10850, lr=0.000579024, gnorm=0.007, clip=0, loss_scale=512, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=46691
2023-06-29 07:33:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-06-29 07:35:54 | INFO | train_inner | epoch 001:  10959 / 102400000 loss=0.091, moe_gate_loss=6.00451, overflow_expert1=0, overflow_expert2=8.804, entropy_gating=2.016, expert1_balance_top=28.139, expert1_balance_bottom=21.953, unused_expert1_count=0, expert2_balance_top=34.02, expert2_balance_bottom=15.844, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=121095, ups=0.23, wpb=524109, bsz=256, num_updates=10900, lr=0.000578924, gnorm=0.008, clip=0, loss_scale=512, train_wall=216, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=46908
2023-06-29 07:39:27 | INFO | train_inner | epoch 001:  11009 / 102400000 loss=0.091, moe_gate_loss=6.00406, overflow_expert1=0, overflow_expert2=9.245, entropy_gating=2.021, expert1_balance_top=27.959, expert1_balance_bottom=22.095, unused_expert1_count=0, expert2_balance_top=34.49, expert2_balance_bottom=15.731, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123610, ups=0.24, wpb=524163, bsz=256, num_updates=10950, lr=0.000578824, gnorm=0.007, clip=0, loss_scale=512, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=47121
2023-06-29 07:40:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 07:43:05 | INFO | train_inner | epoch 001:  11060 / 102400000 loss=0.091, moe_gate_loss=6.00266, overflow_expert1=0, overflow_expert2=9.158, entropy_gating=2.022, expert1_balance_top=27.634, expert1_balance_bottom=22.4, unused_expert1_count=0, expert2_balance_top=34.107, expert2_balance_bottom=15.795, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=120853, ups=0.23, wpb=524335, bsz=256, num_updates=11000, lr=0.000578723, gnorm=0.006, clip=0, loss_scale=256, train_wall=217, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=47339
2023-06-29 07:46:36 | INFO | train_inner | epoch 001:  11110 / 102400000 loss=0.091, moe_gate_loss=6.00295, overflow_expert1=0, overflow_expert2=9.833, entropy_gating=2.021, expert1_balance_top=27.791, expert1_balance_bottom=22.252, unused_expert1_count=0, expert2_balance_top=34.626, expert2_balance_bottom=14.959, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=124435, ups=0.24, wpb=524264, bsz=256, num_updates=11050, lr=0.000578623, gnorm=0.007, clip=0, loss_scale=256, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=47551
2023-06-29 07:50:09 | INFO | train_inner | epoch 001:  11160 / 102400000 loss=0.091, moe_gate_loss=6.00349, overflow_expert1=0, overflow_expert2=9.546, entropy_gating=2.021, expert1_balance_top=27.805, expert1_balance_bottom=22.248, unused_expert1_count=0, expert2_balance_top=34.277, expert2_balance_bottom=16.004, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123524, ups=0.24, wpb=524373, bsz=256, num_updates=11100, lr=0.000578523, gnorm=0.007, clip=0, loss_scale=512, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=47764
2023-06-29 07:53:43 | INFO | train_inner | epoch 001:  11210 / 102400000 loss=0.091, moe_gate_loss=6.00473, overflow_expert1=0, overflow_expert2=10.482, entropy_gating=2.022, expert1_balance_top=28.317, expert1_balance_bottom=21.754, unused_expert1_count=0, expert2_balance_top=35.199, expert2_balance_bottom=16.318, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123129, ups=0.23, wpb=524167, bsz=256, num_updates=11150, lr=0.000578423, gnorm=0.008, clip=0, loss_scale=512, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=47978
2023-06-29 07:55:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 07:57:19 | INFO | train_inner | epoch 001:  11261 / 102400000 loss=0.091, moe_gate_loss=6.00262, overflow_expert1=0, overflow_expert2=8.855, entropy_gating=2.023, expert1_balance_top=27.699, expert1_balance_bottom=22.346, unused_expert1_count=0, expert2_balance_top=34.015, expert2_balance_bottom=16.891, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=121669, ups=0.23, wpb=524171, bsz=256, num_updates=11200, lr=0.000578323, gnorm=0.006, clip=0, loss_scale=256, train_wall=215, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=48194
2023-06-29 08:00:54 | INFO | train_inner | epoch 001:  11311 / 102400000 loss=0.092, moe_gate_loss=6.00345, overflow_expert1=0, overflow_expert2=8.875, entropy_gating=2.023, expert1_balance_top=28.293, expert1_balance_bottom=21.779, unused_expert1_count=0, expert2_balance_top=33.927, expert2_balance_bottom=16.667, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=122872, ups=0.23, wpb=524276, bsz=256, num_updates=11250, lr=0.000578223, gnorm=0.008, clip=0, loss_scale=256, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=48408
2023-06-29 08:04:25 | INFO | train_inner | epoch 001:  11361 / 102400000 loss=0.092, moe_gate_loss=6.00415, overflow_expert1=0, overflow_expert2=9.849, entropy_gating=2.029, expert1_balance_top=28.229, expert1_balance_bottom=21.805, unused_expert1_count=0, expert2_balance_top=34.548, expert2_balance_bottom=16.36, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=124607, ups=0.24, wpb=524233, bsz=256, num_updates=11300, lr=0.000578123, gnorm=0.009, clip=0, loss_scale=256, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=48619
2023-06-29 08:04:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 08:08:01 | INFO | train_inner | epoch 001:  11412 / 102400000 loss=0.092, moe_gate_loss=6.00849, overflow_expert1=0.001, overflow_expert2=12.459, entropy_gating=2.029, expert1_balance_top=29.326, expert1_balance_bottom=20.836, unused_expert1_count=0, expert2_balance_top=36.458, expert2_balance_bottom=15.251, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=121787, ups=0.23, wpb=524152, bsz=256, num_updates=11350, lr=0.000578023, gnorm=0.011, clip=0, loss_scale=256, train_wall=215, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=48836
2023-06-29 08:11:34 | INFO | train_inner | epoch 001:  11462 / 102400000 loss=0.092, moe_gate_loss=6.004, overflow_expert1=0.001, overflow_expert2=13.327, entropy_gating=2.031, expert1_balance_top=28.324, expert1_balance_bottom=21.772, unused_expert1_count=0, expert2_balance_top=37.617, expert2_balance_bottom=14.742, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123589, ups=0.24, wpb=524382, bsz=256, num_updates=11400, lr=0.000577922, gnorm=0.009, clip=0, loss_scale=256, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=49049
2023-06-29 08:14:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 08:14:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-06-29 08:15:15 | INFO | train_inner | epoch 001:  11514 / 102400000 loss=0.092, moe_gate_loss=6.00438, overflow_expert1=0, overflow_expert2=13.478, entropy_gating=2.03, expert1_balance_top=28.463, expert1_balance_bottom=21.611, unused_expert1_count=0, expert2_balance_top=38.258, expert2_balance_bottom=15.571, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=119308, ups=0.23, wpb=524270, bsz=256, num_updates=11450, lr=0.000577822, gnorm=0.009, clip=0, loss_scale=128, train_wall=219, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=49269
2023-06-29 08:18:48 | INFO | train_inner | epoch 001:  11564 / 102400000 loss=0.092, moe_gate_loss=6.00435, overflow_expert1=0, overflow_expert2=14.535, entropy_gating=2.028, expert1_balance_top=28.465, expert1_balance_bottom=21.662, unused_expert1_count=0, expert2_balance_top=39.322, expert2_balance_bottom=14.428, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123234, ups=0.24, wpb=524217, bsz=256, num_updates=11500, lr=0.000577722, gnorm=0.009, clip=0, loss_scale=128, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=49483
2023-06-29 08:22:19 | INFO | train_inner | epoch 001:  11614 / 102400000 loss=0.092, moe_gate_loss=6.00388, overflow_expert1=0, overflow_expert2=14.148, entropy_gating=2.027, expert1_balance_top=28.349, expert1_balance_bottom=21.745, unused_expert1_count=0, expert2_balance_top=39.167, expert2_balance_bottom=14.73, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=125157, ups=0.24, wpb=524207, bsz=256, num_updates=11550, lr=0.000577622, gnorm=0.009, clip=0, loss_scale=128, train_wall=209, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=49693
2023-06-29 08:25:53 | INFO | train_inner | epoch 001:  11664 / 102400000 loss=0.092, moe_gate_loss=6.00428, overflow_expert1=0, overflow_expert2=13.592, entropy_gating=2.027, expert1_balance_top=28.142, expert1_balance_bottom=21.953, unused_expert1_count=0, expert2_balance_top=38.778, expert2_balance_bottom=13.875, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123048, ups=0.23, wpb=524266, bsz=256, num_updates=11600, lr=0.000577522, gnorm=0.009, clip=0, loss_scale=256, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=49907
2023-06-29 08:29:26 | INFO | train_inner | epoch 001:  11714 / 102400000 loss=0.091, moe_gate_loss=6.0036, overflow_expert1=0.001, overflow_expert2=15.595, entropy_gating=2.028, expert1_balance_top=28.062, expert1_balance_bottom=21.987, unused_expert1_count=0, expert2_balance_top=41.157, expert2_balance_bottom=13.494, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123471, ups=0.24, wpb=524181, bsz=256, num_updates=11650, lr=0.000577422, gnorm=0.008, clip=0, loss_scale=256, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=50120
2023-06-29 08:32:58 | INFO | train_inner | epoch 001:  11764 / 102400000 loss=0.092, moe_gate_loss=6.0045, overflow_expert1=0.001, overflow_expert2=17.243, entropy_gating=2.029, expert1_balance_top=28.437, expert1_balance_bottom=21.653, unused_expert1_count=0, expert2_balance_top=42.278, expert2_balance_bottom=12.352, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=124195, ups=0.24, wpb=524308, bsz=256, num_updates=11700, lr=0.000577322, gnorm=0.009, clip=0, loss_scale=512, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=50332
2023-06-29 08:33:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 08:36:37 | INFO | train_inner | epoch 001:  11815 / 102400000 loss=0.092, moe_gate_loss=6.00419, overflow_expert1=0, overflow_expert2=17.553, entropy_gating=2.028, expert1_balance_top=28.452, expert1_balance_bottom=21.663, unused_expert1_count=0, expert2_balance_top=42.633, expert2_balance_bottom=11.879, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=119888, ups=0.23, wpb=524190, bsz=256, num_updates=11750, lr=0.000577222, gnorm=0.009, clip=0, loss_scale=256, train_wall=218, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=50552
2023-06-29 08:36:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-06-29 08:40:14 | INFO | train_inner | epoch 001:  11866 / 102400000 loss=0.092, moe_gate_loss=6.00346, overflow_expert1=0, overflow_expert2=18.903, entropy_gating=2.027, expert1_balance_top=27.971, expert1_balance_bottom=22.077, unused_expert1_count=0, expert2_balance_top=44.706, expert2_balance_bottom=11.539, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=121464, ups=0.23, wpb=524251, bsz=256, num_updates=11800, lr=0.000577121, gnorm=0.008, clip=0, loss_scale=128, train_wall=215, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=50769
2023-06-29 08:43:48 | INFO | train_inner | epoch 001:  11916 / 102400000 loss=0.092, moe_gate_loss=6.00411, overflow_expert1=0, overflow_expert2=18.916, entropy_gating=2.026, expert1_balance_top=28.298, expert1_balance_bottom=21.781, unused_expert1_count=0, expert2_balance_top=44.672, expert2_balance_bottom=11.209, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=122966, ups=0.23, wpb=524205, bsz=256, num_updates=11850, lr=0.000577021, gnorm=0.009, clip=0, loss_scale=128, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=50983
2023-06-29 08:47:20 | INFO | train_inner | epoch 001:  11966 / 102400000 loss=0.092, moe_gate_loss=6.00469, overflow_expert1=0, overflow_expert2=18.446, entropy_gating=2.026, expert1_balance_top=28.208, expert1_balance_bottom=21.834, unused_expert1_count=0, expert2_balance_top=44.172, expert2_balance_bottom=11.137, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=124581, ups=0.24, wpb=524277, bsz=256, num_updates=11900, lr=0.000576921, gnorm=0.008, clip=0, loss_scale=256, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=51194
2023-06-29 08:50:51 | INFO | train_inner | epoch 001:  12016 / 102400000 loss=0.092, moe_gate_loss=6.00313, overflow_expert1=0, overflow_expert2=18.345, entropy_gating=2.03, expert1_balance_top=27.977, expert1_balance_bottom=22.079, unused_expert1_count=0, expert2_balance_top=44.11, expert2_balance_bottom=10.956, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=124274, ups=0.24, wpb=524123, bsz=256, num_updates=11950, lr=0.000576821, gnorm=0.008, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=51406
2023-06-29 08:51:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-06-29 08:54:29 | INFO | train_inner | epoch 001:  12067 / 102400000 loss=0.092, moe_gate_loss=6.0052, overflow_expert1=0, overflow_expert2=18.278, entropy_gating=2.03, expert1_balance_top=28.489, expert1_balance_bottom=21.591, unused_expert1_count=0, expert2_balance_top=43.594, expert2_balance_bottom=10.67, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=120747, ups=0.23, wpb=524230, bsz=256, num_updates=12000, lr=0.000576721, gnorm=0.009, clip=0, loss_scale=128, train_wall=217, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=51624
2023-06-29 08:58:00 | INFO | train_inner | epoch 001:  12117 / 102400000 loss=0.092, moe_gate_loss=6.00317, overflow_expert1=0, overflow_expert2=19.391, entropy_gating=2.03, expert1_balance_top=28.203, expert1_balance_bottom=21.858, unused_expert1_count=0, expert2_balance_top=44.877, expert2_balance_bottom=10.496, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=124816, ups=0.24, wpb=524190, bsz=256, num_updates=12050, lr=0.000576621, gnorm=0.009, clip=0, loss_scale=128, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=51835
2023-06-29 09:01:33 | INFO | train_inner | epoch 001:  12167 / 102400000 loss=0.092, moe_gate_loss=6.00328, overflow_expert1=0.001, overflow_expert2=19.586, entropy_gating=2.03, expert1_balance_top=28.001, expert1_balance_bottom=22.059, unused_expert1_count=0, expert2_balance_top=45.462, expert2_balance_bottom=10.985, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123834, ups=0.24, wpb=524330, bsz=256, num_updates=12100, lr=0.000576521, gnorm=0.009, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=52047
2023-06-29 09:05:06 | INFO | train_inner | epoch 001:  12217 / 102400000 loss=0.092, moe_gate_loss=6.00362, overflow_expert1=0, overflow_expert2=19.547, entropy_gating=2.027, expert1_balance_top=27.906, expert1_balance_bottom=22.149, unused_expert1_count=0, expert2_balance_top=45.661, expert2_balance_bottom=10.426, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123624, ups=0.24, wpb=524335, bsz=256, num_updates=12150, lr=0.000576421, gnorm=0.008, clip=0, loss_scale=256, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=52260
2023-06-29 09:08:39 | INFO | train_inner | epoch 001:  12267 / 102400000 loss=0.092, moe_gate_loss=6.00311, overflow_expert1=0, overflow_expert2=18.756, entropy_gating=2.027, expert1_balance_top=28.094, expert1_balance_bottom=21.973, unused_expert1_count=0, expert2_balance_top=44.974, expert2_balance_bottom=10.589, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123601, ups=0.24, wpb=524198, bsz=256, num_updates=12200, lr=0.00057632, gnorm=0.008, clip=0, loss_scale=256, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=52473
2023-06-29 09:10:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 09:12:17 | INFO | train_inner | epoch 001:  12318 / 102400000 loss=0.092, moe_gate_loss=6.00338, overflow_expert1=0, overflow_expert2=18.523, entropy_gating=2.024, expert1_balance_top=27.813, expert1_balance_bottom=22.243, unused_expert1_count=0, expert2_balance_top=44.772, expert2_balance_bottom=10.987, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=121027, ups=0.23, wpb=524208, bsz=256, num_updates=12250, lr=0.00057622, gnorm=0.008, clip=0, loss_scale=256, train_wall=216, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=52691
2023-06-29 09:15:49 | INFO | train_inner | epoch 001:  12368 / 102400000 loss=0.092, moe_gate_loss=6.00354, overflow_expert1=0, overflow_expert2=18.219, entropy_gating=2.021, expert1_balance_top=27.612, expert1_balance_bottom=22.437, unused_expert1_count=0, expert2_balance_top=44.421, expert2_balance_bottom=11.014, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123834, ups=0.24, wpb=524032, bsz=256, num_updates=12300, lr=0.00057612, gnorm=0.007, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=52903
2023-06-29 09:19:22 | INFO | train_inner | epoch 001:  12418 / 102400000 loss=0.091, moe_gate_loss=6.0031, overflow_expert1=0, overflow_expert2=17.856, entropy_gating=2.021, expert1_balance_top=27.597, expert1_balance_bottom=22.443, unused_expert1_count=0, expert2_balance_top=43.609, expert2_balance_bottom=10.953, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123691, ups=0.24, wpb=524209, bsz=256, num_updates=12350, lr=0.00057602, gnorm=0.007, clip=0, loss_scale=256, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=53116
2023-06-29 09:20:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 09:22:59 | INFO | train_inner | epoch 001:  12469 / 102400000 loss=0.091, moe_gate_loss=6.00324, overflow_expert1=0, overflow_expert2=17.655, entropy_gating=2.022, expert1_balance_top=27.588, expert1_balance_bottom=22.435, unused_expert1_count=0, expert2_balance_top=43.297, expert2_balance_bottom=10.658, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=121340, ups=0.23, wpb=524067, bsz=256, num_updates=12400, lr=0.00057592, gnorm=0.007, clip=0, loss_scale=256, train_wall=216, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=53333
2023-06-29 09:25:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-06-29 09:26:36 | INFO | train_inner | epoch 001:  12520 / 102400000 loss=0.092, moe_gate_loss=6.00343, overflow_expert1=0, overflow_expert2=17.974, entropy_gating=2.022, expert1_balance_top=27.845, expert1_balance_bottom=22.221, unused_expert1_count=0, expert2_balance_top=43.8, expert2_balance_bottom=11.37, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=121267, ups=0.23, wpb=524215, bsz=256, num_updates=12450, lr=0.00057582, gnorm=0.008, clip=0, loss_scale=128, train_wall=216, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=53550
2023-06-29 09:30:10 | INFO | train_inner | epoch 001:  12570 / 102400000 loss=0.092, moe_gate_loss=6.004, overflow_expert1=0, overflow_expert2=17.417, entropy_gating=2.021, expert1_balance_top=27.969, expert1_balance_bottom=22.123, unused_expert1_count=0, expert2_balance_top=43.43, expert2_balance_bottom=11.569, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=122817, ups=0.23, wpb=524348, bsz=256, num_updates=12500, lr=0.00057572, gnorm=0.008, clip=0, loss_scale=128, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=53765
2023-06-29 09:33:42 | INFO | train_inner | epoch 001:  12620 / 102400000 loss=0.091, moe_gate_loss=6.00273, overflow_expert1=0, overflow_expert2=15.486, entropy_gating=2.022, expert1_balance_top=27.665, expert1_balance_bottom=22.387, unused_expert1_count=0, expert2_balance_top=41.258, expert2_balance_bottom=12.892, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=124120, ups=0.24, wpb=524314, bsz=256, num_updates=12550, lr=0.00057562, gnorm=0.007, clip=0, loss_scale=128, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=53977
2023-06-29 09:37:14 | INFO | train_inner | epoch 001:  12670 / 102400000 loss=0.091, moe_gate_loss=6.0022, overflow_expert1=0, overflow_expert2=16.226, entropy_gating=2.021, expert1_balance_top=27.669, expert1_balance_bottom=22.354, unused_expert1_count=0, expert2_balance_top=41.873, expert2_balance_bottom=12.585, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=124354, ups=0.24, wpb=524335, bsz=256, num_updates=12600, lr=0.000575519, gnorm=0.007, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=54189
2023-06-29 09:40:47 | INFO | train_inner | epoch 001:  12720 / 102400000 loss=0.091, moe_gate_loss=6.00338, overflow_expert1=0, overflow_expert2=16.272, entropy_gating=2.021, expert1_balance_top=27.835, expert1_balance_bottom=22.222, unused_expert1_count=0, expert2_balance_top=41.895, expert2_balance_bottom=12.157, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123842, ups=0.24, wpb=524250, bsz=256, num_updates=12650, lr=0.000575419, gnorm=0.007, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=54401
2023-06-29 09:44:20 | INFO | train_inner | epoch 001:  12770 / 102400000 loss=0.091, moe_gate_loss=6.00311, overflow_expert1=0, overflow_expert2=15.91, entropy_gating=2.026, expert1_balance_top=27.737, expert1_balance_bottom=22.298, unused_expert1_count=0, expert2_balance_top=41.732, expert2_balance_bottom=12.654, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123571, ups=0.24, wpb=524236, bsz=256, num_updates=12700, lr=0.000575319, gnorm=0.007, clip=0, loss_scale=512, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=54614
2023-06-29 09:47:54 | INFO | train_inner | epoch 001:  12820 / 102400000 loss=0.091, moe_gate_loss=6.00243, overflow_expert1=0, overflow_expert2=16.666, entropy_gating=2.026, expert1_balance_top=27.61, expert1_balance_bottom=22.425, unused_expert1_count=0, expert2_balance_top=42.482, expert2_balance_bottom=12.259, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=122938, ups=0.23, wpb=524080, bsz=256, num_updates=12750, lr=0.000575219, gnorm=0.006, clip=0, loss_scale=512, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=54828
2023-06-29 09:51:25 | INFO | train_inner | epoch 001:  12870 / 102400000 loss=0.091, moe_gate_loss=6.00273, overflow_expert1=0, overflow_expert2=16.574, entropy_gating=2.026, expert1_balance_top=27.629, expert1_balance_bottom=22.411, unused_expert1_count=0, expert2_balance_top=42.141, expert2_balance_bottom=11.953, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=124372, ups=0.24, wpb=524305, bsz=256, num_updates=12800, lr=0.000575119, gnorm=0.006, clip=0, loss_scale=512, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=55040
2023-06-29 09:53:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-06-29 09:55:03 | INFO | train_inner | epoch 001:  12921 / 102400000 loss=0.091, moe_gate_loss=6.00286, overflow_expert1=0, overflow_expert2=15.931, entropy_gating=2.027, expert1_balance_top=27.685, expert1_balance_bottom=22.362, unused_expert1_count=0, expert2_balance_top=41.767, expert2_balance_bottom=12.598, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=120560, ups=0.23, wpb=524223, bsz=256, num_updates=12850, lr=0.000575019, gnorm=0.007, clip=0, loss_scale=512, train_wall=217, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=55258
2023-06-29 09:58:37 | INFO | train_inner | epoch 001:  12971 / 102400000 loss=0.091, moe_gate_loss=6.00306, overflow_expert1=0, overflow_expert2=14.341, entropy_gating=2.031, expert1_balance_top=27.777, expert1_balance_bottom=22.274, unused_expert1_count=0, expert2_balance_top=39.376, expert2_balance_bottom=13.143, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123098, ups=0.23, wpb=524194, bsz=256, num_updates=12900, lr=0.000574919, gnorm=0.006, clip=0, loss_scale=512, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=55472
2023-06-29 10:02:07 | INFO | train_inner | epoch 001:  13021 / 102400000 loss=0.091, moe_gate_loss=6.00218, overflow_expert1=0, overflow_expert2=13.699, entropy_gating=2.027, expert1_balance_top=27.441, expert1_balance_bottom=22.597, unused_expert1_count=0, expert2_balance_top=38.464, expert2_balance_bottom=13.18, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=124813, ups=0.24, wpb=524227, bsz=256, num_updates=12950, lr=0.000574819, gnorm=0.005, clip=0, loss_scale=512, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=55682
2023-06-29 10:02:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-06-29 10:05:45 | INFO | train_inner | epoch 001:  13072 / 102400000 loss=0.091, moe_gate_loss=6.00202, overflow_expert1=0, overflow_expert2=13.76, entropy_gating=2.026, expert1_balance_top=27.338, expert1_balance_bottom=22.704, unused_expert1_count=0, expert2_balance_top=38.737, expert2_balance_bottom=12.828, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=120734, ups=0.23, wpb=524169, bsz=256, num_updates=13000, lr=0.000574718, gnorm=0.005, clip=0, loss_scale=512, train_wall=217, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=55900
2023-06-29 10:06:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 10:09:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-06-29 10:09:26 | INFO | train_inner | epoch 001:  13124 / 102400000 loss=0.091, moe_gate_loss=6.00182, overflow_expert1=0, overflow_expert2=13.309, entropy_gating=2.025, expert1_balance_top=27.148, expert1_balance_bottom=22.887, unused_expert1_count=0, expert2_balance_top=38.004, expert2_balance_bottom=13.376, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.004, ppl=1, wps=118992, ups=0.23, wpb=524208, bsz=256, num_updates=13050, lr=0.000574618, gnorm=0.005, clip=0, loss_scale=128, train_wall=220, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=56121
2023-06-29 10:12:59 | INFO | train_inner | epoch 001:  13174 / 102400000 loss=0.091, moe_gate_loss=6.00249, overflow_expert1=0, overflow_expert2=14.524, entropy_gating=2.026, expert1_balance_top=27.63, expert1_balance_bottom=22.421, unused_expert1_count=0, expert2_balance_top=39.782, expert2_balance_bottom=12.853, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123751, ups=0.24, wpb=524353, bsz=256, num_updates=13100, lr=0.000574518, gnorm=0.006, clip=0, loss_scale=128, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=56333
2023-06-29 10:14:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-06-29 10:16:35 | INFO | train_inner | epoch 001:  13225 / 102400000 loss=0.091, moe_gate_loss=6.00261, overflow_expert1=0, overflow_expert2=14.94, entropy_gating=2.026, expert1_balance_top=27.603, expert1_balance_bottom=22.439, unused_expert1_count=0, expert2_balance_top=40.548, expert2_balance_bottom=12.435, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.004, ppl=1, wps=121182, ups=0.23, wpb=524160, bsz=256, num_updates=13150, lr=0.000574418, gnorm=0.006, clip=0, loss_scale=64, train_wall=216, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=56550
2023-06-29 10:20:07 | INFO | train_inner | epoch 001:  13275 / 102400000 loss=0.091, moe_gate_loss=6.00203, overflow_expert1=0, overflow_expert2=15.092, entropy_gating=2.027, expert1_balance_top=27.419, expert1_balance_bottom=22.606, unused_expert1_count=0, expert2_balance_top=40.988, expert2_balance_bottom=12.616, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=124419, ups=0.24, wpb=524068, bsz=256, num_updates=13200, lr=0.000574318, gnorm=0.006, clip=0, loss_scale=64, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=56761
2023-06-29 10:22:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-06-29 10:23:45 | INFO | train_inner | epoch 001:  13326 / 102400000 loss=0.091, moe_gate_loss=6.00221, overflow_expert1=0, overflow_expert2=14.959, entropy_gating=2.023, expert1_balance_top=27.431, expert1_balance_bottom=22.592, unused_expert1_count=0, expert2_balance_top=40.619, expert2_balance_bottom=12.492, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.004, ppl=1, wps=120560, ups=0.23, wpb=524274, bsz=256, num_updates=13250, lr=0.000574218, gnorm=0.005, clip=0, loss_scale=32, train_wall=217, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=56979
2023-06-29 10:25:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-06-29 10:27:21 | INFO | train_inner | epoch 001:  13377 / 102400000 loss=0.093, moe_gate_loss=6.01186, overflow_expert1=0.008, overflow_expert2=15.225, entropy_gating=2.023, expert1_balance_top=29.379, expert1_balance_bottom=20.791, unused_expert1_count=0, expert2_balance_top=39.87, expert2_balance_bottom=12.223, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=121796, ups=0.23, wpb=524098, bsz=256, num_updates=13300, lr=0.000574118, gnorm=0.015, clip=0, loss_scale=16, train_wall=215, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=57195
2023-06-29 10:27:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-06-29 10:30:58 | INFO | train_inner | epoch 001:  13428 / 102400000 loss=0.092, moe_gate_loss=6.00794, overflow_expert1=0.001, overflow_expert2=14.325, entropy_gating=2.019, expert1_balance_top=28.31, expert1_balance_bottom=21.744, unused_expert1_count=0, expert2_balance_top=39.559, expert2_balance_bottom=12.276, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=120997, ups=0.23, wpb=524352, bsz=256, num_updates=13350, lr=0.000574018, gnorm=0.011, clip=0, loss_scale=8, train_wall=216, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=57413
2023-06-29 10:34:32 | INFO | train_inner | epoch 001:  13478 / 102400000 loss=0.097, moe_gate_loss=6.04865, overflow_expert1=0.191, overflow_expert2=14.966, entropy_gating=2.006, expert1_balance_top=31.204, expert1_balance_bottom=19.052, unused_expert1_count=0.003, expert2_balance_top=37.651, expert2_balance_bottom=13.516, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.009, ppl=1.01, wps=123007, ups=0.23, wpb=524182, bsz=256, num_updates=13400, lr=0.000573917, gnorm=0.058, clip=0, loss_scale=8, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=57627
2023-06-29 10:38:03 | INFO | train_inner | epoch 001:  13528 / 102400000 loss=0.093, moe_gate_loss=6.00848, overflow_expert1=0.001, overflow_expert2=9.995, entropy_gating=1.988, expert1_balance_top=28.544, expert1_balance_bottom=21.542, unused_expert1_count=0, expert2_balance_top=35.473, expert2_balance_bottom=14.488, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=124804, ups=0.24, wpb=524276, bsz=256, num_updates=13450, lr=0.000573817, gnorm=0.013, clip=0, loss_scale=16, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=57838
2023-06-29 10:41:38 | INFO | train_inner | epoch 001:  13578 / 102400000 loss=0.092, moe_gate_loss=6.00213, overflow_expert1=0.004, overflow_expert2=10.55, entropy_gating=1.985, expert1_balance_top=27.265, expert1_balance_bottom=22.773, unused_expert1_count=0, expert2_balance_top=36.077, expert2_balance_bottom=13.077, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=122349, ups=0.23, wpb=524208, bsz=256, num_updates=13500, lr=0.000573717, gnorm=0.007, clip=0, loss_scale=16, train_wall=214, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=58053
2023-06-29 10:42:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-06-29 10:45:16 | INFO | train_inner | epoch 001:  13629 / 102400000 loss=0.093, moe_gate_loss=6.03214, overflow_expert1=0.178, overflow_expert2=13.457, entropy_gating=1.986, expert1_balance_top=28.881, expert1_balance_bottom=21.312, unused_expert1_count=0, expert2_balance_top=37.621, expert2_balance_bottom=11.822, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=121137, ups=0.23, wpb=524333, bsz=256, num_updates=13550, lr=0.000573617, gnorm=0.023, clip=0, loss_scale=8, train_wall=216, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=58270
2023-06-29 10:48:50 | INFO | train_inner | epoch 001:  13679 / 102400000 loss=0.094, moe_gate_loss=6.04483, overflow_expert1=0.033, overflow_expert2=12.061, entropy_gating=1.954, expert1_balance_top=30.556, expert1_balance_bottom=19.436, unused_expert1_count=0, expert2_balance_top=36.303, expert2_balance_bottom=14.167, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.007, ppl=1, wps=122932, ups=0.23, wpb=524291, bsz=256, num_updates=13600, lr=0.000573517, gnorm=0.023, clip=0, loss_scale=8, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=58484
2023-06-29 10:52:22 | INFO | train_inner | epoch 001:  13729 / 102400000 loss=0.092, moe_gate_loss=6.00434, overflow_expert1=0.003, overflow_expert2=9.878, entropy_gating=1.951, expert1_balance_top=27.805, expert1_balance_bottom=22.261, unused_expert1_count=0, expert2_balance_top=35.834, expert2_balance_bottom=14.193, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123947, ups=0.24, wpb=524015, bsz=256, num_updates=13650, lr=0.000573417, gnorm=0.006, clip=0, loss_scale=16, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=58697
2023-06-29 10:55:55 | INFO | train_inner | epoch 001:  13779 / 102400000 loss=0.091, moe_gate_loss=6.00075, overflow_expert1=0, overflow_expert2=9.998, entropy_gating=1.953, expert1_balance_top=26.893, expert1_balance_bottom=23.128, unused_expert1_count=0, expert2_balance_top=35.871, expert2_balance_bottom=13.717, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123458, ups=0.24, wpb=524156, bsz=256, num_updates=13700, lr=0.000573317, gnorm=0.005, clip=0, loss_scale=16, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=58910
2023-06-29 10:57:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-06-29 10:57:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-06-29 10:59:37 | INFO | train_inner | epoch 001:  13831 / 102400000 loss=0.092, moe_gate_loss=6.00839, overflow_expert1=0.02, overflow_expert2=10.458, entropy_gating=1.955, expert1_balance_top=27.645, expert1_balance_bottom=22.388, unused_expert1_count=0, expert2_balance_top=35.981, expert2_balance_bottom=13.655, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=118306, ups=0.23, wpb=524230, bsz=256, num_updates=13750, lr=0.000573217, gnorm=0.014, clip=0, loss_scale=4, train_wall=221, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=59132
2023-06-29 11:03:08 | INFO | train_inner | epoch 001:  13881 / 102400000 loss=0.098, moe_gate_loss=6.07004, overflow_expert1=0.139, overflow_expert2=13.44, entropy_gating=1.953, expert1_balance_top=33.195, expert1_balance_bottom=17.01, unused_expert1_count=0, expert2_balance_top=35.131, expert2_balance_bottom=14.96, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.01, ppl=1.01, wps=124749, ups=0.24, wpb=524069, bsz=256, num_updates=13800, lr=0.000573116, gnorm=0.045, clip=0, loss_scale=4, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=59343
2023-06-29 11:06:40 | INFO | train_inner | epoch 001:  13931 / 102400000 loss=0.092, moe_gate_loss=6.00912, overflow_expert1=0.007, overflow_expert2=9.87, entropy_gating=1.94, expert1_balance_top=28.993, expert1_balance_bottom=21.071, unused_expert1_count=0, expert2_balance_top=35.709, expert2_balance_bottom=15.594, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=123925, ups=0.24, wpb=524151, bsz=256, num_updates=13850, lr=0.000573016, gnorm=0.013, clip=0, loss_scale=4, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=59555
2023-06-29 11:08:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-06-29 11:10:17 | INFO | train_inner | epoch 001:  13982 / 102400000 loss=0.093, moe_gate_loss=6.01354, overflow_expert1=0.04, overflow_expert2=11.341, entropy_gating=1.942, expert1_balance_top=28.912, expert1_balance_bottom=21.138, unused_expert1_count=0, expert2_balance_top=36.609, expert2_balance_bottom=14.606, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=121227, ups=0.23, wpb=524244, bsz=256, num_updates=13900, lr=0.000572916, gnorm=0.017, clip=0, loss_scale=4, train_wall=216, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=59772
2023-06-29 11:13:49 | INFO | train_inner | epoch 001:  14032 / 102400000 loss=0.092, moe_gate_loss=6.01036, overflow_expert1=0.004, overflow_expert2=11.195, entropy_gating=1.936, expert1_balance_top=28.87, expert1_balance_bottom=21.205, unused_expert1_count=0, expert2_balance_top=36.423, expert2_balance_bottom=14.034, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=124045, ups=0.24, wpb=524264, bsz=256, num_updates=13950, lr=0.000572816, gnorm=0.011, clip=0, loss_scale=4, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=59984
2023-06-29 11:17:22 | INFO | train_inner | epoch 001:  14082 / 102400000 loss=0.092, moe_gate_loss=6.00868, overflow_expert1=0.006, overflow_expert2=11.881, entropy_gating=1.938, expert1_balance_top=28.401, expert1_balance_bottom=21.695, unused_expert1_count=0, expert2_balance_top=36.967, expert2_balance_bottom=14.026, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=123471, ups=0.24, wpb=524228, bsz=256, num_updates=14000, lr=0.000572716, gnorm=0.013, clip=0, loss_scale=4, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=60197
2023-06-29 11:20:53 | INFO | train_inner | epoch 001:  14132 / 102400000 loss=0.092, moe_gate_loss=6.01214, overflow_expert1=0.004, overflow_expert2=12.807, entropy_gating=1.935, expert1_balance_top=28.582, expert1_balance_bottom=21.472, unused_expert1_count=0, expert2_balance_top=37.531, expert2_balance_bottom=13.938, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=124621, ups=0.24, wpb=524345, bsz=256, num_updates=14050, lr=0.000572616, gnorm=0.013, clip=0, loss_scale=8, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=60408
2023-06-29 11:24:27 | INFO | train_inner | epoch 001:  14182 / 102400000 loss=0.093, moe_gate_loss=6.02738, overflow_expert1=0.056, overflow_expert2=13.522, entropy_gating=1.935, expert1_balance_top=30.426, expert1_balance_bottom=19.729, unused_expert1_count=0, expert2_balance_top=37.312, expert2_balance_bottom=13.857, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.006, ppl=1, wps=123013, ups=0.23, wpb=524259, bsz=256, num_updates=14100, lr=0.000572516, gnorm=0.017, clip=0, loss_scale=8, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=60622
2023-06-29 11:27:59 | INFO | train_inner | epoch 001:  14232 / 102400000 loss=0.092, moe_gate_loss=6.00566, overflow_expert1=0.008, overflow_expert2=11.117, entropy_gating=1.937, expert1_balance_top=27.672, expert1_balance_bottom=22.388, unused_expert1_count=0, expert2_balance_top=36.928, expert2_balance_bottom=14.777, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=124175, ups=0.24, wpb=524184, bsz=256, num_updates=14150, lr=0.000572416, gnorm=0.007, clip=0, loss_scale=16, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=60834
2023-06-29 11:31:31 | INFO | train_inner | epoch 001:  14282 / 102400000 loss=0.091, moe_gate_loss=6.0026, overflow_expert1=0.004, overflow_expert2=11.838, entropy_gating=1.945, expert1_balance_top=27.052, expert1_balance_bottom=22.965, unused_expert1_count=0, expert2_balance_top=37.509, expert2_balance_bottom=13.495, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=124070, ups=0.24, wpb=524460, bsz=256, num_updates=14200, lr=0.000572315, gnorm=0.005, clip=0, loss_scale=16, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=61046
2023-06-29 11:35:06 | INFO | train_inner | epoch 001:  14332 / 102400000 loss=0.091, moe_gate_loss=6.00201, overflow_expert1=0.003, overflow_expert2=12.653, entropy_gating=1.948, expert1_balance_top=27.154, expert1_balance_bottom=22.871, unused_expert1_count=0, expert2_balance_top=37.717, expert2_balance_bottom=12.651, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=122283, ups=0.23, wpb=524069, bsz=256, num_updates=14250, lr=0.000572215, gnorm=0.005, clip=0, loss_scale=16, train_wall=214, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=61261
2023-06-29 11:38:40 | INFO | train_inner | epoch 001:  14382 / 102400000 loss=0.091, moe_gate_loss=6.00347, overflow_expert1=0.001, overflow_expert2=13.548, entropy_gating=1.954, expert1_balance_top=27.575, expert1_balance_bottom=22.463, unused_expert1_count=0, expert2_balance_top=38.56, expert2_balance_bottom=12.902, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123336, ups=0.24, wpb=524286, bsz=256, num_updates=14300, lr=0.000572115, gnorm=0.006, clip=0, loss_scale=32, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=61474
Expecting value: line 1 column 1 (char 0)

2023-06-29 11:42:11 | INFO | train_inner | epoch 001:  14432 / 102400000 loss=0.091, moe_gate_loss=6.00355, overflow_expert1=0.001, overflow_expert2=13.768, entropy_gating=1.959, expert1_balance_top=27.472, expert1_balance_bottom=22.565, unused_expert1_count=0, expert2_balance_top=38.521, expert2_balance_bottom=12.461, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=124357, ups=0.24, wpb=524270, bsz=256, num_updates=14350, lr=0.000572015, gnorm=0.006, clip=0, loss_scale=32, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=61686
2023-06-29 11:45:44 | INFO | train_inner | epoch 001:  14482 / 102400000 loss=0.091, moe_gate_loss=6.00141, overflow_expert1=0, overflow_expert2=13.352, entropy_gating=1.964, expert1_balance_top=27.051, expert1_balance_bottom=22.966, unused_expert1_count=0, expert2_balance_top=38.191, expert2_balance_bottom=12.042, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123792, ups=0.24, wpb=524093, bsz=256, num_updates=14400, lr=0.000571915, gnorm=0.005, clip=0, loss_scale=64, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=61898
2023-06-29 11:49:16 | INFO | train_inner | epoch 001:  14532 / 102400000 loss=0.091, moe_gate_loss=6.00226, overflow_expert1=0, overflow_expert2=14.082, entropy_gating=1.97, expert1_balance_top=27.008, expert1_balance_bottom=23, unused_expert1_count=0, expert2_balance_top=39.207, expert2_balance_bottom=12.464, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=124076, ups=0.24, wpb=524262, bsz=256, num_updates=14450, lr=0.000571815, gnorm=0.005, clip=0, loss_scale=64, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=62110
2023-06-29 11:52:49 | INFO | train_inner | epoch 001:  14582 / 102400000 loss=0.091, moe_gate_loss=6.00253, overflow_expert1=0, overflow_expert2=15.725, entropy_gating=1.975, expert1_balance_top=27.29, expert1_balance_bottom=22.725, unused_expert1_count=0, expert2_balance_top=40.968, expert2_balance_bottom=12.094, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123561, ups=0.24, wpb=524296, bsz=256, num_updates=14500, lr=0.000571715, gnorm=0.005, clip=0, loss_scale=64, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=62323
2023-06-29 11:56:20 | INFO | train_inner | epoch 001:  14632 / 102400000 loss=0.091, moe_gate_loss=6.00194, overflow_expert1=0, overflow_expert2=16.987, entropy_gating=1.975, expert1_balance_top=27.053, expert1_balance_bottom=22.945, unused_expert1_count=0, expert2_balance_top=41.923, expert2_balance_bottom=11.555, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=124186, ups=0.24, wpb=524319, bsz=256, num_updates=14550, lr=0.000571615, gnorm=0.005, clip=0, loss_scale=128, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=62535
2023-06-29 11:59:54 | INFO | train_inner | epoch 001:  14682 / 102400000 loss=0.091, moe_gate_loss=6.00217, overflow_expert1=0, overflow_expert2=16.814, entropy_gating=1.975, expert1_balance_top=27.122, expert1_balance_bottom=22.898, unused_expert1_count=0, expert2_balance_top=42.047, expert2_balance_bottom=11.815, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=122809, ups=0.23, wpb=524294, bsz=256, num_updates=14600, lr=0.000571514, gnorm=0.005, clip=0, loss_scale=128, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=62749
2023-06-29 12:03:26 | INFO | train_inner | epoch 001:  14732 / 102400000 loss=0.091, moe_gate_loss=6.00198, overflow_expert1=0, overflow_expert2=15.695, entropy_gating=1.978, expert1_balance_top=27.3, expert1_balance_bottom=22.743, unused_expert1_count=0, expert2_balance_top=40.804, expert2_balance_bottom=11.751, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=124530, ups=0.24, wpb=524259, bsz=256, num_updates=14650, lr=0.000571414, gnorm=0.005, clip=0, loss_scale=256, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=62960
2023-06-29 12:06:57 | INFO | train_inner | epoch 001:  14782 / 102400000 loss=0.091, moe_gate_loss=6.00266, overflow_expert1=0, overflow_expert2=14.892, entropy_gating=1.978, expert1_balance_top=27.314, expert1_balance_bottom=22.713, unused_expert1_count=0, expert2_balance_top=40.223, expert2_balance_bottom=12.223, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=124080, ups=0.24, wpb=524050, bsz=256, num_updates=14700, lr=0.000571314, gnorm=0.005, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=63172
2023-06-29 12:10:32 | INFO | train_inner | epoch 001:  14832 / 102400000 loss=0.091, moe_gate_loss=6.00151, overflow_expert1=0, overflow_expert2=14.558, entropy_gating=1.984, expert1_balance_top=27.178, expert1_balance_bottom=22.839, unused_expert1_count=0, expert2_balance_top=39.622, expert2_balance_bottom=12.921, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=122505, ups=0.23, wpb=524219, bsz=256, num_updates=14750, lr=0.000571214, gnorm=0.005, clip=0, loss_scale=256, train_wall=214, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=63387
2023-06-29 12:13:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 12:14:07 | INFO | train_inner | epoch 001:  14883 / 102400000 loss=0.091, moe_gate_loss=6.00271, overflow_expert1=0, overflow_expert2=14.957, entropy_gating=1.987, expert1_balance_top=27.3, expert1_balance_bottom=22.7, unused_expert1_count=0, expert2_balance_top=40.077, expert2_balance_bottom=13.089, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=121943, ups=0.23, wpb=524231, bsz=256, num_updates=14800, lr=0.000571114, gnorm=0.006, clip=0, loss_scale=256, train_wall=215, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=63602
2023-06-29 12:17:39 | INFO | train_inner | epoch 001:  14933 / 102400000 loss=0.091, moe_gate_loss=6.00167, overflow_expert1=0, overflow_expert2=15.54, entropy_gating=1.986, expert1_balance_top=27.172, expert1_balance_bottom=22.847, unused_expert1_count=0, expert2_balance_top=40.455, expert2_balance_bottom=11.465, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123988, ups=0.24, wpb=524259, bsz=256, num_updates=14850, lr=0.000571014, gnorm=0.005, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=63814
2023-06-29 12:21:12 | INFO | train_inner | epoch 001:  14983 / 102400000 loss=0.091, moe_gate_loss=6.00228, overflow_expert1=0.001, overflow_expert2=14.537, entropy_gating=1.99, expert1_balance_top=27.302, expert1_balance_bottom=22.725, unused_expert1_count=0, expert2_balance_top=39.317, expert2_balance_bottom=12.088, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123601, ups=0.24, wpb=524197, bsz=256, num_updates=14900, lr=0.000570914, gnorm=0.005, clip=0, loss_scale=256, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=64027
2023-06-29 12:24:45 | INFO | train_inner | epoch 001:  15033 / 102400000 loss=0.091, moe_gate_loss=6.00218, overflow_expert1=0, overflow_expert2=12.761, entropy_gating=1.991, expert1_balance_top=27.34, expert1_balance_bottom=22.678, unused_expert1_count=0, expert2_balance_top=37.27, expert2_balance_bottom=12.588, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123547, ups=0.24, wpb=524196, bsz=256, num_updates=14950, lr=0.000570814, gnorm=0.005, clip=0, loss_scale=512, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=64240
2023-06-29 12:28:18 | INFO | train_inner | epoch 001:  15083 / 102400000 loss=0.091, moe_gate_loss=6.00237, overflow_expert1=0, overflow_expert2=13.503, entropy_gating=1.993, expert1_balance_top=27.346, expert1_balance_bottom=22.69, unused_expert1_count=0, expert2_balance_top=38.114, expert2_balance_bottom=12.26, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123465, ups=0.24, wpb=524335, bsz=256, num_updates=15000, lr=0.000570713, gnorm=0.005, clip=0, loss_scale=512, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=64453
2023-06-29 12:28:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 15000 updates
2023-06-29 12:28:18 | INFO | fairseq.trainer | Saving checkpoint to /mnt1/msranlpintern/wuxun/MoE/MoE_results/mhmoe_v1/small-baseline-redstone_v2-flash_attn-8experts-2heads/checkpoint_1_15000-rank-0.pt
2023-06-29 12:28:23 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt1/msranlpintern/wuxun/MoE/MoE_results/mhmoe_v1/small-baseline-redstone_v2-flash_attn-8experts-2heads/checkpoint_1_15000-rank-0.pt
2023-06-29 12:28:23 | INFO | fairseq.trainer | Saving checkpoint to /mnt1/msranlpintern/wuxun/MoE/MoE_results/mhmoe_v1/small-baseline-redstone_v2-flash_attn-8experts-2heads/checkpoint_1_15000-shared.pt
2023-06-29 12:28:35 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt1/msranlpintern/wuxun/MoE/MoE_results/mhmoe_v1/small-baseline-redstone_v2-flash_attn-8experts-2heads/checkpoint_1_15000-shared.pt
2023-06-29 12:28:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /mnt1/msranlpintern/wuxun/MoE/MoE_results/mhmoe_v1/small-baseline-redstone_v2-flash_attn-8experts-2heads/checkpoint_1_15000-rank-0.pt (epoch 1 @ 15000 updates, score None) (writing took 32.855197649041656 seconds)
2023-06-29 12:32:22 | INFO | train_inner | epoch 001:  15133 / 102400000 loss=0.091, moe_gate_loss=6.00304, overflow_expert1=0, overflow_expert2=15.126, entropy_gating=1.993, expert1_balance_top=27.657, expert1_balance_bottom=22.38, unused_expert1_count=0, expert2_balance_top=39.809, expert2_balance_bottom=11.587, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=107550, ups=0.21, wpb=524332, bsz=256, num_updates=15050, lr=0.000570613, gnorm=0.006, clip=0, loss_scale=512, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=64697
2023-06-29 12:32:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-06-29 12:36:00 | INFO | train_inner | epoch 001:  15184 / 102400000 loss=0.091, moe_gate_loss=6.0032, overflow_expert1=0, overflow_expert2=16.423, entropy_gating=1.996, expert1_balance_top=27.669, expert1_balance_bottom=22.357, unused_expert1_count=0, expert2_balance_top=41.015, expert2_balance_bottom=11.341, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=120612, ups=0.23, wpb=524238, bsz=256, num_updates=15100, lr=0.000570513, gnorm=0.006, clip=0, loss_scale=512, train_wall=217, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=64915
2023-06-29 12:39:34 | INFO | train_inner | epoch 001:  15234 / 102400000 loss=0.091, moe_gate_loss=6.00267, overflow_expert1=0, overflow_expert2=17.475, entropy_gating=1.998, expert1_balance_top=27.372, expert1_balance_bottom=22.651, unused_expert1_count=0, expert2_balance_top=42.103, expert2_balance_bottom=10.842, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=122708, ups=0.23, wpb=524232, bsz=256, num_updates=15150, lr=0.000570413, gnorm=0.006, clip=0, loss_scale=512, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=65129
2023-06-29 12:42:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-06-29 12:43:10 | INFO | train_inner | epoch 001:  15285 / 102400000 loss=0.091, moe_gate_loss=6.0032, overflow_expert1=0, overflow_expert2=16.418, entropy_gating=1.998, expert1_balance_top=27.763, expert1_balance_bottom=22.271, unused_expert1_count=0, expert2_balance_top=41.06, expert2_balance_bottom=11.043, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=121582, ups=0.23, wpb=524153, bsz=256, num_updates=15200, lr=0.000570313, gnorm=0.006, clip=0, loss_scale=512, train_wall=215, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=65345
2023-06-29 12:44:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 12:46:50 | INFO | train_inner | epoch 001:  15336 / 102400000 loss=0.091, moe_gate_loss=6.00613, overflow_expert1=0, overflow_expert2=16.338, entropy_gating=2.005, expert1_balance_top=28.117, expert1_balance_bottom=21.911, unused_expert1_count=0, expert2_balance_top=40.985, expert2_balance_bottom=11.1, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=119943, ups=0.23, wpb=524374, bsz=256, num_updates=15250, lr=0.000570213, gnorm=0.006, clip=0, loss_scale=256, train_wall=218, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=65564
2023-06-29 12:50:22 | INFO | train_inner | epoch 001:  15386 / 102400000 loss=0.091, moe_gate_loss=6.00268, overflow_expert1=0, overflow_expert2=17.888, entropy_gating=2.005, expert1_balance_top=27.282, expert1_balance_bottom=22.756, unused_expert1_count=0, expert2_balance_top=43.146, expert2_balance_bottom=11.056, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123819, ups=0.24, wpb=524372, bsz=256, num_updates=15300, lr=0.000570113, gnorm=0.005, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=65777
2023-06-29 12:53:57 | INFO | train_inner | epoch 001:  15436 / 102400000 loss=0.091, moe_gate_loss=6.00627, overflow_expert1=0, overflow_expert2=18.998, entropy_gating=2.007, expert1_balance_top=28.397, expert1_balance_bottom=21.606, unused_expert1_count=0, expert2_balance_top=44.221, expert2_balance_bottom=11.019, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=122359, ups=0.23, wpb=524071, bsz=256, num_updates=15350, lr=0.000570013, gnorm=0.006, clip=0, loss_scale=512, train_wall=214, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=65992
2023-06-29 12:55:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 12:57:34 | INFO | train_inner | epoch 001:  15487 / 102400000 loss=0.091, moe_gate_loss=6.0019, overflow_expert1=0, overflow_expert2=17.421, entropy_gating=2.012, expert1_balance_top=26.977, expert1_balance_bottom=23.039, unused_expert1_count=0, expert2_balance_top=43.153, expert2_balance_bottom=11.973, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.004, ppl=1, wps=121122, ups=0.23, wpb=524221, bsz=256, num_updates=15400, lr=0.000569912, gnorm=0.004, clip=0, loss_scale=256, train_wall=216, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=66209
2023-06-29 13:01:07 | INFO | train_inner | epoch 001:  15537 / 102400000 loss=0.091, moe_gate_loss=6.00159, overflow_expert1=0, overflow_expert2=15.98, entropy_gating=2.01, expert1_balance_top=27.21, expert1_balance_bottom=22.819, unused_expert1_count=0, expert2_balance_top=41.509, expert2_balance_bottom=11.952, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.004, ppl=1, wps=123487, ups=0.24, wpb=524274, bsz=256, num_updates=15450, lr=0.000569812, gnorm=0.005, clip=0, loss_scale=256, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=66422
2023-06-29 13:04:41 | INFO | train_inner | epoch 001:  15587 / 102400000 loss=0.091, moe_gate_loss=6.00304, overflow_expert1=0, overflow_expert2=16.773, entropy_gating=2.008, expert1_balance_top=27.372, expert1_balance_bottom=22.646, unused_expert1_count=0, expert2_balance_top=42.115, expert2_balance_bottom=10.598, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.004, ppl=1, wps=122739, ups=0.23, wpb=524018, bsz=256, num_updates=15500, lr=0.000569712, gnorm=0.005, clip=0, loss_scale=512, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=66636
2023-06-29 13:08:14 | INFO | train_inner | epoch 001:  15637 / 102400000 loss=0.091, moe_gate_loss=6.00406, overflow_expert1=0, overflow_expert2=17.812, entropy_gating=2.014, expert1_balance_top=27.57, expert1_balance_bottom=22.442, unused_expert1_count=0, expert2_balance_top=42.809, expert2_balance_bottom=10.519, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.004, ppl=1, wps=123603, ups=0.24, wpb=524214, bsz=256, num_updates=15550, lr=0.000569612, gnorm=0.005, clip=0, loss_scale=512, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=66848
2023-06-29 13:11:46 | INFO | train_inner | epoch 001:  15687 / 102400000 loss=0.091, moe_gate_loss=6.0024, overflow_expert1=0, overflow_expert2=18.532, entropy_gating=2.02, expert1_balance_top=27.254, expert1_balance_bottom=22.746, unused_expert1_count=0, expert2_balance_top=43.462, expert2_balance_bottom=10.255, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.004, ppl=1, wps=123588, ups=0.24, wpb=524242, bsz=256, num_updates=15600, lr=0.000569512, gnorm=0.004, clip=0, loss_scale=512, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=67061
2023-06-29 13:13:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-06-29 13:15:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 13:15:27 | INFO | train_inner | epoch 001:  15739 / 102400000 loss=0.091, moe_gate_loss=6.00553, overflow_expert1=0, overflow_expert2=18.377, entropy_gating=2.021, expert1_balance_top=28.241, expert1_balance_bottom=21.759, unused_expert1_count=0, expert2_balance_top=43.806, expert2_balance_bottom=10.95, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=118977, ups=0.23, wpb=524268, bsz=256, num_updates=15650, lr=0.000569412, gnorm=0.006, clip=0, loss_scale=256, train_wall=220, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=67282
2023-06-29 13:19:00 | INFO | train_inner | epoch 001:  15789 / 102400000 loss=0.091, moe_gate_loss=6.00267, overflow_expert1=0, overflow_expert2=18.804, entropy_gating=2.023, expert1_balance_top=27.204, expert1_balance_bottom=22.805, unused_expert1_count=0, expert2_balance_top=43.645, expert2_balance_bottom=10.043, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123862, ups=0.24, wpb=524075, bsz=256, num_updates=15700, lr=0.000569312, gnorm=0.005, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=67494
2023-06-29 13:22:33 | INFO | train_inner | epoch 001:  15839 / 102400000 loss=0.091, moe_gate_loss=6.00173, overflow_expert1=0, overflow_expert2=19.174, entropy_gating=2.024, expert1_balance_top=27.216, expert1_balance_bottom=22.797, unused_expert1_count=0, expert2_balance_top=43.28, expert2_balance_bottom=10.397, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.004, ppl=1, wps=123417, ups=0.24, wpb=524338, bsz=256, num_updates=15750, lr=0.000569212, gnorm=0.005, clip=0, loss_scale=256, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=67707
2023-06-29 13:26:05 | INFO | train_inner | epoch 001:  15889 / 102400000 loss=0.091, moe_gate_loss=6.00206, overflow_expert1=0, overflow_expert2=20.631, entropy_gating=2.024, expert1_balance_top=27.279, expert1_balance_bottom=22.734, unused_expert1_count=0, expert2_balance_top=44.745, expert2_balance_bottom=9.68, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.004, ppl=1, wps=123912, ups=0.24, wpb=524274, bsz=256, num_updates=15800, lr=0.000569111, gnorm=0.004, clip=0, loss_scale=512, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=67920
2023-06-29 13:28:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 13:29:43 | INFO | train_inner | epoch 001:  15940 / 102400000 loss=0.091, moe_gate_loss=6.00265, overflow_expert1=0, overflow_expert2=20.078, entropy_gating=2.025, expert1_balance_top=27.222, expert1_balance_bottom=22.796, unused_expert1_count=0, expert2_balance_top=44.579, expert2_balance_bottom=10.023, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=120405, ups=0.23, wpb=524066, bsz=256, num_updates=15850, lr=0.000569011, gnorm=0.005, clip=0, loss_scale=256, train_wall=217, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=68138
2023-06-29 13:33:16 | INFO | train_inner | epoch 001:  15990 / 102400000 loss=0.091, moe_gate_loss=6.00171, overflow_expert1=0, overflow_expert2=22.165, entropy_gating=2.022, expert1_balance_top=27.443, expert1_balance_bottom=22.573, unused_expert1_count=0, expert2_balance_top=46.817, expert2_balance_bottom=9.246, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123387, ups=0.24, wpb=524174, bsz=256, num_updates=15900, lr=0.000568911, gnorm=0.005, clip=0, loss_scale=256, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=68351
2023-06-29 13:36:48 | INFO | train_inner | epoch 001:  16040 / 102400000 loss=0.091, moe_gate_loss=6.00208, overflow_expert1=0.001, overflow_expert2=21.774, entropy_gating=2.018, expert1_balance_top=27.371, expert1_balance_bottom=22.645, unused_expert1_count=0, expert2_balance_top=46.186, expert2_balance_bottom=8.135, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123879, ups=0.24, wpb=524061, bsz=256, num_updates=15950, lr=0.000568811, gnorm=0.005, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=68563
2023-06-29 13:40:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 13:40:27 | INFO | train_inner | epoch 001:  16091 / 102400000 loss=0.091, moe_gate_loss=6.00215, overflow_expert1=0, overflow_expert2=20.98, entropy_gating=2.018, expert1_balance_top=27.218, expert1_balance_bottom=22.801, unused_expert1_count=0, expert2_balance_top=45.641, expert2_balance_bottom=8.349, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=120362, ups=0.23, wpb=524311, bsz=256, num_updates=16000, lr=0.000568711, gnorm=0.005, clip=0, loss_scale=256, train_wall=217, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=68781
2023-06-29 13:43:59 | INFO | train_inner | epoch 001:  16141 / 102400000 loss=0.091, moe_gate_loss=6.00178, overflow_expert1=0, overflow_expert2=20.89, entropy_gating=2.015, expert1_balance_top=27.352, expert1_balance_bottom=22.65, unused_expert1_count=0, expert2_balance_top=45.377, expert2_balance_bottom=8.022, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=124108, ups=0.24, wpb=524246, bsz=256, num_updates=16050, lr=0.000568611, gnorm=0.005, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=68993
2023-06-29 13:47:30 | INFO | train_inner | epoch 001:  16191 / 102400000 loss=0.091, moe_gate_loss=6.00379, overflow_expert1=0, overflow_expert2=22.307, entropy_gating=2.017, expert1_balance_top=27.645, expert1_balance_bottom=22.371, unused_expert1_count=0, expert2_balance_top=46.493, expert2_balance_bottom=7.653, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=124294, ups=0.24, wpb=524245, bsz=256, num_updates=16100, lr=0.000568511, gnorm=0.005, clip=0, loss_scale=256, train_wall=210, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=69205
2023-06-29 13:51:04 | INFO | train_inner | epoch 001:  16241 / 102400000 loss=0.091, moe_gate_loss=6.0018, overflow_expert1=0, overflow_expert2=23.069, entropy_gating=2.018, expert1_balance_top=27.152, expert1_balance_bottom=22.87, unused_expert1_count=0, expert2_balance_top=47.538, expert2_balance_bottom=8.137, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.004, ppl=1, wps=123008, ups=0.23, wpb=524354, bsz=256, num_updates=16150, lr=0.000568411, gnorm=0.005, clip=0, loss_scale=512, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=69419
2023-06-29 13:53:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 13:54:41 | INFO | train_inner | epoch 001:  16292 / 102400000 loss=0.091, moe_gate_loss=6.00203, overflow_expert1=0, overflow_expert2=22.549, entropy_gating=2.017, expert1_balance_top=26.954, expert1_balance_bottom=23.066, unused_expert1_count=0, expert2_balance_top=47.404, expert2_balance_bottom=8.775, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.004, ppl=1, wps=120828, ups=0.23, wpb=524037, bsz=256, num_updates=16200, lr=0.00056831, gnorm=0.004, clip=0, loss_scale=256, train_wall=216, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=69636
2023-06-29 13:58:14 | INFO | train_inner | epoch 001:  16342 / 102400000 loss=0.091, moe_gate_loss=6.00175, overflow_expert1=0, overflow_expert2=21.597, entropy_gating=2.019, expert1_balance_top=27.033, expert1_balance_bottom=22.971, unused_expert1_count=0, expert2_balance_top=47.16, expert2_balance_bottom=8.854, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.004, ppl=1, wps=123541, ups=0.24, wpb=524125, bsz=256, num_updates=16250, lr=0.00056821, gnorm=0.004, clip=0, loss_scale=256, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=69849
2023-06-29 14:01:46 | INFO | train_inner | epoch 001:  16392 / 102400000 loss=0.091, moe_gate_loss=6.00158, overflow_expert1=0, overflow_expert2=21.362, entropy_gating=2.015, expert1_balance_top=27.203, expert1_balance_bottom=22.817, unused_expert1_count=0, expert2_balance_top=46.602, expert2_balance_bottom=8.147, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.004, ppl=1, wps=124195, ups=0.24, wpb=524316, bsz=256, num_updates=16300, lr=0.00056811, gnorm=0.005, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=70061
2023-06-29 14:02:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 14:02:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-06-29 14:05:27 | INFO | train_inner | epoch 001:  16444 / 102400000 loss=0.091, moe_gate_loss=6.00208, overflow_expert1=0, overflow_expert2=23.391, entropy_gating=2.011, expert1_balance_top=27.076, expert1_balance_bottom=22.951, unused_expert1_count=0, expert2_balance_top=48.61, expert2_balance_bottom=7.865, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.004, ppl=1, wps=118819, ups=0.23, wpb=524096, bsz=256, num_updates=16350, lr=0.00056801, gnorm=0.004, clip=0, loss_scale=128, train_wall=220, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=70282
2023-06-29 14:09:00 | INFO | train_inner | epoch 001:  16494 / 102400000 loss=0.091, moe_gate_loss=6.00174, overflow_expert1=0, overflow_expert2=23.127, entropy_gating=2.015, expert1_balance_top=26.949, expert1_balance_bottom=23.059, unused_expert1_count=0, expert2_balance_top=48.475, expert2_balance_bottom=8.222, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.004, ppl=1, wps=123517, ups=0.24, wpb=524258, bsz=256, num_updates=16400, lr=0.00056791, gnorm=0.004, clip=0, loss_scale=128, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=70495
2023-06-29 14:12:30 | INFO | train_inner | epoch 001:  16544 / 102400000 loss=0.091, moe_gate_loss=6.00208, overflow_expert1=0, overflow_expert2=22.404, entropy_gating=2.016, expert1_balance_top=27.25, expert1_balance_bottom=22.769, unused_expert1_count=0, expert2_balance_top=47.372, expert2_balance_bottom=8.998, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.004, ppl=1, wps=124937, ups=0.24, wpb=524260, bsz=256, num_updates=16450, lr=0.00056781, gnorm=0.005, clip=0, loss_scale=256, train_wall=209, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=70705
2023-06-29 14:16:05 | INFO | train_inner | epoch 001:  16594 / 102400000 loss=0.091, moe_gate_loss=6.00139, overflow_expert1=0, overflow_expert2=22.644, entropy_gating=2.015, expert1_balance_top=27.071, expert1_balance_bottom=22.948, unused_expert1_count=0, expert2_balance_top=47.489, expert2_balance_bottom=8.913, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.004, ppl=1, wps=122364, ups=0.23, wpb=524230, bsz=256, num_updates=16500, lr=0.00056771, gnorm=0.005, clip=0, loss_scale=256, train_wall=214, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=70920
2023-06-29 14:19:38 | INFO | train_inner | epoch 001:  16644 / 102400000 loss=0.091, moe_gate_loss=6.00106, overflow_expert1=0, overflow_expert2=22.494, entropy_gating=2.013, expert1_balance_top=27.013, expert1_balance_bottom=23, unused_expert1_count=0, expert2_balance_top=47.412, expert2_balance_bottom=8.846, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=123187, ups=0.24, wpb=524086, bsz=256, num_updates=16550, lr=0.00056761, gnorm=0.005, clip=0, loss_scale=256, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=71133
2023-06-29 14:23:10 | INFO | train_inner | epoch 001:  16694 / 102400000 loss=0.091, moe_gate_loss=6.00214, overflow_expert1=0, overflow_expert2=23.144, entropy_gating=2.009, expert1_balance_top=27.172, expert1_balance_bottom=22.833, unused_expert1_count=0, expert2_balance_top=48.224, expert2_balance_bottom=8.01, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.004, ppl=1, wps=124306, ups=0.24, wpb=524313, bsz=256, num_updates=16600, lr=0.000567509, gnorm=0.005, clip=0, loss_scale=512, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=71345
2023-06-29 14:24:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 14:26:47 | INFO | train_inner | epoch 001:  16745 / 102400000 loss=0.091, moe_gate_loss=6.00226, overflow_expert1=0.001, overflow_expert2=22.77, entropy_gating=2.011, expert1_balance_top=27.235, expert1_balance_bottom=22.79, unused_expert1_count=0, expert2_balance_top=47.49, expert2_balance_bottom=7.944, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.005, ppl=1, wps=121142, ups=0.23, wpb=524160, bsz=256, num_updates=16650, lr=0.000567409, gnorm=0.005, clip=0, loss_scale=256, train_wall=216, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=71562
2023-06-29 14:30:20 | INFO | train_inner | epoch 001:  16795 / 102400000 loss=0.091, moe_gate_loss=6.00127, overflow_expert1=0, overflow_expert2=22.989, entropy_gating=2.011, expert1_balance_top=27.022, expert1_balance_bottom=22.985, unused_expert1_count=0, expert2_balance_top=47.786, expert2_balance_bottom=7.505, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.004, ppl=1, wps=123870, ups=0.24, wpb=524208, bsz=256, num_updates=16700, lr=0.000567309, gnorm=0.005, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=71774
2023-06-29 14:33:55 | INFO | train_inner | epoch 001:  16845 / 102400000 loss=0.091, moe_gate_loss=6.00159, overflow_expert1=0, overflow_expert2=22.17, entropy_gating=2.01, expert1_balance_top=27.085, expert1_balance_bottom=22.92, unused_expert1_count=0, expert2_balance_top=47.051, expert2_balance_bottom=8.195, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.004, ppl=1, wps=122318, ups=0.23, wpb=524234, bsz=256, num_updates=16750, lr=0.000567209, gnorm=0.005, clip=0, loss_scale=512, train_wall=214, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=71990
Expecting value: line 1 column 1 (char 0)

2023-06-29 14:37:28 | INFO | train_inner | epoch 001:  16895 / 102400000 loss=0.091, moe_gate_loss=6.00184, overflow_expert1=0, overflow_expert2=23.173, entropy_gating=2.007, expert1_balance_top=27.261, expert1_balance_bottom=22.741, unused_expert1_count=0, expert2_balance_top=47.581, expert2_balance_bottom=7.382, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.004, ppl=1, wps=123728, ups=0.24, wpb=524143, bsz=256, num_updates=16800, lr=0.000567109, gnorm=0.005, clip=0, loss_scale=512, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=72202
2023-06-29 14:37:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 14:41:05 | INFO | train_inner | epoch 001:  16946 / 102400000 loss=0.091, moe_gate_loss=6.00173, overflow_expert1=0, overflow_expert2=24.488, entropy_gating=2.005, expert1_balance_top=27.157, expert1_balance_bottom=22.855, unused_expert1_count=0, expert2_balance_top=48.987, expert2_balance_bottom=7.049, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.004, ppl=1, wps=120961, ups=0.23, wpb=524061, bsz=256, num_updates=16850, lr=0.000567009, gnorm=0.005, clip=0, loss_scale=256, train_wall=216, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=72420
2023-06-29 14:44:37 | INFO | train_inner | epoch 001:  16996 / 102400000 loss=0.091, moe_gate_loss=6.00157, overflow_expert1=0, overflow_expert2=24.107, entropy_gating=2.005, expert1_balance_top=27.106, expert1_balance_bottom=22.898, unused_expert1_count=0, expert2_balance_top=48.835, expert2_balance_bottom=7.207, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.004, ppl=1, wps=123914, ups=0.24, wpb=524320, bsz=256, num_updates=16900, lr=0.000566909, gnorm=0.005, clip=0, loss_scale=256, train_wall=211, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=72632
2023-06-29 14:47:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-06-29 14:48:11 | INFO | train_inner | epoch 001:  17047 / 102400000 loss=0.091, moe_gate_loss=6.00204, overflow_expert1=0, overflow_expert2=23.022, entropy_gating=2.005, expert1_balance_top=27.171, expert1_balance_bottom=22.844, unused_expert1_count=0, expert2_balance_top=48.139, expert2_balance_bottom=7.447, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.004, ppl=1, wps=122709, ups=0.23, wpb=524220, bsz=256, num_updates=16950, lr=0.000566809, gnorm=0.005, clip=0, loss_scale=256, train_wall=213, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=72846
2023-06-29 14:51:45 | INFO | train_inner | epoch 001:  17097 / 102400000 loss=0.091, moe_gate_loss=6.00216, overflow_expert1=0, overflow_expert2=23.069, entropy_gating=2.003, expert1_balance_top=27.185, expert1_balance_bottom=22.833, unused_expert1_count=0, expert2_balance_top=47.68, expert2_balance_bottom=7.253, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, inner_loss=0.004, ppl=1, wps=123320, ups=0.24, wpb=524123, bsz=256, num_updates=17000, lr=0.000566708, gnorm=0.005, clip=0, loss_scale=256, train_wall=212, cuda_gb_allocated=12.8, cuda_gb_reserved=14.7, cuda_gb_free=19, wall=73059
