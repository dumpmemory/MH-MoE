cd fairseq/
python -m torch.distributed.launch --nproc_per_node=16 train.py /mnt/msranlp/shaohanh/data/redstone_v2_1_config/    \
    --num-workers 2 \
    --share-decoder-input-output-embed \
    --save-interval-updates 1000 \
    --no-epoch-checkpoints \
    --memory-efficient-fp16 \
    --fp16-init-scale 4 \
    --arch gpt_small \
    --task gpt \
    --sample-break-mode none \
    --tokens-per-sample 2048 \
    --optimizer adam --adam-betas "(0.9, 0.98)" \
    --adam-eps 1e-06 \
    --clip-norm 2.0 \
    --lr 6e-4 \
    --lr-scheduler polynomial_decay \
    --warmup-updates 375 \
    --dropout 0.1 \
    --attention-dropout 0.1 \
    --weight-decay 0.01 \
    --batch-size 4 \
    --update-freq 1 \
    --required-batch-size-multiple 1 \
    --total-num-update 300000 \
    --max-update 300000 \
    --seed 1 \
    --ddp-backend=c10d \
    --save-dir /mnt/msranlp/shaohanh/exp/ts_moe_exp/0605-small-baseline-redstone_v2-flash_attn \
    --tensorboard-logdir /mnt/msranlp/shaohanh/exp/ts_moe_exp/0605-small-baseline-redstone_v2-flash_attn/tb-logs      \
    --log-format simple      --log-interval 50     --disable-validation      \
    --subln    --xpos-rel-pos  --no-token-positional-embeddings \
    --tiktoken-model cl100k_base \
    --dict-path /mnt/msranlp/shaohanh/exp/unigpt_exp/data/tiktoken/cl100k_w_code_dict.txt